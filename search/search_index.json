{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pandemonium's cheat sheet","text":"<p>My awesome cheat sheet, because they're awesomes. Don't forget to share/fork/star.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This project brings together all the curations on the subjects that fascinate me.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>All this cheat sheet are available 'as in' or with mkdocs.</p>"},{"location":"#installing","title":"Installing","text":"<p>Simply clone the repository</p> <pre><code>mkdir -p ~/git/pandemonium1986\ncd  ~/git/pandemonium1986\ngit clone https://github.com/pandemonium1986/cheatsheet.git\n</code></pre> <p>To run Pandemonium1986/cheatsheet with mkdocs</p> <pre><code>cd  ~/git/pandemonium1986/cheatsheet\nmkdocs serve -a 192.168.56.10:8081\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Michael Maffait - Initial work - Pandemonium1986</li> </ul>"},{"location":"cloud/azure-devops/","title":"Azure DevOps","text":""},{"location":"cloud/azure-devops/#azure-overview-for-az-400","title":"Azure : Overview for AZ-400","text":""},{"location":"cloud/azure-devops/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Azure X.X.X"},{"location":"cloud/azure-devops/#evolve-your-devops-practices","title":"Evolve your DevOps practices","text":"<p>DevOps is the union of people, process, and products to enable continuous delivery of value to your end users. Azure DevOps is a set of services that gives you the tools you need to do just that. With Azure DevOps, you can build, test, and deploy any application, either to the cloud or on premises. DevOps practices that enable transparency, cooperation, continuous delivery and continuous deployment become embedded in your software development lifecycle.</p>"},{"location":"cloud/azure-devops/#assess-your-existing-software-development-process","title":"Assess your existing software development process","text":""},{"location":"cloud/azure-devops/#the-teams-release-process","title":"The team's release process","text":"<p>The first step to setting up a DevOps practice is to assess your current process. This means analyzing:</p> <ul> <li>Your existing artifacts, such as deployment packages and NuGet, as well as your container repositories.</li> <li>Your existing test management tools.</li> <li>Your existing work management tools.</li> </ul>"},{"location":"cloud/azure-devops/#assess-process-efficiency-with-value-stream-maps","title":"Assess process efficiency with value stream maps","text":"<p>Creating a value stream map, or VSM, helps you analyze your current release cycle process. The purpose of a VSM is to visually show where in the process a team creates value and where there's waste. The goal, of course, is to arrive at a process that delivers maximum value to the customer with minimum waste. A VSM can help you pinpoint those areas that either don't contribute any value or that actually reduce the value of the product.</p>"},{"location":"cloud/azure-devops/#get-started-with-azure-devops","title":"Get started with Azure DevOps","text":""},{"location":"cloud/azure-devops/#what-is-devops","title":"What is DevOps?","text":"<p>DevOps is the union of people, process, and products to enable continuous delivery of value to our end users</p> <ul> <li> <p>Agile planning.     Together, we'll create a backlog of work that everyone on the team and in management can see. We'll prioritize the items so we know what we need to work on first. The backlog can include user stories, bugs, and any other information that helps us.</p> </li> <li> <p>Continuous integration (CI).     We'll automate how we build and test our code. We'll run that every time a team member commits changes to version control.</p> </li> <li> <p>Continuous delivery (CD).     CD is how we test, configure, and deploy from a build to a QA or production environment.     Monitoring. We'll use telemetry to get information about an application's performance and usage patterns. We can use that information to improve as we iterate.</p> </li> <li> <p>Deploy more frequently     In fact, some teams deploy up to dozens of times per day.     Practices such as monitoring, continuous testing, database change management, and integrating security earlier in the software development process help elite performers deploy more frequently, and with greater predictability and security.</p> </li> <li> <p>Reduce lead time from commit to deploy     Lead time is the time it takes for a feature to make it to the customer. By working in smaller batches, automating manual processes, and deploying more frequently, elite performers can achieve in hours or days what once took weeks or even months.</p> </li> <li> <p>Reduce change failure rate     A new feature that fails in production or that causes other features to break can create a lost opportunity between you and your users. As high-performing teams mature, they reduce their change failure rate over time.</p> </li> <li> <p>Recover from incidents more quickly     When incidents do occur, elite performers are able to recover more quickly. Acting on metrics helps elite performers recover more quickly while also deploying more frequently.</p> </li> </ul>"},{"location":"cloud/azure-devops/#what-is-azure-devops","title":"What is Azure DevOps?","text":"<ul> <li> <p>Azure Boards     These are agile tools that help us plan, track, and discuss our work, even with other teams.</p> </li> <li> <p>Azure Pipelines     These will let us build, test, and deploy with CI/CD that works with any language, platform, and cloud.</p> </li> <li> <p>Azure Test Plans     These are manual and exploratory testing tools.</p> </li> <li> <p>Azure Repos     These provide unlimited, cloud-hosted private, and public Git repos.</p> </li> <li> <p>Azure Artifacts     These let us create, host, and share packages.</p> </li> </ul>"},{"location":"cloud/azure-devops/#measures","title":"Measures","text":"<p>Faster Outcomes</p> <ul> <li>Deployment Frequency. Increasing the frequency of deployments is often a critical driver in DevOps teams.</li> <li>Deployment Speed. As well as increasing how often deployments happen, it's important to decrease the time that they take.</li> <li>Deployment Size. How many features, stories, and bugfixes are being deployed each time?     Lead Time. How long does it take from starting on a work item, until it is deployed?</li> </ul> <p>Efficiency</p> <ul> <li>Server to Admin Ratio. Are the projects reducing the number of administrators required for a given number of servers?</li> <li>Staff Member to Customers Ratio. Is it possible for fewer staff members to serve a given number of customers?</li> <li>Application Usage. How busy is the application?</li> <li>Application Performance. Is the application performance improving or dropping? (Based upon application metrics.)</li> </ul> <p>Quality and Security</p> <ul> <li>Deployment Failure Rates. How often do deployments (and/or applications) fail?</li> <li>Application Failure Rates. How often do application failures occur, such as configuration failures, performance timeouts, and so on?</li> <li>Mean Time to Recover. How quickly can you recover from a failure?</li> <li>Bug Report Rates. You don't want customers finding bugs in your code. Is the amount they are finding increasing or decreasing?</li> <li>Test Pass Rates. How well is your automated testing working?</li> <li>Defect Escape Rate. What percentage of defects are being found in production?</li> <li>Availability. What percentage of time is the application truly available for customers?</li> <li>SLA Achievement. Are you meeting your service level agreements (SLAs)?</li> <li>Mean Time to Detection. If there is a failure, how long does it take for it to be detected?</li> </ul> <p>Culture</p> <ul> <li>Employee Morale. Are employees happy with the transformation and where the organization is heading? Are they still willing to respond to further changes?</li> <li>Retention Rates. Is the organization losing staff?</li> </ul> <p>Common quality metrics</p> <ul> <li>Failed Builds Percentage - Overall, what percentage of builds are failing?</li> <li>Failed Deployments Percentage - Overall, what percentage of deployments are failing?</li> <li>Ticket Volume - What is the overall volume of customer bug tickets?</li> <li>Bug Bounce Percentage - What percentage of customer or bug tickets are being reopened?</li> <li>Unplanned Work Percentage - What percentage of the overall work being performed is unplanned?</li> </ul>"},{"location":"cloud/azure-devops/#choose-an-agile-approach-to-software-development","title":"Choose an Agile approach to software development","text":""},{"location":"cloud/azure-devops/#what-is-agile","title":"What is Agile?","text":"<ul> <li>Individuals and interactions over processes and tools</li> <li>Working software over comprehensive documentation</li> <li>Customer collaboration over contract negotiation</li> <li>Responding to change over following a plan</li> </ul>"},{"location":"cloud/azure-devops/#what-is-azure-boards","title":"What is Azure Boards?","text":"<p>There are four processes to choose from. We can use:</p> <ul> <li>Capability Maturity Model Integration (CMMI). This is really for large organizations and it's pretty complicated so I didn't use it.</li> <li>Scrum. Scrum depends on a Scrum master who leads the Scrum team. The Scrum master makes sure everybody understands Scrum theory, practices, and rules. We don't have a Scrum master; that's someone who's usually received some training and certification so I didn't pick that one either.</li> <li>Agile. This seemed like the obvious choice since I'm always talking about Agile, but it has a few more things to consider than the simplest option.</li> <li>Basic. Basic is, well, basic. It's simple but gives us enough power to start doing effective planning right away, and that's why I picked it. In the Basic workflow, you move work from To Do to Doing to Done.</li> </ul>"},{"location":"cloud/azure-devops/#build-applications-with-azure-devops","title":"Build applications with Azure DevOps","text":""},{"location":"cloud/azure-devops/#create-a-build-pipeline-with-azure-pipelines","title":"Create a build pipeline with Azure Pipelines","text":""},{"location":"cloud/azure-devops/#introduction","title":"Introduction","text":"<p>Gestionnaire de package Ubuntu 18,04-installer .NET Core</p>"},{"location":"cloud/azure-devops/#what-is-azure-pipelines","title":"What is Azure Pipelines?","text":"<p>What is continuous integration?</p> <p>Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control.</p> <p>Implement and manage build infrastructure</p> <ul> <li> <p>Build agents :     As you know, a build agent is a piece of installable software that runs one build or deployment job at a time. To build your code or deploy your software you need at least one agent. As you add more code and people, you'll eventually need more than one. Let's examine build agents in a bit more depth.</p> </li> <li> <p>The differences between implementing hosted and private agents     You can use either a Microsoft-hosted or a private agent. What are the differences?</p> <p>If your pipelines are in Azure Pipelines, then you've got a convenient option to build and deploy using a Microsoft-hosted agent. With Microsoft-hosted agents, maintenance and upgrades are taken care of for you. Each time you run a pipeline, you get a fresh virtual machine. The virtual machine is discarded after one use.</p> <p>For many teams this is the simplest way to build and deploy. You can try it first and see if it works for your build or deployment. If not, you can use a self-hosted agent.</p> <p>An agent that you set up and manage on your own to run build and deployment jobs is a self-hosted agent. You can use self-hosted agents in Azure Pipelines. Self-hosted agents give you more control and let you install any software you need for your builds and deployments.</p> <p>You can install the agent on Linux, macOS, or Windows machines. You can also install an agent on a Linux Docker container. After you've installed the agent on a machine, you can install any other software on that machine as required by your build or deployment jobs.</p> </li> <li> <p>Agent pools     Instead of managing each agent individually, you can organize agents into agent pools. An agent pool defines the sharing boundary for all agents in that pool. In Azure Pipelines, agent pools are scoped to the Azure DevOps organization so you can share an agent pool across projects.</p> <p>A project agent pool provides access to an organization agent pool. When you create a build or release pipeline, you specify which pool it uses. Pools are scoped to your project so you can only use them across build and release pipelines within a project.</p> <p>To share an agent pool with multiple projects, in each of those projects, you create a project agent pool pointing to an organization agent pool. While multiple pools across projects can use the same organization agent pool, multiple pools within a project cannot use the same organization agent pool. Also, each project agent pool can use only one organization agent pool.</p> </li> <li> <p>Agent queues     If you are a project team member, you create and manage agent build queues from the agent pools tab in project settings.</p> </li> <li> <p>Service endpoints for integration with third-party systems     Service endpoints are a way for Azure DevOps to connect to external systems or services. They are a bundle of securely stored properties that includes but is not limited to:</p> <p>Service name Description Server URL Certificates or tokens Usernames and passwords</p> </li> </ul> <p>Extensions are then able to access the service endpoint to get the stored details to perform the necessary operations on that service.</p> <ul> <li> <p>Concurrent pipelines     You can run concurrent pipelines (also called parallel jobs) in Azure Pipelines. One parallel job in Azure Pipeline lets you run a single build or release job at any given time. This rule is true whether you run the job on Microsoft-hosted or self-hosted agents. Parallel jobs are purchased at the organization level, and they are shared by all projects in an organization.</p> </li> <li> <p>Microsoft-hosted CI/CD     If you want to run your builds and releases on machines that Microsoft manages, use Microsoft-hosted parallel jobs. Your jobs run on the pool of hosted agents. Microsoft provides a free tier of service by default for every organization. Consult the Azure DevOps documentation to see the criteria.</p> <p>If you want Azure Pipelines to orchestrate your builds and releases, but use your own machines to run them, use self-hosted parallel jobs. You start by deploying agents on your machines. You can register any number of these self-hosted agents in your organization. Microsoft charges based on the number of jobs you want to run at a time, not the number of agents registered.</p> </li> <li> <p>Plan a strategy for concurrent pipelines</p> </li> </ul> <p>Here are some steps to take to plan for concurrent pipelines. Determine how many parallel jobs you need</p> <p>Begin by seeing if the free tier offered in your organization is enough for your teams. When you've reached the per-month limit for the free tier of Microsoft-hosted parallel jobs, you can start by buying one parallel job. As the number of queued builds and releases exceeds the number of parallel jobs you have, your build and release queues will grow longer. When you find the queue delays are too long, you can purchase additional parallel jobs as needed. A simple rule of thumb is to estimate that you'll need one parallel job for every four to five users in your organization.</p> <p>Think about your scenario</p> <p>Here are some examples of where you might need multiple parallel jobs.</p> <pre><code>If you have multiple teams, and if each of them requires a CI build, you'll likely need a parallel job for each team.\nIf your CI build trigger applies to multiple branches, you'll likely need a parallel job for each active branch.\nIf you develop multiple applications by using one organization or server, you'll likely need additional parallel jobs, one to deploy each application at the same time.\n</code></pre>"},{"location":"cloud/azure-devops/#exercise-get-the-sample-application","title":"Exercise - Get the sample application","text":""},{"location":"cloud/azure-devops/#plan-your-build-tasks","title":"Plan your build tasks","text":""},{"location":"cloud/azure-devops/#exercise-set-up-your-azure-devops-environment","title":"Exercise - Set up your Azure DevOps environment","text":""},{"location":"cloud/azure-devops/#exercise-create-the-pipeline","title":"Exercise - Create the pipeline","text":""},{"location":"cloud/azure-devops/#exercise-publish-the-result-to-the-pipeline","title":"Exercise - Publish the result to the pipeline","text":""},{"location":"cloud/azure-devops/#exercise-build-multiple-configurations-by-using-templates","title":"Exercise - Build multiple configurations by using templates","text":""},{"location":"cloud/azure-devops/#exercise-clean-up-your-azure-devops-environment","title":"Exercise - Clean up your Azure DevOps environment","text":""},{"location":"cloud/azure-devops/#summary","title":"Summary","text":""},{"location":"cloud/azure-devops/#implement-a-code-workflow-in-your-build-pipeline-by-using-git-and-github","title":"Implement a code workflow in your build pipeline by using Git and GitHub","text":""},{"location":"cloud/azure-devops/#run-quality-tests-in-your-build-pipeline-by-using-azure-pipelines","title":"Run quality tests in your build pipeline by using Azure Pipelines","text":""},{"location":"cloud/azure-devops/#scan-code-for-vulnerabilities-in-azure-pipelines","title":"Scan code for vulnerabilities in Azure Pipelines","text":""},{"location":"cloud/azure-devops/#scan-open-source-components-for-vulnerabilities-and-license-ratings-in-azure-pipelines","title":"Scan open source components for vulnerabilities and license ratings in Azure Pipelines","text":""},{"location":"cloud/azure-devops/#manage-build-dependencies-with-azure-artifacts","title":"Manage build dependencies with Azure Artifacts","text":""},{"location":"cloud/azure-devops/#host-your-own-build-agent-in-azure-pipelines","title":"Host your own build agent in Azure Pipelines","text":""},{"location":"cloud/azure-devops/#deploy-applications-with-azure-devops","title":"Deploy applications with Azure DevOps","text":""},{"location":"cloud/azure-devops/#automate-your-deployments-with-azure-devops","title":"Automate your deployments with Azure DevOps","text":""},{"location":"cloud/azure-devops/#capture-feedback-and-monitoring-data-to-continuously-improve-your-software","title":"Capture feedback and monitoring data to continuously improve your software","text":""},{"location":"cloud/azure-devops/#source","title":"Source","text":"<p>AZ-400: Microsoft Azure DevOps Solutions</p>"},{"location":"cloud/azure/","title":"Azure","text":""},{"location":"cloud/azure/#azure-overview-for-az-900","title":"Azure : Overview for AZ-900","text":""},{"location":"cloud/azure/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Azure X.X.X"},{"location":"cloud/azure/#azure-fundamentals","title":"Azure fundamentals","text":""},{"location":"cloud/azure/#cloud-concepts-principles-of-cloud-computing","title":"Cloud Concepts - Principles of cloud computing","text":""},{"location":"cloud/azure/#what-is-cloud-computing","title":"What is cloud computing?","text":"<ul> <li>Compute power - such as Linux servers or web applications.</li> <li>Storage - such as files and databases.</li> <li>Networking - such as secure connections between the cloud provider and your company.</li> <li>Analytics - such as visualizing telemetry and performance data.</li> </ul>"},{"location":"cloud/azure/#benefits-of-cloud-computing","title":"Benefits of cloud computing","text":"<p>It's cost-effective</p> <ul> <li>Pay-as-you-go.</li> <li>Consumption-based.</li> </ul> <p>It's scalable</p> <ul> <li>Vertical scaling.</li> <li>Horizontal scaling.</li> </ul> <p>It's elastic</p> <ul> <li>Compensate automatically adding or removing resources.</li> </ul> <p>It's current</p> <ul> <li>Focus on what matters: building and deploying applications.</li> <li>Eliminates the burdens of maintaining software patches, hardware setup, upgrades.</li> </ul> <p>It's reliable</p> <ul> <li>Data backup</li> <li>Disaster recovery.</li> <li>Data replication.</li> </ul> <p>It's global</p> <ul> <li>Fully redundant datacenters located in various regions all over the globe.</li> </ul> <p>It's secure</p> <ul> <li>Physical security.</li> <li>Digital security.</li> </ul>"},{"location":"cloud/azure/#compliance-terms-and-requirements","title":"Compliance terms and requirements","text":"<ul> <li>How compliant is the cloud provider when it comes to handling sensitive data ?</li> <li>How compliant are the services offered by the cloud provider ?</li> <li>How can I deploy my own cloud-based solutions to scenarios that have accreditation or compliance requirements ?</li> <li>What terms are part of the privacy statement for the provider ?</li> </ul>"},{"location":"cloud/azure/#economies-of-scale-capital-expenditure-capex-versus-operational-expenditure-opex","title":"Economies of scale Capital expenditure (CapEx) versus operational expenditure (OpEx)","text":"<p>CapEx</p> <ul> <li>Server costs.</li> <li>Storage costs.</li> <li>Network costs.</li> <li>Backup and archive costs.</li> <li>Organization continuity and disaster recovery costs.</li> <li>Datacenter infrastructure costs.</li> <li>Technical personnel.</li> </ul> <p>Benefits of CapEx</p> <p>With capital expenditures, you plan your expenses at the start of a project or budget period. Your costs are fixed, meaning you know exactly how much is being spent. This is appealing when you need to predict the expenses before a project starts due to a limited budget.</p> <p>OpEx</p> <ul> <li>Leasing software and customized features.</li> <li>Scaling charges based on usage/demand instead of fixed hardware or capacity.</li> <li>Billing at the user or organization level.</li> </ul> <p>Benefits of OpEx</p> <p>With the OpEx model, companies wanting to try a new product or service don't need to invest in equipment. Instead, they pay as much or as little for the infrastructure as required.</p>"},{"location":"cloud/azure/#cloud-deployment-models","title":"Cloud deployment models","text":""},{"location":"cloud/azure/#public-cloud","title":"Public cloud","text":"<p>Advantages</p> <ul> <li>High scalability/agility \u2013 you don't have to buy a new server in order to scale.</li> <li>Pay-as-you-go pricing \u2013 you pay only for what you use, no CapEx costs.</li> <li>You're not responsible for maintenance or updates of the hardware.</li> <li>Minimal technical knowledge to set up and use - you can leverage the skills and expertise of the cloud provider to ensure workloads are secure, safe, and highly available.</li> </ul> <p>Disadvantages</p> <ul> <li>There may be specific security requirements that cannot be met by using public cloud.</li> <li>There may be government policies, industry standards, or legal requirements which public clouds cannot meet.</li> <li>You don't own the hardware or services and cannot manage them as you may want to.</li> <li>Unique business requirements, such as having to maintain a legacy application might be hard to meet.</li> </ul>"},{"location":"cloud/azure/#private-cloud","title":"Private cloud","text":"<p>Advantages</p> <ul> <li>You can ensure the configuration can support any scenario or legacy application.</li> <li>You have control (and responsibility) over security.</li> <li>Private clouds can meet strict security, compliance, or legal requirements.</li> </ul> <p>Disadvantages</p> <ul> <li>You have some initial CapEx costs and must purchase the hardware for startup and maintenance.</li> <li>Owning the equipment limits the agility - to scale you must buy, install, and setup new hardware.</li> <li>Private clouds require IT skills and expertise that's hard to come by.</li> </ul>"},{"location":"cloud/azure/#hybrid-cloud","title":"Hybrid cloud","text":"<p>Advantages</p> <ul> <li>You can keep any systems running and accessible that use out-of-date hardware or an out-of-date operating system.</li> <li>You have flexibility with what you run locally versus in the cloud.</li> <li>You can take advantage of economies of scale from public cloud providers for services and resources where it's cheaper, and then supplement with your own equipment when it's not.</li> <li>You can use your own equipment to meet security, compliance, or legacy scenarios where you need to completely control the environment.</li> </ul> <p>Disadvantages</p> <ul> <li>It can be more expensive than selecting one deployment model since it involves some CapEx cost up front.</li> <li>It can be more complicated to set up and manage.</li> </ul>"},{"location":"cloud/azure/#types-of-cloud-services","title":"Types of cloud services","text":"<p>Infrastructure as a service (IaaS)</p> <p>Infrastructure as a Service is the most flexible category of cloud services. It aims to give you complete control over the hardware that runs your application (IT infrastructure servers and virtual machines (VMs), storage, networks, and operating systems). Instead of buying hardware, with IaaS, you rent it. It's an instant computing infrastructure, provisioned and managed over the internet.</p> <p>Platform as a service (PaaS)</p> <p>PaaS provides an environment for building, testing, and deploying software applications. The goal of PaaS is to help you create an application quickly without managing the underlying infrastructure. For example, when deploying a web application using PaaS, you don't have to install an operating system, web server, or even system updates.</p> <p>Software as a service (SaaS)</p> <p>SaaS is software that is centrally hosted and managed for the end customer. It is usually based on an architecture where one version of the application is used for all customers, and licensed through a monthly or annual subscription. Office 365, Skype, and Dynamics CRM Online are perfect examples of SaaS software.</p> <p>Management responsibilities</p> <p></p>"},{"location":"cloud/azure/#core-cloud-services-introduction-to-azure","title":"Core Cloud Services - Introduction to Azure","text":""},{"location":"cloud/azure/#what-is-azure","title":"What is Azure?","text":"<p>Azure is Microsoft's cloud computing platform. Azure is a continually expanding set of cloud services that help your organization meet your current and future business challenges. Azure gives you the freedom to build, manage, and deploy applications on a massive global network using your favorite tools and frameworks.</p>"},{"location":"cloud/azure/#tour-of-azure-services","title":"Tour of Azure services","text":""},{"location":"cloud/azure/#core-cloud-services-azure-architecture-and-service-guarantees","title":"Core Cloud Services - Azure architecture and service guarantees","text":""},{"location":"cloud/azure/#understand-datacenters-and-regions-in-azure","title":"Understand Datacenters and Regions in Azure","text":"<p>A region is a geographical area on the planet containing at least one, but potentially multiple datacenters that are nearby and networked together with a low-latency network. Azure intelligently assigns and controls the resources within each region to ensure workloads are appropriately balanced.</p>"},{"location":"cloud/azure/#understand-geographies-in-azure","title":"Understand Geographies in Azure","text":"<p>Azure divides the world into geographies that are defined by geopolitical boundaries or country borders. An Azure geography is a discrete market typically containing two or more regions that preserve data residency and compliance boundaries</p> <p>Geographies are broken up into the following areas:</p> <ul> <li>Americas.</li> <li>Europe.</li> <li>Asia Pacific.</li> <li>Middle East and Africa.</li> </ul>"},{"location":"cloud/azure/#understand-availability-zones-in-azure","title":"Understand Availability Zones in Azure","text":"<p>Availability Zones are physically separate datacenters within an Azure region.</p> <p></p>"},{"location":"cloud/azure/#understand-region-pairs-in-azure","title":"Understand Region Pairs in Azure","text":"<p>Each Azure region is always paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. This approach allows for the replication of resources (such as virtual machine storage) across a geography that helps reduce the likelihood of interruptions due to events such as natural disasters, civil unrest, power outages, or physical network outages affecting both regions at once.</p>"},{"location":"cloud/azure/#understand-service-level-agreements-for-azure","title":"Understand Service-Level Agreements for Azure","text":"<p>There are three key characteristics of SLAs for Azure products and services:</p> <ul> <li>Performance Targets.</li> <li>Uptime and Connectivity Guarantees.</li> <li>Service credits.</li> </ul>"},{"location":"cloud/azure/#compose-slas-across-services","title":"Compose SLAs across services","text":"<p>When combining SLAs across different service offerings, the resultant SLA is called a Composite SLA. The resulting composite SLA can provide higher or lower uptime values, depending on your application architecture.</p>"},{"location":"cloud/azure/#sign-up-for-azure","title":"Sign up for Azure","text":""},{"location":"cloud/azure/#understand-azure-billing","title":"Understand Azure billing","text":"<p>Azure subscription</p> <p>When you sign up, an Azure subscription is created by default. An Azure subscription is a logical container used to provision resources in Azure.</p> <p>Create additional Azure subscriptions</p> <ul> <li>Environments.</li> <li>Organizational structures.</li> <li>Billing: You might.</li> </ul> <p></p>"},{"location":"cloud/azure/#azure-support-options","title":"Azure support options","text":"<p>Azure free support resources</p> <p>You have 24/7 access to the online documentation, community support, and new Azure capabilities demo videos on YouTube channel.</p> <p>Azure support plans</p> <ul> <li>Developer.</li> <li>Standard.</li> <li>Professional Direct.</li> </ul>"},{"location":"cloud/azure/#core-cloud-services-manage-services-with-the-azure-portal","title":"Core Cloud Services - Manage services with the Azure portal","text":""},{"location":"cloud/azure/#azure-management-options","title":"Azure management options","text":"<ul> <li>Azure portal for interacting with Azure via a Graphical User Interface (GUI).</li> <li>Azure PowerShell and Azure Command-Line Interface (CLI) for command-line and automation-based interactions with Azure.</li> <li>Azure Cloud Shell for a web-based command-line interface.</li> <li>Azure mobile app for monitoring and managing your resources from your mobile device.</li> </ul>"},{"location":"cloud/azure/#azure-portal-dashboards","title":"Azure Portal dashboards","text":"<p>A dashboard is a customizable collection of UI tiles displayed in the Azure portal. You add, remove, and position tiles to create the exact view you want, and then save that view as a dashboard. Multiple dashboards are supported, and you can switch between them as needed. You can even share your dashboards with other team members.</p>"},{"location":"cloud/azure/#access-public-and-private-preview-features","title":"Access public and private preview features","text":"<ul> <li>Private Preview. An Azure feature marked \"private preview\" is available to specific Azure customers for evaluation purposes. This is typically by invite only and issued directly by the product team responsible for the feature or service.</li> <li>Public Preview. An Azure feature marked \"public preview\" is available to all Azure customers for evaluation purposes. These previews can be turned on through the preview features page as detailed below.</li> </ul>"},{"location":"cloud/azure/#core-cloud-services-azure-compute-options","title":"Core Cloud Services - Azure compute options","text":""},{"location":"cloud/azure/#essential-azure-compute-concepts","title":"Essential Azure compute concepts","text":"<p>What is Azure compute?</p> <p>Azure compute is an on-demand computing service for running cloud-based applications. It provides computing resources like multi-core processors and supercomputers via virtual machines and containers. It also provides serverless computing to run apps without requiring infrastructure setup or configuration. The resources are available on-demand and can typically be created in minutes or even seconds. You pay only for the resources you use and only for as long as you're using them.</p> <ul> <li>Virtual machines.</li> <li>Containers.</li> <li>Azure App Service.</li> <li>Serverless computing.</li> </ul>"},{"location":"cloud/azure/#explore-azure-virtual-machines","title":"Explore Azure Virtual Machines","text":"<p>Azure Virtual Machines (VMs) let you create and use virtual machines in the cloud. They provide infrastructure as a service (IaaS) in the form of a virtualized server and can be used in many ways. Just like a physical computer, you can customize all of the software running on the VM. VMs are an ideal choice when you need:</p> <ul> <li>Total control over the operating system (OS).</li> <li>The ability to run custom software.</li> <li>To use custom hosting configurations.</li> </ul> <p>Scaling VMs in Azure</p> <p>You can run single VMs for testing, development, or minor tasks; or you can group VMs together to provide high availability, scalability, and redundancy. Azure has several features such that, no matter what your uptime requirements are, Azure can meet them. These features include:</p> <ul> <li>Availability sets :     An availability set is a logical grouping of two or more VMs that help keep your application available during planned or unplanned maintenance.</li> <li>Virtual Machine Scale Sets :     Azure Virtual Machine Scale Sets let you create and manage a group of identical, load balanced VMs</li> <li>Azure Batch :     Azure Batch enables large-scale job scheduling and compute management with the ability to scale to tens, hundreds, or thousands of VMs.</li> </ul> <p></p>"},{"location":"cloud/azure/#explore-containers-in-azure","title":"Explore Containers in Azure","text":"<p>Azure supports Docker containers (a standardized container model), and there are several ways to manage containers in Azure.</p> <ul> <li>Azure Container Instances (ACI) :     Azure Container Instances (ACI) offers the fastest and simplest way to run a container in Azure. You don't have to manage any virtual machines or configure any additional services. It is a PaaS offering that allows you to upload your containers and execute them directly with automatic elastic scale.</li> <li>Azure Kubernetes Service (AKS) :     The task of automating, managing, and interacting with a large number of containers is known as orchestration. Azure Kubernetes Service (AKS) is a complete orchestration service for containers with distributed architectures with multiple containers.</li> </ul>"},{"location":"cloud/azure/#explore-azure-app-service","title":"Explore Azure App Service","text":"<p>Azure App Service enables you to build and host web apps, background jobs, mobile backends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers automatic scaling and high availability.</p> <p>With Azure App Service, you can host most common web app styles including:</p> <ul> <li>Web Apps.</li> <li>API Apps.</li> <li>WebJobs.</li> <li>Mobile Apps.</li> </ul>"},{"location":"cloud/azure/#explore-serverless-computing-in-azure","title":"Explore Serverless computing in Azure","text":"<p>Serverless computing is the abstraction of servers, infrastructure, and OSs. With serverless computing, Azure takes care of managing the server infrastructure and allocation/deallocation of resources based on demand. Infrastructure isn't your responsibility. Scaling and performance are handled automatically, and you are billed only for the exact resources you use. There's no need to even reserve capacity.</p> <p>Serverless computing encompasses three ideas:</p> <ul> <li>Abstraction of servers.</li> <li>Event-driven scale.</li> <li>Micro-billing.</li> </ul> <p>Azure has two implementations of serverless compute:</p> <ul> <li>Azure Functions.</li> <li>Azure Logic Apps.</li> </ul>"},{"location":"cloud/azure/#core-cloud-services-azure-data-storage-options","title":"Core Cloud Services - Azure data storage options","text":""},{"location":"cloud/azure/#benefits-of-using-azure-to-store-data","title":"Benefits of using Azure to store data","text":"<p>Benefits</p> <ul> <li>Automated backup and recovery.</li> <li>Replication across the globe.</li> <li>Support for data analytics.</li> <li>Encryption capabilities.</li> <li>Multiple data types.</li> <li>Data storage in virtual disks.</li> <li>Storage tiers.</li> </ul> <p>Types of data</p> <ul> <li>Structured data.</li> <li>Semi-structured data.</li> <li>Unstructured data.</li> </ul>"},{"location":"cloud/azure/#how-azure-data-storage-can-meet-your-business-storage-needs","title":"How Azure data storage can meet your business storage needs","text":"<p>Azure SQL Database</p> <p>Azure SQL Database is a relational database as a service (DaaS) based on the latest stable version of the Microsoft SQL Server database engine.</p> <p>Azure Cosmos DB</p> <p>Azure Cosmos DB is a globally distributed database service. It supports schema-less data that lets you build highly responsive and Always On applications to support constantly changing data.</p> <p>Azure Blob storage</p> <p>Azure Blob Storage is unstructured, meaning that there are no restrictions on the kinds of data it can hold.</p> <p>Azure Data Lake Storage</p> <p>The Data Lake feature allows you to perform analytics on your data usage and prepare reports. Data Lake is a large repository that stores both structured and unstructured data.</p> <p>Azure Files</p> <p>Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol.</p> <p>Azure Queue</p> <p>Azure Queue storage is a service for storing large numbers of messages that can be accessed from anywhere in the world.</p> <p>Disk Storage</p> <p>Disk storage provides disks for virtual machines, applications, and other services to access and use as they need, similar to how they would in on-premises scenarios</p> <p>Storage tiers</p> <p>Azure offers three storage tiers for blob object storage:</p> <ul> <li>Hot storage tier: optimized for storing data that is accessed frequently.</li> <li>Cool storage tier: optimized for data that are infrequently accessed and stored for at least 30 days.</li> <li>Archive storage tier: for data that are rarely accessed and stored for at least 180 days with flexible latency requirements.</li> </ul> <p>Encryption and replication</p> <p>Azure provides security and high availability to your data through encryption and replication features.</p> <ul> <li>Azure Storage Service Encryption (SSE) for data at rest helps you secure your data to meet the organization's security and regulatory compliance. It encrypts the data before storing it and decrypts the data before retrieving it. The Encryption and decryption are transparent to the user.</li> <li>Client-side encryption is where the data is already encrypted by the client libraries. Azure stores the data in the encrypted state at rest, which is then decrypted during retrieval.</li> </ul>"},{"location":"cloud/azure/#comparison-between-azure-data-storage-and-on-premises-storage","title":"Comparison between Azure data storage and on-premises storage","text":"<ul> <li>Cost effectiveness.</li> <li>Reliability.</li> <li>Storage types.</li> <li>Agility.</li> </ul>"},{"location":"cloud/azure/#core-cloud-services-azure-networking-options","title":"Core Cloud Services - Azure networking options","text":""},{"location":"cloud/azure/#deploy-your-site-to-azure","title":"Deploy your site to Azure","text":"<p>Using an N-tier architecture</p> <p>An N-tier architecture divides an application into two or more logical tiers. Architecturally, a higher tier can access services from a lower tier, but a lower tier should never access a higher tier.</p> <ul> <li>The web tier provides the web interface to your users through a browser.</li> <li>The application tier runs business logic.</li> <li>The data tier includes databases and other storage that hold product information and customer orders.</li> </ul> <p></p> <p>What's an Azure region?</p> <p>A region is one or more Azure data centers within a specific geographic location. East US, West US, and North Europe are examples of regions. In this instance, you see that the application is running in the East US region.</p> <p>What's a virtual network?</p> <p>A virtual network allows Azure resources to securely communicate with each other, the internet, and on-premises networks. A virtual network is scoped to a single region; however, multiple virtual networks from different regions can be connected together using virtual network peering.</p> <p>What's a network security group?</p> <p>A network security group, or NSG, allows or denies inbound network traffic to your Azure resources. Think of a network security group as a cloud-level firewall for your network.</p>"},{"location":"cloud/azure/#scale-with-azure-load-balancer","title":"Scale with Azure Load Balancer","text":"<p>What are availability and high availability?</p> <p>Availability refers to how long your service is up and running without interruption. High availability, or highly available, refers to a service that's up and running for a long period of time.</p> <p>What is resiliency?</p> <p>Resiliency refers to a system's ability to stay operational during abnormal conditions.</p> <ul> <li>Natural disasters.</li> <li>System maintenance, both planned and unplanned, including software updates and security patches.</li> <li>Spikes in traffic to your site.</li> <li>Threats made by malicious parties, such as distributed denial of service, or DDoS, attacks.</li> </ul> <p>What is a load balancer?</p> <p>A load balancer distributes traffic evenly among each system in a pool. A load balancer can help you achieve both high availability and resiliency.</p> <p>What is Azure Load Balancer?</p> <p>Azure Load Balancer is a load balancer service that Microsoft provides that helps take care of the maintenance for you. Load Balancer supports inbound and outbound scenarios, provides low latency and high throughput, and scales up to millions of flows for all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) applications.</p> <p></p> <p>Azure Application Gateway</p> <p>Application Gateway is a load balancer designed for web applications. It uses Azure Load Balancer at the transport level (TCP) and applies sophisticated URL-based routing rules to support several advanced scenarios.</p> <p>Here are some of the benefits of using Azure Application Gateway over a simple load balancer:</p> <ul> <li>Cookie affinity.</li> <li>SSL termination.</li> <li>Web application firewall.</li> <li>URL rule-based routes.</li> <li>Rewrite HTTP headers.</li> </ul> <p>What is a Content Delivery Network?</p> <p>A content delivery network (CDN) is a distributed network of servers that can efficiently deliver web content to users. It is a way to get content to users in their local region to minimize latency.</p> <p>What about DNS?</p> <p>DNS, or Domain Name System, is a way to map user-friendly names to their IP addresses. You can think of DNS as the phonebook of the internet.</p>"},{"location":"cloud/azure/#reduce-latency-with-azure-traffic-manager","title":"Reduce latency with Azure Traffic Manager","text":"<p>What is network latency?</p> <p>Latency refers to the time it takes for data to travel over the network. Latency is typically measured in milliseconds.</p> <p>Compare latency to bandwidth. Bandwidth refers to the amount of data that can fit on the connection. Latency refers to the time it takes for that data to reach its destination.</p> <p>Scale out to different regions</p> <p>One way to reduce latency is to provide exact copies of your service in more than one region. The following illustration shows an example of global deployment.</p> <p></p> <p>Use Traffic Manager to route users to the closest endpoint</p> <p>One answer is Azure Traffic Manager. Traffic Manager uses the DNS server that's closest to the user to direct user traffic to a globally distributed endpoint.</p> <p></p>"},{"location":"cloud/azure/#security-responsibility-and-trust-in-azure","title":"Security, responsibility, and trust in Azure","text":""},{"location":"cloud/azure/#cloud-security-is-a-shared-responsibility","title":"Cloud security is a shared responsibility","text":"<p>A layered approach to security</p> <p></p> <ul> <li>Data :     It's the responsibility of those storing and controlling access to data to ensure that it's properly secured.</li> <li>Application :     Integrating security into the application development lifecycle will help reduce the number of vulnerabilities introduced in code.</li> <li>Compute :     Malware, unpatched systems, and improperly secured systems open your environment to attacks.</li> <li>Networking :     At this layer, the focus is on limiting the network connectivity across all your resources to allow only what is required. By limiting this communication, you reduce the risk of lateral movement throughout your network.</li> <li>Perimeter :     At the network perimeter, it's about protecting from network-based attacks against your resources.</li> <li>Identity and access :     The identity and access layer is all about ensuring identities are secure, access granted is only what is needed, and changes are logged.</li> <li>Physical security :     With physical security, the intent is to provide physical safeguards against access to assets. These safeguards ensure that other layers can't be bypassed, and loss or theft is handled appropriately.</li> </ul>"},{"location":"cloud/azure/#get-tips-from-azure-security-center","title":"Get tips from Azure Security Center","text":"<p>A great place to start when examining the security of your Azure-based solutions is Azure Security Center. Security Center is a monitoring service that provides threat protection across all of your services both in Azure, and on-premises. Security Center can:</p> <p>Available tiers</p> <ul> <li>Free :     Available as part of your Azure subscription, this tier is limited to assessments and recommendations of Azure resources only.</li> <li>Standard :     This tier provides a full suite of security-related services including continuous monitoring, threat detection, just-in-time access control for ports, and more.</li> </ul>"},{"location":"cloud/azure/#identity-and-access","title":"Identity and access","text":"<p>Authentication and authorization</p> <ul> <li>Authentication is the process of establishing the identity of a person or service looking to access a resource. It involves the act of challenging a party for legitimate credentials, and provides the basis for creating a security principal for identity and access control use. It establishes if they are who they say they are.</li> <li>Authorization is the process of establishing what level of access an authenticated person or service has. It specifies what data they're allowed to access and what they can do with it.</li> </ul> <p>What is Azure Active Directory?</p> <p>Azure AD is a cloud-based identity service. It has built-in support for synchronizing with your existing on-premises Active Directory or can be used stand-alone.</p> <p>Azure AD provides services such as:</p> <ul> <li>Authentication.</li> <li>Single-Sign-On (SSO).</li> <li>Business to business (B2B) identity services.</li> <li>Device Management.</li> </ul> <p>Single sign-on</p> <p>Multi-factor authentication</p> <ul> <li>Something you know : password.</li> <li>Something you possess : mobile app.</li> <li>Something you are : biometric property.</li> </ul> <p>Providing identities to services</p> <p>It's usually valuable for services to have identities. Often, and against best practices, credential information is embedded in configuration files. With no security around these configuration files, anyone with access to the systems or repositories can access these credentials and risk exposure.</p> <ul> <li>Service principals :     An identity is just a thing that can be authenticated.     A principal is an identity acting with certain roles or claims. Usually, it is not useful to consider identity and principal separately, but think of using 'sudo' on a Bash prompt in Linux or on Windows using \"run as Administrator.\"</li> <li>Managed identities for Azure services :     The creation of service principals can be a tedious process, and there are a lot of touch points that can make maintaining them difficult. Managed identities for Azure services are much easier and will do most of the work for you.</li> </ul> <p>Role-based access control</p> <p>Roles are sets of permissions, like \"Read-only\" or \"Contributor\", that users can be granted to access an Azure service instance.</p> <p>Identities are mapped to roles directly or through group membership. Separating security principals, access permissions, and resources provides simple access management and fine-grained control. Administrators are able to ensure the minimum necessary permissions are granted.</p> <p></p>"},{"location":"cloud/azure/#encryption","title":"Encryption","text":"<p>What is encryption?</p> <ul> <li>Symmetric encryption uses the same key to encrypt and decrypt the data.</li> <li>Asymmetric encryption uses a public key and private key pair.</li> </ul> <p>Encryption at rest</p> <p></p> <p>Encryption in transit</p> <p></p> <p>Encryption on Azure</p> <ul> <li>Encrypt raw storage : Azure Storage Service Encryption.</li> <li>Encrypt virtual machine disks : Azure Disk Encryption.</li> <li>Encrypt databases : Transparent data encryption (TDE).</li> <li>Encrypt secrets : Azure Key Vault.</li> </ul>"},{"location":"cloud/azure/#overview-of-azure-certificates","title":"Overview of Azure certificates","text":"<p>Certificates used in Azure are x.509 v3 can be signed by a trusted certificate authority, or they can be self-signed.</p> <p>Types of certificates</p> <ul> <li>Service certificates are used for cloud services.</li> <li>Management certificates are used for authenticating with the management API.</li> </ul> <p>Using Azure Key Vault with certificates</p> <p>You can store your certificates in Azure Key Vault - much like any other secret. However, Key Vault provides additional features above and beyond the typical certificate management.</p>"},{"location":"cloud/azure/#protect-your-network","title":"Protect your network","text":"<ul> <li>Azure Firewall is a managed, cloud-based, network security service that protects your Azure Virtual Network.</li> <li>Azure Application Gateway is a load balancer that includes a Web Application Firewall (WAF) that provides protection from common, known vulnerabilities in sites.</li> <li>Network virtual appliances (NVAs)are ideal options for non-HTTP services or advanced configurations, and are similar to hardware firewall appliances.</li> </ul>"},{"location":"cloud/azure/#protect-your-shared-documents","title":"Protect your shared documents","text":"<p>Microsoft Azure Information Protection (sometimes referred to as AIP) is a cloud-based solution that helps organizations classify and optionally protect documents and emails by applying labels.</p>"},{"location":"cloud/azure/#azure-advanced-threat-protection","title":"Azure Advanced Threat Protection","text":"<p>Azure Advanced Threat Protection (Azure ATP) is a cloud-based security solution that identifies, detects, and helps you investigate advanced threats, compromised identities, and malicious insider actions directed at your organization.</p>"},{"location":"cloud/azure/#apply-and-monitor-infrastructure-standards-with-azure-policy","title":"Apply and monitor infrastructure standards with Azure Policy","text":""},{"location":"cloud/azure/#define-it-compliance-with-azure-policy","title":"Define IT compliance with Azure Policy","text":"<p>Planning out a consistent cloud infrastructure starts with setting up policy. Your policies will enforce your rules for created resources, so your infrastructure stays compliant with your corporate standards, cost requirements, and service-level agreements (SLAs) you have with your customers.  </p> <p>Azure Policy is an Azure service you use to create, assign and, manage policies. These policies enforce different rules and effects over your resources so that those resources stay compliant with your corporate standards and service level agreements.</p> <p>Creating a policy</p> <ul> <li>Create a policy definition.</li> <li>Assign a definition to a scope of resources.</li> <li>View policy evaluation results.</li> </ul>"},{"location":"cloud/azure/#organize-policy-with-initiatives","title":"Organize policy with initiatives","text":"<p>Managing a few policy definitions is easy, but once you have more than a few, you will want to organize them. That's where initiatives come in.</p> <p>Initiatives work alongside policies in Azure Policy. An initiative definition is a set or group of policy definitions to help track your compliance state for a larger goal. Even if you have a single policy, we recommend using initiatives if you anticipate increasing the number of policies over time.</p>"},{"location":"cloud/azure/#enterprise-governance-management","title":"Enterprise governance management","text":"<p>Access management occurs at the Azure subscription level. This allows an organization to configure each division of the company in a specific fashion based on their responsibilities and requirements. Planning and keeping rules consistent across subscriptions can be challenging without a little help.</p> <p>Azure Management Groups are containers for managing access, policies, and compliance across multiple Azure subscriptions.</p> <p></p>"},{"location":"cloud/azure/#define-standard-resources-with-azure-blueprints","title":"Define standard resources with Azure Blueprints","text":"<p>Adhering to security or compliance requirements, whether government or industry requirements, can be difficult and time-consuming. To help you with auditing, traceability, and compliance with your deployments, use Azure Blueprint artifacts and tools.</p>"},{"location":"cloud/azure/#explore-your-service-compliance-with-compliance-manager","title":"Explore your service compliance with Compliance Manager","text":"<p>Governing your own resources and how they are used is only part of the solution when using a cloud provider. You also have to understand how the provider manages the underlying resources you are building on.</p> <p>Microsoft takes this management very seriously and provides full transparency with four sources:</p> <ul> <li>Microsoft Privacy Statement.</li> <li>Microsoft Trust Center.</li> <li>Service Trust Portal.</li> <li>Compliance Manager.</li> </ul>"},{"location":"cloud/azure/#monitor-your-service-health","title":"Monitor your service health","text":"<p>Defining policy and access provides fine-grained control over resources in your cloud IT infrastructure. Once those resources are deployed, you will want to know about any issues or performance problems they might encounter.</p> <p>Azure provides two primary services to monitor the health of your apps and resources.</p> <ul> <li>Azure Monitor.</li> <li>Azure Service Health.</li> </ul> <p>Azure Monitor</p> <p>Azure Monitor maximizes the availability and performance of your applications by delivering a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments.</p> <p>Azure Service Health</p> <p>Azure Service Health is a suite of experiences that provide personalized guidance and support when issues with Azure services affect you. It can notify you, help you understand the impact of issues, and keep you updated as the issue is resolved. Azure Service Health can also help you prepare for planned maintenance and changes that could affect the availability of your resources.</p>"},{"location":"cloud/azure/#control-and-organize-azure-resources-with-azure-resource-manager","title":"Control and organize Azure resources with Azure Resource Manager","text":""},{"location":"cloud/azure/#principles-of-resource-groups","title":"Principles of resource groups","text":"<p>What are resource groups?</p> <p>Resource groups are a fundamental element of the Azure platform. A resource group is a logical container for resources deployed on Azure. These resources are anything you create in an Azure subscription like virtual machines, Application Gateways, and CosmosDB instances. All resources must be in a resource group and a resource can only be a member of a single resource group. Many resources can be moved between resource groups with some services having specific limitations or requirements to move. Resource groups can't be nested. Before any resource can be provisioned, you need a resource group for it to be placed in.</p> <p>Lifecycle If you delete a resource group, all resources contained within are also deleted. Organizing resources by lifecycle can be useful in non-production environments, where you might try an experiment, but then dispose of it when done. Resource groups make it easy to remove a set of resources at once.</p> <p>Authorization Resource groups are also a scope for applying role-based access control (RBAC) permissions. By applying RBAC permissions to a resource group, you can ease administration and limit access to allow only what is needed.</p> <p>Organizing principles</p> <p>Core infrastructure : </p> <p>Environments : </p> <p>Departments : </p> <p>Combinaison : </p>"},{"location":"cloud/azure/#use-tagging-to-organize-resources","title":"Use tagging to organize resources","text":"<p>What are tags?</p> <p>Tags are name/value pairs of text data that you can apply to resources and resource groups. Tags allow you to associate custom details about your resource, in addition to the standard Azure properties a resource has:</p> <ul> <li>Department (like finance, marketing, and more).</li> <li>Environment (prod, test, dev).</li> <li>Cost center.</li> <li>Lifecycle and automation (like shutdown and startup of virtual machines).</li> </ul>"},{"location":"cloud/azure/#use-resource-locks-to-protect-resources","title":"Use resource locks to protect resources","text":"<p>What are resource locks?</p> <p>Resource locks are a setting that can be applied to any resource to block modification or deletion. Resource locks can set to either Delete or Read-only. Delete will allow all operations against the resource but block the ability to delete it. Read-only will only allow read activities to be performed against it, blocking any modification or deletion of the resource. Resource locks can be applied to subscriptions, resource groups, and to individual resources, and are inherited when applied at higher levels.</p>"},{"location":"cloud/azure/#predict-costs-and-optimize-spending-for-azure","title":"Predict costs and optimize spending for Azure","text":""},{"location":"cloud/azure/#purchase-azure-products-and-services","title":"Purchase Azure products and services","text":"<p>Usage meters When you provision an Azure resource, Azure creates one or more meter instances for that resource. The meters track the resources' usage, and generate a usage record that is used to calculate your bill.</p> <p>For example, a single virtual machine that you provision in Azure might have the following meters tracking its usage:</p> <ul> <li>Compute Hours.</li> <li>IP Address Hours.</li> <li>Data Transfer In.</li> <li>Data Transfer Out.</li> <li>Standard Managed Disk.</li> <li>Standard Managed Disk Operations.</li> <li>Standard IO-Disk.</li> <li>Standard IO-Block Blob Read.</li> <li>Standard IO-Block Blob Write.</li> <li>Standard IO-Block Blob Delete.</li> </ul> <p>The meters and pricing vary per product and often have different pricing tiers based on the size or capacity of the resource. Check the documentation for specific details on what each service area costs.</p>"},{"location":"cloud/azure/#factors-affecting-costs","title":"Factors affecting costs","text":"<p>Just like your on-premises equipment costs, there are several elements that will affect your monthly costs when using Azure services. Let's look at a few of the primary factors including resource type, services, the user's location, and the billing zone.</p> <p>Resource type</p> <p>Costs are resource-specific, so the usage that a meter tracks and the number of meters associated with a resource depend on the resource type.</p> <p>Services</p> <p>Azure usage rates and billing periods can differ between Enterprise, Web Direct, and Cloud Solution Provider (CSP) customers. Some subscription types also include usage allowances, which affect costs.</p> <p>Location</p> <p>Azure has datacenters all over the world. Usage costs vary between locations that offer particular Azure products, services, and resources based on popularity, demand, and local infrastructure costs</p> <p>Azure billing zones</p> <p>Bandwidth refers to data moving in and out of Azure datacenters. Most of the time inbound data transfers (data going into Azure datacenters) are free. For outbound data transfers (data going out of Azure datacenters), the data transfer pricing is based on Billing Zones.</p>"},{"location":"cloud/azure/#estimate-costs-with-the-azure-pricing-calculator","title":"Estimate costs with the Azure pricing calculator","text":"<p>Introducing the Azure pricing calculator</p> <p>To make estimates easy for customers to create, Microsoft developed the Azure pricing calculator. The Azure pricing calculator is a free web-based tool that allows you to input Azure services and modify properties and options of the services. It outputs the costs per service and total cost for the full estimate.</p> <p>The options that you can configure in the pricing calculator vary between products, but basic configuration options include:</p> <ul> <li>Region.</li> <li>Tier.</li> <li>Billing Options.</li> <li>Support Options.</li> <li>Programs and Offers.</li> <li>Azure Dev/Test Pricing.</li> </ul>"},{"location":"cloud/azure/#predict-and-optimize-with-cost-management-and-azure-advisor","title":"Predict and optimize with Cost Management and Azure Advisor","text":"<p>What is Azure Advisor?</p> <p>Azure Advisor is a free service built-into Azure that provides recommendations on high availability, security, performance, and cost. Advisor analyzes your deployed services and looks for ways to improve your environment across those four areas. We'll focus on the cost recommendations, but you'll want to take some time to review the other recommendations as well.</p> <p>Advisor makes cost recommendations in the following areas:</p> <ul> <li>Reduce costs by eliminating unprovisioned Azure ExpressRoute circuits.</li> <li>Buy reserved instances to save money over pay-as-you-go.</li> <li>Right-size or shutdown underutilized virtual machines.</li> </ul>"},{"location":"cloud/azure/#estimate-the-total-cost-of-ownership-with-the-azure-tco-calculator","title":"Estimate the Total Cost of Ownership with the Azure TCO calculator","text":"<p>The pricing calculator and cost management advisor can help you predict and analyze your spend for new or existing services.</p> <p>If you are starting to migrate to the cloud, a useful tool you can use to predict your cost savings is the Total Cost of Ownership (TCO) calculator. To use the TCO calculator, you need to complete four steps.</p>"},{"location":"cloud/azure/#save-on-infrastructure-costs","title":"Save on infrastructure costs","text":"<ul> <li>Use Azure credits.</li> <li>Use spending limits.</li> <li>Use reserved instances.</li> <li>Choose low-cost locations and regions.</li> <li>Research available cost-saving offers.</li> <li>Right-size underutilized virtual machines.</li> <li>Deallocate virtual machines in off hours.</li> <li>Delete unused virtual machines.</li> <li>Migrate to PaaS or SaaS services.</li> </ul>"},{"location":"cloud/azure/#azure-ressources","title":"Azure ressources","text":""},{"location":"cloud/azure/#organiser-vos-ressources-azure","title":"Organiser vos ressources Azure","text":"<ul> <li>Groupes d\u2019administration : Ces groupes sont des conteneurs qui vous permettent de g\u00e9rer plus facilement l\u2019acc\u00e8s, la strat\u00e9gie et la conformit\u00e9 pour plusieurs abonnements. Tous les abonnements dans un groupe d\u2019administration h\u00e9ritent automatiquement des conditions appliqu\u00e9es \u00e0 ce groupe d\u2019administration.</li> <li>Abonnements : Un abonnement regroupe les comptes d\u2019utilisateur et les ressources qui ont \u00e9t\u00e9 cr\u00e9\u00e9es par ces derniers. Chaque abonnement a des limites ou quotas sur la quantit\u00e9 de ressources que vous pouvez cr\u00e9er et utiliser. Les organisations peuvent utiliser des abonnements pour g\u00e9rer les co\u00fbts et les ressources qui sont cr\u00e9\u00e9es par les utilisateurs, les \u00e9quipes ou les projets.</li> <li>Groupes de ressources : Un groupe de ressources est un conteneur logique dans lequel les ressources Azure comme les applications web, les bases de donn\u00e9es et les comptes de stockage sont d\u00e9ploy\u00e9es et g\u00e9r\u00e9es.</li> <li>Ressources : les ressources sont des instances de services que vous cr\u00e9ez, telles que des machines virtuelles, un stockage ou des bases de donn\u00e9es SQL.</li> </ul>"},{"location":"cloud/azure/#gerer-vos-couts-avec-azure-cost-management","title":"G\u00e9rer vos co\u00fbts avec Azure Cost Management","text":"<p>Azure Cost Management fournit quelques fonctionnalit\u00e9s utiles pour pr\u00e9voir et g\u00e9rer les co\u00fbts :</p> <ul> <li>Analyser les co\u00fbts du cloud vous aide \u00e0 examiner et \u00e0 analyser tous vos co\u00fbts. Vous pouvez afficher le co\u00fbt agr\u00e9g\u00e9 pour votre compte ou tous les co\u00fbts cumul\u00e9s au fil du temps.</li> <li>Superviser avec les budgets vous permet de cr\u00e9er un budget et de configurer des alertes qui vous avertissent quand vous \u00eates sur le point de le d\u00e9passer.</li> <li>Optimiser avec des recommandations vous aide \u00e0 identifier les ressources inutilis\u00e9es et sous-exploit\u00e9es afin de r\u00e9duire le gaspillage.</li> <li>G\u00e9rer les factures et les paiements vous donne une visibilit\u00e9 de vos investissements dans le cloud.</li> </ul>"},{"location":"cloud/azure/#gouvernance-securite-et-conformite-dans-azure","title":"Gouvernance, s\u00e9curit\u00e9 et conformit\u00e9 dans Azure","text":"<p>Azure Blueprints Azure Blueprints permet aux architectes cloud et aux groupes centraux responsables des technologies de l\u2019information de d\u00e9finir un ensemble reproductible de ressources Azure qui impl\u00e9mentent et respectent les normes, mod\u00e8les et exigences d\u2019une organisation. Azure Blueprints permet aux \u00e9quipes de d\u00e9veloppement de cr\u00e9er et mettre en place rapidement de nouveaux environnements et d\u2019avoir confiance en leur conformit\u00e9 aux exigences de l\u2019organisation \u00e0 l\u2019aide d\u2019un ensemble de composants int\u00e9gr\u00e9s, comme la mise en r\u00e9seau, visant \u00e0 acc\u00e9l\u00e9rer le d\u00e9veloppement et la livraison.</p> <p>Azure Policy Azure Policy est un service utilis\u00e9 pour cr\u00e9er, attribuer et g\u00e9rer des strat\u00e9gies. Ces strat\u00e9gies appliquent des r\u00e8gles \u00e0 vos ressources afin qu\u2019elles restent conformes aux standards et aux contrats de niveau de service de l\u2019entreprise. Azure Policy analyse vos ressources pour identifier celles qui ne sont pas conformes aux strat\u00e9gies que vous impl\u00e9mentez. Par exemple, vous pouvez disposer d\u2019une strat\u00e9gie qui n\u2019autorise que les machines virtuelles d\u2019une certaine taille \u00e0 s\u2019ex\u00e9cuter dans votre environnement. Une fois impl\u00e9ment\u00e9e, cette strat\u00e9gie \u00e9value les machines virtuelles existantes dans votre environnement ainsi que toutes les nouvelles machines virtuelles qui y sont d\u00e9ploy\u00e9es. L\u2019\u00e9valuation de la strat\u00e9gie g\u00e9n\u00e8re des \u00e9v\u00e9nements de conformit\u00e9 que vous pouvez utiliser \u00e0 des fins de supervision et de cr\u00e9ation de rapports.</p> <p>Centre de s\u00e9curit\u00e9 Azure Azure Security Center joue un r\u00f4le important dans votre strat\u00e9gie de gouvernance. Il vous aide \u00e0 assurer un haut niveau de s\u00e9curit\u00e9, car il :</p> <ul> <li>Fournit une vue unifi\u00e9e de la s\u00e9curit\u00e9 sur l\u2019ensemble de vos charges de travail.</li> <li>Collecte, explore et analyse les donn\u00e9es de s\u00e9curit\u00e9 \u00e0 partir d\u2019un large \u00e9ventail de sources, dont les pare-feu et d\u2019autres solutions partenaires.</li> <li>Fournit des recommandations de s\u00e9curit\u00e9 actionnables pour corriger les probl\u00e8mes avant qu\u2019ils ne soient exploit\u00e9s de fa\u00e7on malveillante.</li> <li>Peut \u00eatre utilis\u00e9 pour appliquer des strat\u00e9gies de s\u00e9curit\u00e9 sur l\u2019ensemble de vos charges de travail cloud hybrides pour garantir la conformit\u00e9 aux normes de s\u00e9curit\u00e9.</li> </ul> <p>La plupart des fonctionnalit\u00e9s de s\u00e9curit\u00e9, telles que la strat\u00e9gie de s\u00e9curit\u00e9 et les recommandations, sont disponibles gratuitement. Certaines des fonctionnalit\u00e9s plus avanc\u00e9es, telles que l\u2019acc\u00e8s aux machines virtuelles juste-\u00e0-temps et la prise en charge des charges de travail hybrides, sont disponibles au niveau standard de Security Center. L\u2019acc\u00e8s aux machines virtuelles juste-\u00e0-temps peut aider \u00e0 r\u00e9duire la surface d\u2019attaque du r\u00e9seau en contr\u00f4lant l\u2019acc\u00e8s aux ports de gestion sur les machines virtuelles Azure</p>"},{"location":"cloud/azure/#supervision-et-creation-de-rapports-dans-azure","title":"Supervision et cr\u00e9ation de rapports dans Azure","text":"<p>Azure Monitor Azure Monitor fournit un seul hub unifi\u00e9 pour toutes les donn\u00e9es de supervision et de diagnostic dans Azure. Vous pouvez l\u2019utiliser pour obtenir une visibilit\u00e9 sur l\u2019ensemble de vos ressources. Gr\u00e2ce \u00e0 Azure Monitor, vous pouvez rechercher et r\u00e9soudre les probl\u00e8mes et optimiser le niveau de performance. Vous pouvez \u00e9galement comprendre le comportement des clients.</p> <p>Azure Service Health Azure Service Health fournit un affichage personnalis\u00e9 de l\u2019int\u00e9grit\u00e9 des services et r\u00e9gions Azure que vous utilisez. Service Health publie des informations sur les probl\u00e8mes actuels qui sont utiles pour comprendre l\u2019impact sur vos ressources. Des mises \u00e0 jour r\u00e9guli\u00e8res vous tiennent inform\u00e9 de la r\u00e9solution des probl\u00e8mes.</p> <p>Azure Advisor Azure Advisor est un conseiller personnalis\u00e9 gratuit bas\u00e9 dans le cloud qui vous aide \u00e0 suivre et \u00e0 impl\u00e9menter de bonnes pratiques pour les d\u00e9ploiements Azure. Il analyse la configuration de vos ressources et les donn\u00e9es de t\u00e9l\u00e9m\u00e9trie d\u2019utilisation et sugg\u00e8re des solutions qui peuvent vous aider \u00e0 optimiser votre environnement. Les recommandations sont divis\u00e9es en quatre cat\u00e9gories :</p> <ul> <li>Haute disponibilit\u00e9 : vous aide \u00e0 am\u00e9liorer la continuit\u00e9 de vos applications strat\u00e9giques. Les recommandations peuvent inclure l\u2019ajout de machines virtuelles \u00e0 un groupe \u00e0 haute disponibilit\u00e9 ou l\u2019ajout de points de terminaison g\u00e9oredondants.</li> <li>S\u00e9curit\u00e9 : permet de d\u00e9tecter les menaces et vuln\u00e9rabilit\u00e9s pouvant conduire \u00e0 des failles de s\u00e9curit\u00e9. Les recommandations peuvent inclure l\u2019application du chiffrement des disques ou l\u2019activation de groupes de s\u00e9curit\u00e9 r\u00e9seau.</li> <li>Performances : pour am\u00e9liorer la vitesse de vos applications. Les recommandations peuvent inclure l\u2019am\u00e9lioration des niveaux de performance des requ\u00eates SQL en cr\u00e9ant des index ou en reconfigurant vos param\u00e8tres de gestion du trafic.</li> <li>Co\u00fbt : pour optimiser et r\u00e9duire vos d\u00e9penses Azure globales. Les recommandations peuvent concerner le redimensionnement ou l\u2019arr\u00eat des machines virtuelles sous-exploit\u00e9es ou encore le transfert vers des r\u00e9servations Azure pour diminuer le co\u00fbt total de possession.</li> </ul> <p>Centre de s\u00e9curit\u00e9 Azure Azure Security Center joue \u00e9galement un r\u00f4le important dans votre strat\u00e9gie de supervision. Il peut vous aider \u00e0 superviser la s\u00e9curit\u00e9 de vos machines, r\u00e9seaux, syst\u00e8mes de stockage, services de donn\u00e9es et applications. Security Center fournit la d\u00e9tection avanc\u00e9e des menaces en utilisant l\u2019apprentissage automatique et l\u2019analytique comportementale pour aider \u00e0 identifier les menaces actives ciblant vos ressources Azure. Il met \u00e9galement \u00e0 disposition une protection contre les menaces qui bloque les programmes malveillants ou autres codes ind\u00e9sirables et r\u00e9duit la surface d\u2019exposition aux attaques par force brute et autres attaques r\u00e9seau.</p>"},{"location":"cloud/azure/#availability-in-azure","title":"Availability in Azure","text":"<p>Availability sets An availability set is a logical grouping of VMs within a datacenter that allows Azure to understand how your application is built to provide for redundancy and availability.</p> <p>Fault domains A fault domain is a logical group of underlying hardware that share a common power source and network switch, similar to a rack within an on-premises datacenter. As you create VMs within an availability set, the Azure platform automatically distributes your VMs across these fault domains. This approach limits the impact of potential physical hardware failures, network outages, or power interruptions.</p> <p>Update domains An update domain is a logical group of underlying hardware that can undergo maintenance or be rebooted at the same time. As you create VMs within an availability set, the Azure platform automatically distributes your VMs across these update domains. This approach ensures that at least one instance of your application always remains running as the Azure platform undergoes periodic maintenance. The order of update domains being rebooted may not proceed sequentially during planned maintenance, but only one update domain is rebooted at a time.</p>"},{"location":"cloud/azure/#source","title":"Source","text":"<p>Azure fundamentals</p>"},{"location":"cloud/ovh/","title":"Ovh","text":""},{"location":"cloud/ovh/#ovh","title":"Ovh","text":""},{"location":"cloud/ovh/#tools-versions","title":"Tools versions","text":"Os / Tool Version Sles 15sp6 Ovh Cloud Public"},{"location":"cloud/ovh/#intro-ovh","title":"Intro Ovh","text":"<p>En tant que leader europ\u00e9en du cloud, nous fournissons des solutions de cloud public et priv\u00e9, d\u2019h\u00e9bergement mutualis\u00e9 et de serveurs d\u00e9di\u00e9s dans 140 pays \u00e0 travers le globe. Nous proposons \u00e9galement \u00e0 nos clientes et clients l\u2019enregistrement de noms de domaine, de la t\u00e9l\u00e9phonie, ainsi que de l\u2019acc\u00e8s \u00e0 internet. Cr\u00e9\u00e9 en 1999, OVHcloud est une entreprise fran\u00e7aise pr\u00e9sente dans le monde entier, gr\u00e2ce \u00e0 la localisation internationale de ses datacenters et points de pr\u00e9sence.</p> <p></p>"},{"location":"cloud/ovh/#public-cloud-produit","title":"Public Cloud produit","text":""},{"location":"cloud/ovh/#moscow","title":"MoSCoW","text":""},{"location":"cloud/ovh/#produits","title":"Produits","text":"Produits MoSCoW (D.C) MoSCoW (A.V.P.G) Compute M M Storage M M Network M M Containers &amp; Orchestration M M Databases W C Analytics C S Data Platform W W AI &amp; Machine learning S M Idam M M"},{"location":"cloud/ovh/#services","title":"Services","text":"Produits Services Options MoSCoW (D.C) MoSCoW (A.V.P.G) Compute Virtual Machine Instances General Purpose M M Compute Optimized W M Memory Optimized W M Storage Optimized C M Discovery M M Cloud GPU C M Metal Instances W C Instance Backup C C Private Image catalog M M Public Image Catalog S S Storage Block Storage Block Storage M M Volume snapshot C S Volume Backup C S Object Storage Standard M M Standard 3-AZ W C High Performance C S Standard (SWIFT) W C Cloud Archive W C Cold Archive W W Network Private Network M M Load Balancer M M Floating IP M M Gateway M M Instance Public Traffic M M Anti-DDoS M M Containers &amp; Orchestration Managed Rancher Service W C Managed Kubernetes Service M M Managed Private Registry M M Load Balancer for M.K.S M M Databases MySQL Essential Plan W W Business Plan W W Enterprise Plan W W PostgreSQL Essential Plan W C Business Plan W W Enterprise Plan W W MongoDB Discovery W W Production W W Advanced W W Managed Caching Essential Plan W W Business Plan W W Cassandra Essential Plan W W Business Plan W W Enterprise Plan W W M3DB Essential Plan W W Business Plan W W Enterprise Plan W W M3 Aggregator Essential Plan W W Business Plan W W Enterprise Plan W W Analytics Data Processing W W Kafka Business W W Kafka Enterprise W W OpenSearch Essential W W OpenSearch Business W W OpenSearch Enterprise W W Kafka MirrorMaker Essential W W Kafka MirrorMaker Business W W Kafka MirrorMaker Enterprise W W Kafka Connect Essential W W Kafka Connect Business W W Kafka Connect Enterprise W W Managed Dashboards (Grafana) Essential C S Logs Data Platform Standard C S Entreprise W W Data Platform Lakehouse Manager W W Data Catalog W W Data Processing Engine W W Analytics Manager W W Applications Services W W Control Center W W AI &amp; Machine learning AI Notebooks &amp; Quantum Notebooks Unit\u00e9s CPU M M Unit\u00e9s GPU M M AI Training Unit\u00e9s CPU C S Unit\u00e9s GPU C S AI Deploy Unit\u00e9s CPU W S Unit\u00e9s GPU W S Lettria W W Voxist W W NVIDIA NGC Platform C S AI Endpoints (Beta) W C Idam IAM M M Logs Data Platform C M Key Management Service (KMS) C S Service Logs C M"},{"location":"cloud/ovh/#public-cloud-deep-dive","title":"Public Cloud deep dive","text":""},{"location":"cloud/ovh/#compute","title":"Compute","text":""},{"location":"cloud/ovh/#description","title":"Description","text":"<p>Le cloud computing est un mod\u00e8le de fourniture de services informatiques qui permet aux utilisateurs d\u2019acc\u00e9der \u00e0 des ressources informatiques (comme des serveurs, du stockage, des bases de donn\u00e9es, des applications et des services r\u00e9seau) via internet. Ces services sont factur\u00e9s \u00e0 la demande et \u00e0 l\u2019usage. Ce mod\u00e8le informatique offre une flexibilit\u00e9, une scalabilit\u00e9 et une efficacit\u00e9 consid\u00e9rables en \u00e9liminant la n\u00e9cessit\u00e9 pour les entreprises de g\u00e9rer leur propre infrastructure physique.</p>"},{"location":"cloud/ovh/#services_1","title":"Services","text":""},{"location":"cloud/ovh/#virtual-machine-instances","title":"Virtual Machine Instances","text":"<p>Un large choix d\u2019instances adapt\u00e9es \u00e0 tous vos usages:</p> <ul> <li>General Purpose: Ces instances offrent \u00e0 vos serveurs de d\u00e9veloppement et vos applications web ou d'entreprise des ressources CPU/RAM \u00e9quilibr\u00e9es. Les vCores sont cadenc\u00e9s \u00e0 2 GHz et plus.</li> <li>Compute Optimized: Ces instances sont id\u00e9ales pour les applications n\u00e9cessitant des fr\u00e9quences de calcul importantes ou de la parall\u00e9lisation de t\u00e2ches. Les vCores sont cadenc\u00e9s \u00e0 2,3 GHz et plus.</li> <li>Memory Optimized: Ces instances sont pens\u00e9es pour les usages d'analyse des donn\u00e9es et la datascience gr\u00e2ce \u00e0 leurs ratios CPU/RAM optimis\u00e9s et des IOPS acc\u00e9l\u00e9r\u00e9es. Les vCores sont cadenc\u00e9s \u00e0 2 GHz et plus.</li> <li>Storage Optimized: Profitez d'IOPS ultrarapides gr\u00e2ce \u00e0 des cartes NVMe sp\u00e9cialement con\u00e7ues pour les bases de donn\u00e9es et les applications big data</li> <li>Discovery: D\u00e9marrez l\u2019exp\u00e9rience Public Cloud avec des instances dont les ressources sont partag\u00e9es, qui fourniront des performances stables \u00e0 un prix tr\u00e8s accessible.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro","title":"Pro","text":"<ul> <li>La base du cloud computing en mode IaaS.</li> <li>Peut permettre une approche Lift and Shift.</li> <li>Facile d'acc\u00e8s.</li> </ul>"},{"location":"cloud/ovh/#cons","title":"Cons","text":"<ul> <li>Il n'y a que l'infra sous jacente qui est manag\u00e9.</li> <li>Il faut faire la M.C.O.</li> </ul> <p>Notes</p> <ul> <li>Garantie 99.99 % sauf pour Discovery</li> </ul>"},{"location":"cloud/ovh/#cloud-gpu","title":"Cloud GPU","text":"<p>Des serveurs cloud taill\u00e9s pour le traitement de l'IA, des graphismes et des t\u00e2ches massivement parall\u00e8les.</p> <ul> <li>Solutions Cloud GPU sur mesure.</li> <li>Prix transparents et comp\u00e9titifs.</li> <li>Durabilit\u00e9 et efficacit\u00e9.</li> <li>S\u00e9curit\u00e9 des donn\u00e9es et conformit\u00e9.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_1","title":"Pro","text":"<ul> <li>La base du cloud computing en mode IaaS.</li> <li>Acc\u00e8s \u00e0 un GPU.</li> </ul>"},{"location":"cloud/ovh/#cons_1","title":"Cons","text":"<ul> <li>Il n'y a que l'infra sous jacente qui est manag\u00e9.</li> <li>Il faut faire la M.C.O.</li> </ul>"},{"location":"cloud/ovh/#metal-instances","title":"Metal Instances","text":"<p>Combinez les avantages du Bare Metal avec l\u2019automatisation du cloud.</p> <ul> <li>Les avantages du serveur d\u00e9di\u00e9\u2026</li> <li>\u2026combin\u00e9s avec l\u2019automatisation du cloud.</li> <li>Une ma\u00eetrise des co\u00fbts assur\u00e9e.</li> <li>S\u00e9curit\u00e9 des donn\u00e9es et conformit\u00e9.</li> </ul>"},{"location":"cloud/ovh/#instance-backup","title":"Instance Backup","text":"<p>Des sauvegardes syst\u00e8me \u00e0 la demande.</p> <p>\u00c0 l\u2019heure des d\u00e9ploiements automatis\u00e9s et de \u00ab l\u2019infrastructure as code \u00bb, de nombreuses situations peuvent n\u00e9cessiter une sauvegarde de votre syst\u00e8me. Vos instances peuvent \u00eatre sauvegard\u00e9es \u00e0 tout moment. La machine r\u00e9alisera un export du disque de l\u2019instance. Gr\u00e2ce \u00e0 cela, vous pouvez donc industrialiser vos d\u00e9ploiements.</p>"},{"location":"cloud/ovh/#private-image-catalog","title":"Private Image catalog","text":"<p>Vos syst\u00e8mes d'exploitation rassembl\u00e9s et disponibles pour vos instances.</p> <p>En plus du Public Image Catalog fournissant des images syst\u00e8me maintenues par OVHcloud, vous pouvez b\u00e2tir un Private Image Catalog pour mettre \u00e0 disposition de vos instances des syst\u00e8mes d'exploitation sp\u00e9cifiquement param\u00e9tr\u00e9s pour vos usages ou des appliances fournis par un \u00e9diteur par exemple.</p>"},{"location":"cloud/ovh/#pro_2","title":"Pro","text":"<ul> <li>Permettrait de d\u00e9ployer les ova/qcow2 de nos instances.</li> </ul>"},{"location":"cloud/ovh/#cons_2","title":"Cons","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#public-image-catalog","title":"Public Image Catalog","text":"<p>Les images syst\u00e8me et applications pr\u00e9-install\u00e9es les plus populaires.</p> <p>Afin de d\u00e9marrer au plus vite votre infrastructure, OVHcloud fournit les images cloud standards du march\u00e9, ainsi que les applications pr\u00e9-install\u00e9es les plus populaires.</p>"},{"location":"cloud/ovh/#storage","title":"Storage","text":""},{"location":"cloud/ovh/#description_1","title":"Description","text":"<p>Le stockage cloud est un service qui permet de sauvegarder des donn\u00e9es sur des serveurs distants accessibles via internet. Il offre la possibilit\u00e9 de stocker, g\u00e9rer et consulter des fichiers et des donn\u00e9es depuis n\u2019importe quel appareil connect\u00e9 \u00e0 internet, \u00e0 tout moment. Ce type de stockage est g\u00e9r\u00e9 par des fournisseurs de services qui maintiennent l\u2019infrastructure et garantissent la s\u00e9curit\u00e9 et la disponibilit\u00e9 des donn\u00e9es. Le stockage cloud est souvent utilis\u00e9 pour la sauvegarde de donn\u00e9es, le partage de fichiers et la collaboration en ligne. Il repr\u00e9sente une alternative flexible et \u00e9volutive au stockage traditionnel sur disque dur, sur des dispositifs de stockage locaux ou sur des serveurs d\u00e9di\u00e9s</p> <p>Notes</p> <ul> <li>Les options des service block et object storage d\u00e9pendent de la r\u00e9gion.</li> <li>Globalement l'object storage est moins ch\u00e8re que le block storage.</li> </ul>"},{"location":"cloud/ovh/#services_2","title":"Services","text":""},{"location":"cloud/ovh/#block-storage","title":"Block Storage","text":"<p>Block Storage est une solution de stockage de donn\u00e9es par blocs rendue possible gr\u00e2ce \u00e0 une m\u00e9thode divisant les contenus en blocs de donn\u00e9es de taille fixe. Ces derni\u00e8res sont stock\u00e9es sur des disques attach\u00e9s \u00e0 vos instances dans des baies de stockage. Vos informations sont r\u00e9parties sur des disques ind\u00e9pendants de votre machine. Chaque bloc de donn\u00e9es est trait\u00e9 comme une unit\u00e9 distincte et re\u00e7oit un identifiant unique. Ainsi, vous contr\u00f4lez votre capacit\u00e9 de stockage par l\u2019ajout de disques selon vos besoins, tout en ma\u00eetrisant la s\u00e9curit\u00e9 et les performances de votre infrastructure.</p> <ul> <li>S\u00e9curis\u00e9 par la r\u00e9plication.</li> <li>Bas\u00e9 sur Ceph.</li> <li>Performance et scalabilit\u00e9 au meilleur co\u00fbt.</li> </ul>"},{"location":"cloud/ovh/#pro_3","title":"Pro","text":"<ul> <li>Int\u00e9r\u00e9ssant pour un acc\u00e8s rapide \u00e0 la donn\u00e9es.</li> <li>Backupable et snapshotable.</li> </ul>"},{"location":"cloud/ovh/#cons_3","title":"Cons","text":"<ul> <li>Pas de client S3.</li> <li>Pas d'acc\u00e8s direct (A confirmer).</li> </ul>"},{"location":"cloud/ovh/#object-storage","title":"Object Storage","text":"<p>Il s\u2019agit d\u2019une technologie proposant un service de stockage en ligne, accessible via API et permettant de stocker des donn\u00e9es sans limite d\u2019espace, le cluster d\u2019Object Storage \u00e9tant g\u00e9r\u00e9 par le fournisseur de cloud. Il est possible de pousser tous types de donn\u00e9es dans un service de stockage d\u2019objets.</p> <ul> <li>Co\u00fbts de stockage ma\u00eetris\u00e9s et optimis\u00e9s.</li> <li>Solution r\u00e9versible et interop\u00e9rable.</li> <li>R\u00e9silience et haute disponibilit\u00e9 de vos donn\u00e9es.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_4","title":"Pro","text":"<ul> <li>Client S3.</li> <li>Acc\u00e8s direct au stockage.</li> </ul>"},{"location":"cloud/ovh/#cons_4","title":"Cons","text":"<ul> <li>Pas de backup si ce n'est la Cloud Archive.</li> <li>Moins performant que le block par nature.</li> </ul>"},{"location":"cloud/ovh/#cold-archive","title":"Cold Archive","text":"<ul> <li>Strat\u00e9gies de r\u00e9silience \u00e0 long terme.</li> <li>Service compatible S3.</li> <li>Archivage de donn\u00e9es critiques.</li> </ul>"},{"location":"cloud/ovh/#network","title":"Network","text":""},{"location":"cloud/ovh/#description_2","title":"Description","text":"<p>Un r\u00e9seau cloud est une infrastructure virtuelle permettant de connecter appareils, services et applications via internet en utilisant la puissance et la flexibilit\u00e9 des ressources cloud. Il repose sur des centres de donn\u00e9es distants pour offrir \u00e9volutivit\u00e9 et gestion centralis\u00e9e des ressources r\u00e9seau.</p>"},{"location":"cloud/ovh/#services_3","title":"Services","text":""},{"location":"cloud/ovh/#private-network","title":"Private Network","text":"<p>Une connexion priv\u00e9e et flexible entre vos instances.</p> <p></p> <ul> <li>R\u00e9seaux \u00e9tendus entre les localisations.</li> <li>Isolation.</li> <li>Extension aux autres services d\u2019OVHcloud.</li> <li>Certifications ISO/IEC 27001, 27701 et HDS.</li> </ul>"},{"location":"cloud/ovh/#pro_5","title":"Pro","text":"<ul> <li>Obligatoire pour toutes construction d'un vpc (virtual private cloud).</li> </ul>"},{"location":"cloud/ovh/#cons_5","title":"Cons","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#load-balancer","title":"Load Balancer","text":"<p>L\u2019OVHcloud Load Balancer vous permet d\u2019assurer plus facilement l\u2019\u00e9volutivit\u00e9, la haute disponibilit\u00e9 et la r\u00e9silience de vos applications. Pour ce faire, la charge de trafic est r\u00e9partie de mani\u00e8re dynamique entre plusieurs instances et r\u00e9gions. Am\u00e9liorez l\u2019exp\u00e9rience utilisateur en automatisant la gestion du trafic et de la charge, tout en ma\u00eetrisant les co\u00fbts. En combinant le Load Balancer et laFloating IP, vous pouvez cr\u00e9er un point d\u2019entr\u00e9e unique et s\u00e9curis\u00e9 pour votre application, tout en activant des sc\u00e9narios de basculement et en prot\u00e9geant vos ressources priv\u00e9es.</p> <ul> <li>D\u00e9ployable dans les r\u00e9gions.</li> <li>Connect\u00e9 aux r\u00e9seaux priv\u00e9s.</li> <li>Gestion simplifi\u00e9e.</li> <li>Charges de travail priv\u00e9es.</li> <li>Int\u00e9gration \u00e0 l\u2019\u00e9cosyst\u00e8me Public Cloud.</li> <li>Plusieurs protocoles de contr\u00f4le d'int\u00e9grit\u00e9.</li> <li>Chiffrement SSL/TLS.</li> <li>Compatible avec les instances Public Cloud.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_6","title":"Pro","text":"<ul> <li>Permet d'avoir un LB dans le vpc.</li> <li>Permet des d\u00e9ploiements blue green sur du legacy.</li> <li>Fait office de terminaison SSL.</li> </ul>"},{"location":"cloud/ovh/#cons_6","title":"Cons","text":"<ul> <li>A voir si obligatoire avec une gateway pour une mono-instance.</li> </ul>"},{"location":"cloud/ovh/#floating-ip","title":"Floating IP","text":"<p>Une Floating IP est une adresse IP publique et statique qui peut \u00eatre r\u00e9affect\u00e9e dynamiquement \u00e0 plusieurs appareils sur votre r\u00e9seau, ce qui facilite la haute disponibilit\u00e9, la tol\u00e9rance aux pannes et le basculement pour vos applications et services dans le cloud. Ce type d\u2019IP peut \u00eatre assign\u00e9 \u00e0 une instance comme un Load Balancer, puis rapidement r\u00e9assign\u00e9. Vous pouvez \u00e9galement anticiper et g\u00e9rer les allocations d\u2019IP publiques gr\u00e2ce \u00e0 l\u2019automatisation via API.</p> <ul> <li>Haute disponibilit\u00e9.</li> <li>Migration des environnements.</li> <li>Point d\u2019acc\u00e8s principal.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_7","title":"Pro","text":"<ul> <li>Obligatoire pour exposer un service.</li> </ul>"},{"location":"cloud/ovh/#cons_7","title":"Cons","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#gateway","title":"Gateway","text":"<p>Le service Gateway est le moyen le plus simple d\u2019assurer une connexion s\u00e9curis\u00e9e et \u00e9volutive entre une infrastructure vRack et un r\u00e9seau connect\u00e9 \u00e0 internet. Il permet un acc\u00e8s internet s\u00e9curis\u00e9 \u00e0 toutes vos instances, sans n\u00e9cessiter d'adresse IP publique s\u00e9par\u00e9e. Plusieurs offres sont disponibles, chacune disposant de capacit\u00e9s de bande passante diff\u00e9rentes pour r\u00e9pondre \u00e0 vos besoins sp\u00e9cifiques.</p> <ul> <li>Exposition flexible des services gr\u00e2ce aux Floating IP.</li> <li>Combiner un Load Balancer et des Floating IP.</li> <li>Trafic sortant vers internet.</li> </ul>"},{"location":"cloud/ovh/#pro_8","title":"Pro","text":"<ul> <li>Obligatoire pour exposer un service combinant LB &amp; Floating IP.</li> <li>Obligatoire pour avoir un traffic sorant vers internet.</li> </ul>"},{"location":"cloud/ovh/#cons_8","title":"Cons","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#instance-public-traffic","title":"Instance Public Traffic","text":"<p>Le trafic r\u00e9seau public sortant des instances est inclus dans le prix des instances sur toutes les localisations, except\u00e9 la r\u00e9gion Asie-Pacifique (Singapour, Sydney et Mumbai). Sur ces trois r\u00e9gions, 1 To/mois de trafic public sortant est inclus pour chaque projet Public Cloud. Au-del\u00e0 de ce quota, chaque Go de trafic suppl\u00e9mentaire est factur\u00e9. Le trafic r\u00e9seau entrant depuis le r\u00e9seau public est inclus dans tous les cas et dans toutes les r\u00e9gions.</p>"},{"location":"cloud/ovh/#anti-ddos","title":"Anti-DDoS","text":"<p>Profitez d'une protection permanente sur l'ensemble de vos ressources cloud, pour garantir un niveau de service optimal.</p>"},{"location":"cloud/ovh/#containers-orchestration","title":"Containers &amp; Orchestration","text":""},{"location":"cloud/ovh/#description_3","title":"Description","text":"<p>L'orchestration de conteneurs est un processus automatis\u00e9 qui permet de g\u00e9rer le d\u00e9ploiement, la mise \u00e0 l'\u00e9chelle et l'administration de conteneurs applicatifs sur des environnements de serveurs multiples. Cette technique facilite le d\u00e9ploiement d'applications complexes r\u00e9parties sur plusieurs conteneurs, assure leur fonctionnement optimal, et g\u00e8re les aspects tels que la mise \u00e0 jour sans interruption de service, la r\u00e9partition de la charge et la r\u00e9silience face aux pannes.</p>"},{"location":"cloud/ovh/#services_4","title":"Services","text":""},{"location":"cloud/ovh/#managed-rancher-service","title":"Managed Rancher Service","text":"<p>Unifiez l\u2019administration de vos clusters Kubernetes gr\u00e2ce \u00e0 un outil manag\u00e9 de gestion centralis\u00e9e.</p> <ul> <li>Versatilit\u00e9.</li> <li>S\u00e9curit\u00e9.</li> <li>Scalabilit\u00e9.</li> <li>Support.</li> </ul>"},{"location":"cloud/ovh/#pro_9","title":"Pro","text":"<ul> <li>La gestion du rancher est manag\u00e9.</li> <li>Maitrise du d\u00e9ploiement des clusters k8s.</li> <li>Possibilit\u00e9 de d\u00e9ployer en mode multi cloud et/ou on premise.</li> </ul>"},{"location":"cloud/ovh/#cons_9","title":"Cons","text":"<ul> <li>Cas d'usage particuli\u00e9 ou \u00e0 raffiner.</li> <li>Il faut avoir les comp\u00e9tences.</li> <li>Il faut faire la M.C.O.</li> </ul> <p>Notes</p> <ul> <li>Managed Rancher Service permet de faciliter la gestion de plusieurs clusters Kubernetes, dont ceux cr\u00e9\u00e9s avec Managed Kubernetes Service.</li> <li>Managed Kubernetes Service est un service manag\u00e9 pour g\u00e9rer vos applications conteneuris\u00e9es au sein d\u2019un m\u00eame cluster Kubernetes.</li> <li>Ces deux services sont compl\u00e9mentaires, mais l\u2019un ne n\u00e9cessite pas obligatoirement l\u2019utilisation de l\u2019autre.   Vous pouvez g\u00e9rer manuellement vos clusters Kubernetes cr\u00e9\u00e9s avec Managed Kubernetes Service sans utiliser Managed Rancher Service. \u00c0 l\u2019inverse, vous pouvez utiliser Managed Rancher Service pour g\u00e9rer des clusters Kubernetes autres que ceux cr\u00e9\u00e9s avec Managed Kubernetes Service, par exemple, des clusters Kubernetes sur des serveurs Bare Metal OVHcloud ou bien chez d\u2019autres fournisseurs de cloud (sur site ou non).</li> <li>Vous pouvez g\u00e9rer n\u2019importe quel cluster Kubernetes (chez OVHcloud, d\u2019autres fournisseurs de cloud ou sur site) via Managed Rancher Service.</li> </ul>"},{"location":"cloud/ovh/#managed-kubernetes-service","title":"Managed Kubernetes Service","text":"<p>Lib\u00e9rez-vous de l\u2019installation et de la maintenance de vos clusters Kubernetes et d\u00e9ployez rapidement vos applications gr\u00e2ce \u00e0 la solution d\u2019orchestration de conteneurs de r\u00e9f\u00e9rence.</p> <ul> <li>Gestion int\u00e9grale par OVHcloud.</li> <li>Interop\u00e9rabilit\u00e9.</li> <li>\u00c9cosyst\u00e8me de services.</li> <li>\u00c9volutivit\u00e9 et r\u00e9silience.</li> </ul>"},{"location":"cloud/ovh/#pro_10","title":"Pro","text":"<ul> <li>La gestion du cluster est manag\u00e9.</li> </ul>"},{"location":"cloud/ovh/#cons_10","title":"Cons","text":"<ul> <li>Limit\u00e9 \u00e0 ce que supporte le cloud op\u00e9rateur (CSI/CNI/Version de k8s).</li> </ul>"},{"location":"cloud/ovh/#managed-private-registry","title":"Managed Private Registry","text":"<p>Stockez, g\u00e9rez et acc\u00e9dez facilement \u00e0 vos images de conteneurs et Helm charts gr\u00e2ce \u00e0 ce service enti\u00e8rement g\u00e9r\u00e9.</p> <ul> <li>Interop\u00e9rabilit\u00e9 compl\u00e8te.</li> <li>S\u00e9curit\u00e9 maximale.</li> <li>Tarifs pr\u00e9visibles.</li> <li>Conformit\u00e9 en mati\u00e8re d'h\u00e9bergement de donn\u00e9es de sant\u00e9.</li> </ul>"},{"location":"cloud/ovh/#pro_11","title":"Pro","text":"<ul> <li>Nous avons besoin d'un registry priv\u00e9 accessible depuis le cloud afin de d\u00e9ployer nos applications.</li> </ul>"},{"location":"cloud/ovh/#cons_11","title":"Cons","text":"<ul> <li>N/A.</li> </ul> <p>Notes</p> <ul> <li>L'h\u00e9bergement d'un registre priv\u00e9 de conteneurs n\u00e9cessite un environnement s\u00e9curis\u00e9 pour le stockage de vos images de conteneurs. Pour ce faire, vous pouvez soit utiliser un service d'un fournisseur de cloud, installer des logiciels tiers sur votre propre infrastructure ou cr\u00e9er votre propre solution personnalis\u00e9e.</li> <li>Utiliser le service d'un fournisseur de cloud, tel que Managed Private Registry d'OVHcloud, constitue la solution la plus simple pour h\u00e9berger un registre priv\u00e9 de conteneurs. Ce service enti\u00e8rement g\u00e9r\u00e9 vous lib\u00e8re du stress li\u00e9 \u00e0 la maintenance de votre registre priv\u00e9, tout en offrant une s\u00e9curit\u00e9 \u00e9lev\u00e9e, des tarifs pr\u00e9visibles et une gamme de fonctionnalit\u00e9s.</li> </ul>"},{"location":"cloud/ovh/#load-balancer-for-mks","title":"Load Balancer for M.K.S","text":"<p>G\u00e9rez les variations d\u2019activit\u00e9 en r\u00e9partissant le trafic sur vos diff\u00e9rentes ressources. \u00c9quilibrez la charge de votre application en temps r\u00e9el sur plusieurs n\u0153uds de mani\u00e8re automatique et s\u00e9curis\u00e9e.</p> <ul> <li>99,99 % de disponibilit\u00e9.</li> <li>Gestion automatique des n\u0153uds.</li> <li>Int\u00e9gr\u00e9 directement \u00e0 Kubernetes.</li> <li>Conforme et certifi\u00e9.</li> </ul>"},{"location":"cloud/ovh/#pro_12","title":"Pro","text":"<ul> <li>Service manag\u00e9 compatible nativement avec l'offre ovh.</li> <li>Seul moyen pour avoir un LB sur du multinode.</li> <li>Compatible avec les ingress nginx.</li> </ul>"},{"location":"cloud/ovh/#cons_12","title":"Cons","text":"<ul> <li>Comp\u00e9tence n\u00e9c\u00e9ssaire \u00e0 l'op\u00e9rationnalisation.</li> </ul>"},{"location":"cloud/ovh/#databases","title":"Databases","text":""},{"location":"cloud/ovh/#description_4","title":"Description","text":"<p>Une base de donn\u00e9es cloud est souvent appel\u00e9e \u00ab Database as a Service \u00bb ou \u00ab Cloud database Service \u00bb. Elle est con\u00e7ue et accessible via une plateforme cloud. Le cloud permet aux entreprises d'h\u00e9berger des bases de donn\u00e9es sans acheter de mat\u00e9riel d\u00e9di\u00e9. Les bases de donn\u00e9es cloud prennent en charge \u00e0 la fois les bases de donn\u00e9es relationnelles (MySQL , PostgreSQL) et NoSQL (MongoDB, Caching, Apache Cassandra).</p>"},{"location":"cloud/ovh/#services_5","title":"Services","text":""},{"location":"cloud/ovh/#mysql","title":"MySQL","text":"<p>B\u00e9n\u00e9ficiez d\u2019un service 100 % manag\u00e9 pour d\u00e9ployer et op\u00e9rer les bases de donn\u00e9es MySQL de vos sites e-commerce et de vos applications. Nous nous chargeons de la gestion de votre service.</p>"},{"location":"cloud/ovh/#postgresql","title":"PostgreSQL","text":"<p>D\u00e9ployez un cluster manag\u00e9 PostgreSQL en quelques clics. B\u00e9n\u00e9ficiez du moteur de bases de donn\u00e9es relationnelles open-source de r\u00e9f\u00e9rence pour vos donn\u00e9es et vos applications.</p>"},{"location":"cloud/ovh/#mongodb","title":"MongoDB","text":"<p>Acc\u00e9l\u00e9rez votre time to market en nous confiant l\u2019administration de vos bases de donn\u00e9es NoSQL. Laissez vos \u00e9quipes se concentrer sur leur c\u0153ur de m\u00e9tier et le d\u00e9veloppement de vos services. Nous nous occupons de la configuration, de la maintenance, de la sauvegarde, de la s\u00e9curit\u00e9 et du monitoring de votre moteur de base de donn\u00e9es NoSQL orient\u00e9 documents pr\u00e9f\u00e9r\u00e9.</p>"},{"location":"cloud/ovh/#managed-caching","title":"Managed Caching","text":"<p>Votre service \u00ab in-memory database \u00bb NoSQL compatible Redis\u00ae OSS et manag\u00e9e par OVHcloud. Am\u00e9liorez la vitesse d'ex\u00e9cution de vos applications gr\u00e2ce \u00e0 une solution de caching haut performance.</p>"},{"location":"cloud/ovh/#cassandra","title":"Cassandra","text":"<p>D\u00e9ployez et ex\u00e9cutez un cluster distribu\u00e9 Apache Cassandra en quelques clics, manag\u00e9 par nos \u00e9quipes. Tr\u00e8s performant et hautement disponibilit\u00e9, ce service NoSQL est sp\u00e9cialement adapt\u00e9 pour vos traitements massifs de requ\u00eates en \u00e9criture.</p>"},{"location":"cloud/ovh/#m3db","title":"M3DB","text":"<p>D\u00e9ployez un moteur de base de donn\u00e9es time series (TSBD) open source, distribu\u00e9 et performant. Un service 100% manag\u00e9 id\u00e9al pour collecter vos m\u00e9triques \u00e0 petite ou grande \u00e9chelle en un instant.</p>"},{"location":"cloud/ovh/#m3-aggregator","title":"M3 Aggregator","text":"<p>D\u00e9ployez une architecture M3 Aggregator d\u00e9di\u00e9e \u00e0 l'agr\u00e9gation de flux pour vos bases de donn\u00e9es M3DB. Int\u00e9grez vos flux directement \u00e0 vos workflows de m\u00e9triques, en fonction de crit\u00e8res pr\u00e9d\u00e9finis.</p>"},{"location":"cloud/ovh/#analytics","title":"Analytics","text":""},{"location":"cloud/ovh/#description_5","title":"Description","text":"<p>Le cloud analytics d\u00e9signe l\u2019utilisation de services d\u2019analyse de donn\u00e9es h\u00e9berg\u00e9s sur des plateformes cloud. Cette approche permet de traiter et d\u2019analyser de grandes quantit\u00e9s de donn\u00e9es en utilisant des ressources informatiques cloud fournies et g\u00e9r\u00e9es par des tiers. Elle offre \u00e9volutivit\u00e9, efficacit\u00e9 et r\u00e9duction des co\u00fbts li\u00e9s \u00e0 l\u2019infrastructure informatique.</p>"},{"location":"cloud/ovh/#services_6","title":"Services","text":""},{"location":"cloud/ovh/#data-processing","title":"Data Processing","text":"<p>Analysez vos donn\u00e9es rapidement et en toute simplicit\u00e9 sur Apache Spark. OVHcloud se charge de d\u00e9ployer en quelques minutes un cluster Apache Spark d\u00e9di\u00e9 pour traiter votre requ\u00eate.</p> <ul> <li>Parall\u00e9lisation des traitements.</li> <li>Vous codez, nous d\u00e9ployons.</li> <li>R\u00e9duction des co\u00fbts.</li> <li>S\u00e9curit\u00e9 et conformit\u00e9.</li> </ul>"},{"location":"cloud/ovh/#kafka","title":"Kafka","text":"<p>D\u00e9ployez en quelques clics un cluster Apache Kafka complet et manag\u00e9 par nos \u00e9quipes, tout en conservant le contr\u00f4le de vos donn\u00e9es.</p>"},{"location":"cloud/ovh/#opensearch","title":"OpenSearch","text":"<p>Le moteur NoSQL d\u2019indexation, de recherche de contenu et d\u2019analyse de donn\u00e9es enti\u00e8rement manag\u00e9 par nos soins. Simplifiez le quotidien de vos d\u00e9veloppeurs en leur permettant de se concentrer sur la cr\u00e9ation d\u2019applications.</p>"},{"location":"cloud/ovh/#kafka-mirrormaker","title":"Kafka MirrorMaker","text":"<p>Copiez et streamez facilement les donn\u00e9es entre deux clusters Apache Kafka. Service 100% manag\u00e9 pour maintenir une replication compl\u00e8te dans un datacenter distant et une haute disponibilit\u00e9 de vos clusters Kafka.</p>"},{"location":"cloud/ovh/#kafka-connect","title":"Kafka Connect","text":"<p>Kafka Connect permet de connecter des syst\u00e8mes externes \u00e0 un cluster Apache Kafka. Plus besoin de vous pr\u00e9occuper de la gestion mat\u00e9rielle du service : configuration, monitoring et mises \u00e0 jour.</p>"},{"location":"cloud/ovh/#managed-dashboards-grafana","title":"Managed Dashboards (Grafana)","text":"<p>Cr\u00e9ez des tableaux de bord et des graphiques dynamiques depuis diff\u00e9rentes sources avec la plateforme Grafana\u00ae.</p> <ul> <li>Open-source et manag\u00e9.</li> <li>Rapport performances/prix.</li> <li>S\u00e9curit\u00e9 et conformit\u00e9.</li> <li>Plus de 60 services Public Cloud.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_13","title":"Pro","text":"<ul> <li>La gestion du dashboard est manag\u00e9.</li> </ul>"},{"location":"cloud/ovh/#cons_13","title":"Cons","text":"<ul> <li>Pricing \u00e9lev\u00e9 pour le service rendu.</li> <li>A \u00e9tudier face aux grafana(s) fournient par les autres services.</li> </ul>"},{"location":"cloud/ovh/#logs-data-platform","title":"Logs Data Platform","text":"<p>Augmentez la visibilit\u00e9 des environnements de vos applications en collectant, traitant, analysant et stockant vos logs sur une plateforme \u00e0 la fois compl\u00e8te et manag\u00e9e. L'analyse de logs est essentielle pour maintenir votre infrastructure et vos applications en bon \u00e9tat de fonctionnement.</p> <ul> <li>Standard et r\u00e9versible.</li> <li>Performante et \u00e9volutive.</li> <li>Tarification compl\u00e8te.</li> <li>S\u00e9curit\u00e9 et conformit\u00e9.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_14","title":"Pro","text":"<ul> <li>Une plateforme compl\u00e8te entierement manag\u00e9.</li> <li>Plateforme offerte en \"standard\".</li> </ul>"},{"location":"cloud/ovh/#cons_14","title":"Cons","text":"<ul> <li>Pricing flous ...</li> <li>Beaucoup d'options flous ...</li> </ul>"},{"location":"cloud/ovh/#data-platform","title":"Data Platform","text":""},{"location":"cloud/ovh/#description_6","title":"Description","text":"<p>Une plateforme collaborative unifi\u00e9e pour int\u00e9grer, stocker massivement, pr\u00e9parer et exploiter vos donn\u00e9es, afin d'acc\u00e9l\u00e9rer vos projets Data &amp; Analytics.</p>"},{"location":"cloud/ovh/#services_7","title":"Services","text":""},{"location":"cloud/ovh/#lakehouse-manager","title":"Lakehouse Manager","text":"<p>Un service serverless de data lake et data warehouse sur OVHcloud Data Platform, utilisant Apache Iceberg pour le stockage massif de donn\u00e9es.</p>"},{"location":"cloud/ovh/#data-catalog","title":"Data Catalog","text":"<p>R\u00e9f\u00e9rentiel central pour la gestion de toutes les sources de donn\u00e9es de OVHcloud Data Platform : connectez-les, analysez-les et ajoutez des r\u00e8gles de planification pour d\u00e9finir des normes de formatage.</p>"},{"location":"cloud/ovh/#data-processing-engine","title":"Data Processing Engine","text":"<p>Service d'int\u00e9gration et de transformation des donn\u00e9es de OVHcloud Data Platform, pour automatiser l\u2019ex\u00e9cution et l\u2019orchestration de vos workflows ETL/ELT en production.</p>"},{"location":"cloud/ovh/#analytics-manager","title":"Analytics Manager","text":"<p>Service de visualisation de donn\u00e9es et de gestion des requ\u00eates de OVHcloud Data Platform. Faites vos analyses en libre-service, r\u00e9alisez vos tableaux de bord, et facilitez les prises de d\u00e9cisions en vous appuyant sur vos donn\u00e9es.</p>"},{"location":"cloud/ovh/#applications-services","title":"Applications Services","text":"<p>Service de conception et d\u00e9ploiement d\u2019API et de web apps personnalis\u00e9es. D\u00e9veloppez vos API et partagez des tableaux de bord d\u00e9taill\u00e9s avec OVHcloud Data Platform.</p>"},{"location":"cloud/ovh/#control-center","title":"Control Center","text":"<p>G\u00e9rez les droits d'acc\u00e8s et acc\u00e9dez en temps r\u00e9el aux informations de votre infrastructure. Obtenez une vue d\u2019ensemble compl\u00e8te de vos pipelines de donn\u00e9es, applications et connecteurs afin de collecter des m\u00e9triques, surveiller les performances et configurer des alertes.</p>"},{"location":"cloud/ovh/#ai-machine-learning","title":"AI &amp; Machine learning","text":""},{"location":"cloud/ovh/#description_7","title":"Description","text":"<p>Le Machine learning as a Service (ou MLaaS) est un service cloud r\u00e9cent qui propose du machine learning. Il b\u00e9n\u00e9ficie de l\u2019essor du cloud ces derni\u00e8res ann\u00e9es, \u00e0 l\u2019instar des SaaS, PaaS et IaaS. Gr\u00e2ce \u00e0 lui, les entreprises peuvent se lancer dans l\u2019intelligence artificielle (IA) \u00e0 moindres frais, gr\u00e2ce aux diff\u00e9rents outils de machine learning et solutions de d\u00e9veloppement. Vous gagnez ainsi du temps lors du lancement de votre activit\u00e9. De plus, la ma\u00eetrise de votre budget rend possible le d\u00e9veloppement de tous les usages pour lesquels le logiciel de machine learning est con\u00e7u : estimation des risques, d\u00e9tection des fraudes, strat\u00e9gies marketing, optimisation logistique, etc.</p>"},{"location":"cloud/ovh/#services_8","title":"Services","text":""},{"location":"cloud/ovh/#ai-notebooks-quantum-notebooks","title":"AI Notebooks &amp; Quantum Notebooks","text":"<p>Acc\u00e9l\u00e9rez le lancement de vos projets et mod\u00e8les avec des notebooks enti\u00e8rement g\u00e9r\u00e9s en quelques secondes. Acc\u00e9dez \u00e0 Jupyter ou Visual Studio Code et d\u00e9marrez rapidement avec les ressources ad\u00e9quates.</p> <ul> <li>Pour les d\u00e9veloppeurs et les data scientists.</li> <li>Tarification claire et flexible.</li> <li>Vos frameworks AI pr\u00e9f\u00e9r\u00e9s.</li> <li>Un cloud europ\u00e9en pour le respect de vos donn\u00e9es.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_15","title":"Pro","text":"<ul> <li>Environnements manag\u00e9s.</li> <li>Frameworks pr\u00e9-configur\u00e9s.</li> <li>Notebooks collaboratif.</li> </ul>"},{"location":"cloud/ovh/#cons_15","title":"Cons","text":"<ul> <li>Pricing \u00e9lev\u00e9 pour le service rendu.</li> <li>A \u00e9tudier face aux grafana(s) fournient par les autres services.</li> </ul>"},{"location":"cloud/ovh/#ai-training","title":"AI Training","text":"<p>Entra\u00eenez efficacement et simplement vos mod\u00e8les d\u2019intelligence artificielle, de machine learning et de deep learning tout en optimisant vos usages GPU.</p> <ul> <li>Pour les \u00e9quipes de d\u00e9veloppement et les data scientists.</li> <li>Optimisation de l\u2019usage des ressources.</li> <li>Puissance GPU au meilleur prix.</li> <li>Certifications et conformit\u00e9.</li> </ul>"},{"location":"cloud/ovh/#pro_16","title":"Pro","text":"<ul> <li>Orchestrateur manag\u00e9.</li> </ul>"},{"location":"cloud/ovh/#cons_16","title":"Cons","text":"<ul> <li>Learning curve.</li> </ul>"},{"location":"cloud/ovh/#ai-deploy","title":"AI Deploy","text":"<p>D\u00e9ployez facilement des mod\u00e8les et des applications de machine learning en production, cr\u00e9ez vos points d\u2019acc\u00e8s API en toute simplicit\u00e9 et r\u00e9alisez des pr\u00e9dictions efficaces.</p> <ul> <li>Flexibilit\u00e9, performance et haute disponibilit\u00e9.</li> <li>D\u00e9ploiement rapide et architecture manag\u00e9e.</li> <li>S\u00e9curit\u00e9 et conformit\u00e9.</li> </ul>"},{"location":"cloud/ovh/#pro_17","title":"Pro","text":"<ul> <li>Fournit des service SaaS afin de faire tourner des algo headless ou des mini-apps.</li> </ul>"},{"location":"cloud/ovh/#cons_17","title":"Cons","text":"<ul> <li>Ce n'est pas adpat\u00e9 au contexte d'imaging fabric.</li> </ul>"},{"location":"cloud/ovh/#lettria","title":"Lettria","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#voxist","title":"Voxist","text":"<ul> <li>N/A.</li> </ul>"},{"location":"cloud/ovh/#nvidia-ngc-platform","title":"NVIDIA NGC Platform","text":"<p>OVHcloud et NVIDIA s\u2019associent pour proposer la meilleure plateforme d\u2019acc\u00e9l\u00e9ration GPU pour le deep learning et le calcul haute performance.</p> <p>Le NVIDIA GPU Cloud d\u2019OVHcloud combine la flexibilit\u00e9 du Public Cloud et la puissance de la carte graphique NVIDIA Tesla V100, pour fournir un catalogue complet de conteneurs \u00e0 acc\u00e9l\u00e9ration GPU, pouvant \u00eatre d\u00e9ploy\u00e9s et maintenus dans le cadre d\u2019applications d\u2019intelligence artificielle.</p> <p>Il permet aux utilisateurs d\u2019ex\u00e9cuter leurs projets sur une plateforme fiable et performante qui respecte confidentialit\u00e9, r\u00e9versibilit\u00e9 et transparence de la localisation des donn\u00e9es.</p>"},{"location":"cloud/ovh/#ai-endpoints-beta","title":"AI Endpoints (Beta)","text":"<p>Designed with simplicity in mind, our platform allows developers of all skill levels to enhance their applications with cutting-edge AI APIs \u2014no AI expertise required.</p> <p></p>"},{"location":"cloud/ovh/#idam","title":"Idam","text":""},{"location":"cloud/ovh/#description_8","title":"Description","text":"<p>Identit\u00e9, s\u00e9curit\u00e9 et op\u00e9rations est une gamme de services d'OVHcloud. Ils visent \u00e0 am\u00e9liorer la s\u00e9curit\u00e9, la gestion et l'efficacit\u00e9 op\u00e9rationnelle de votre solution. Ces services comprennent la gestion des identit\u00e9s et des acc\u00e8s (IAM) pour le contr\u00f4le d'acc\u00e8s \u00e0 vos donn\u00e9es, le service de gestion de cl\u00e9s (KMS) pour g\u00e9rer les cl\u00e9s de chiffrement et les journaux de service pour la surveillance des performances et de la s\u00e9curit\u00e9.</p>"},{"location":"cloud/ovh/#services_9","title":"Services","text":""},{"location":"cloud/ovh/#gestion-des-identites-et-des-acces-iam","title":"Gestion des identit\u00e9s et des acc\u00e8s (IAM)","text":"<p>G\u00e9rez de mani\u00e8re s\u00e9curis\u00e9e l\u2019identit\u00e9 de vos utilisateurs et applications, ainsi que leurs droits via une interface unique pour tous vos services. La solution IAM d\u2019OVHcloud assure une gestion granulaire des acc\u00e8s \u00e0 vos produits OVHcloud et renforce la s\u00e9curit\u00e9 de votre gestion des acc\u00e8s en s\u2019appuyant sur une interface centralis\u00e9e.</p> <ul> <li>Identit\u00e9 f\u00e9d\u00e9r\u00e9e.</li> <li>Unifi\u00e9 et harmonis\u00e9 sur l\u2019ensemble du portefeuille OVHcloud y compris pour les logiciels tiers.</li> <li>Gestion fine des strat\u00e9gies.</li> <li>Une plus grande s\u00e9curit\u00e9 pour vos services.</li> <li>Inclus sans frais suppl\u00e9mentaires.</li> </ul> <p>Les politiques contiennent une liste d'identit\u00e9s (comptes, utilisateurs, groupes d'utilisateurs) concern\u00e9es par les politiques, une liste de ressources auxquelles les politiques doivent s'appliquer et une liste d'actions autoris\u00e9es sur ces ressources.</p> <p></p>"},{"location":"cloud/ovh/#pro_18","title":"Pro","text":"<ul> <li>Oligatoire pour cr\u00e9er des identit\u00e9s, des actions et des policies.</li> </ul>"},{"location":"cloud/ovh/#cons_18","title":"Cons","text":"<ul> <li>Comp\u00e9tence n\u00e9c\u00e9ssaire \u00e0 l'op\u00e9rationnalisation.</li> </ul>"},{"location":"cloud/ovh/#logs-data-platform_1","title":"Logs Data Platform","text":"<p>Augmentez la visibilit\u00e9 des environnements de vos applications en collectant, traitant, analysant et stockant vos logs sur une plateforme \u00e0 la fois compl\u00e8te et manag\u00e9e. L'analyse de logs est essentielle pour maintenir votre infrastructure et vos applications en bon \u00e9tat de fonctionnement.</p> <ul> <li>Standard et r\u00e9versible.</li> <li>Performante et \u00e9volutive.</li> <li>Tarification compl\u00e8te.</li> <li>S\u00e9curit\u00e9 et conformit\u00e9.</li> </ul> <p></p>"},{"location":"cloud/ovh/#pro_19","title":"Pro","text":"<ul> <li>Solution clefs en main.</li> <li>C'est un service cross service.</li> <li>Obligatoire pour des besoins d'audit et de surveillance des acc\u00e8s.</li> </ul>"},{"location":"cloud/ovh/#cons_19","title":"Cons","text":"<ul> <li>Tarification incompr\u00e9hensible.</li> </ul>"},{"location":"cloud/ovh/#key-management-service-kms","title":"Key Management Service (KMS)","text":"<p>Am\u00e9liorez votre s\u00e9curit\u00e9 et g\u00e9rez efficacement vos cl\u00e9s de chiffrement avec le service de gestion de cl\u00e9s (KMS) d'OVHcloud.</p> <p>Con\u00e7u pour une int\u00e9gration transparente, notre KMS vous permet de g\u00e9rer de mani\u00e8re centralis\u00e9e les cl\u00e9s de chiffrement de toutes vos applications, qu'elles soient h\u00e9berg\u00e9es dans le cloud ou on-premises. Cela maximise la s\u00e9curisation des donn\u00e9es et rationalise les op\u00e9rations de s\u00e9curit\u00e9.</p> <ul> <li>Protection instantan\u00e9e des donn\u00e9es gr\u00e2ce au chiffrement en un clic.</li> <li>S\u00e9curit\u00e9 renforc\u00e9e avec gestion compl\u00e8te des acc\u00e8s aux cl\u00e9s.</li> <li>Vos cl\u00e9s : renforcez la confidentialit\u00e9 des donn\u00e9es avec Bring Your Own Keys (BYOK).</li> <li>Mod\u00e8le de tarification pr\u00e9visible : requ\u00eates incluses sans frais suppl\u00e9mentaires.</li> <li>Certification Nutanix Ready.</li> <li>SDK et CLI open source.</li> </ul>"},{"location":"cloud/ovh/#service-logs-beta","title":"Service Logs (Beta)","text":"<p>OVHcloud Service logs vous aide \u00e0 suivre \u00ab qui a fait quoi, o\u00f9 et quand \u00bb sur l\u2019ensemble de vos ressources OVHcloud. Combin\u00e9e \u00e0 Logs Data Platform, cette solution vous permet de transformer vos logs en donn\u00e9es pr\u00e9cieuses \u00e0 l'aide d'outils de stockage, d'archivage, d'interrogation et de visualisation. Vous pouvez d\u00e9sormais surveiller vos donn\u00e9es et votre syst\u00e8me en temps r\u00e9el, pour une meilleure s\u00e9curit\u00e9 et une efficacit\u00e9 op\u00e9rationnelle accrue.</p> <ul> <li>Efficacit\u00e9 op\u00e9rationnelle accrue.</li> <li>S\u00e9curit\u00e9 renforc\u00e9e.</li> <li>Conformit\u00e9 sans effort.</li> <li>Int\u00e9gration fluide et non intrusive avec le catalogue de services OVHcloud.</li> <li>Gestion unifi\u00e9e des logs.</li> <li>Solution open source \u00e9volutive et enti\u00e8rement manag\u00e9e.</li> </ul>"},{"location":"cloud/ovh/#local-zones","title":"Local Zones","text":"<p>D\u00e9veloppez votre activit\u00e9 et celle de votre client\u00e8le \u00e0 l\u2019\u00e9chelle mondiale tout en op\u00e9rant localement. Stimulez la croissance de votre entreprise en optimisant votre acc\u00e8s aux donn\u00e9es, en am\u00e9liorant les performances des applications et en tirant parti de la faible latence, du stockage optimal, de l\u2019edge computing, etc.</p> <ul> <li>Faible latence et proximit\u00e9.</li> <li>Localisation des donn\u00e9es.</li> <li>Couverture internationale.</li> <li>De hautes performances au meilleur prix.</li> <li>Flexibilit\u00e9 et \u00e9volutivit\u00e9 du cloud.</li> <li>Standard et facile d'utilisation.</li> </ul> <p></p>"},{"location":"cloud/ovh/#saving-plan","title":"Saving Plan","text":"<ul> <li>Optimisez vos co\u00fbts avec les Savings Plans.</li> <li>\u00c9voluez \u00e0 votre rythme.</li> <li>Gagnez en pr\u00e9visibilit\u00e9.</li> <li>\u00c9conomisez sur vos workloads stables.</li> <li>Pilotez la croissance de votre application.</li> <li>G\u00e9rez la saisonnalit\u00e9 de votre infrastructure.</li> </ul>"},{"location":"cloud/ovh/#misc","title":"Misc","text":""},{"location":"cloud/ovh/#quels-sont-les-avantages-du-cloud-computing","title":"Quels sont les avantages du cloud computing ?","text":"<p>Les avantages du cloud computing sont nombreux : il permet une grande flexibilit\u00e9 et une scalabilit\u00e9 des ressources informatiques, r\u00e9duit les co\u00fbts en faisant payer uniquement pour ce qui est utilis\u00e9, facilite l\u2019acc\u00e8s aux services et applications de n\u2019importe o\u00f9 et minimise la maintenance n\u00e9cessaire, car l\u2019infrastructure est g\u00e9r\u00e9e par le fournisseur.</p>"},{"location":"cloud/ovh/#quels-sont-les-principaux-types-de-cloud-computing","title":"Quels sont les principaux types de cloud computing ?","text":"<p>Il existe plusieurs types de cloud computing :</p> <p>Infrastructure-as-a-service (IaaS) : fournit des ressources informatiques virtualis\u00e9es via internet. Les utilisateurs g\u00e8rent les syst\u00e8mes d\u2019exploitation, les applications et la configuration, tandis que le fournisseur g\u00e8re le mat\u00e9riel sous-jacent.</p> <p>Platform-as-a-service (PaaS) : met \u00e0 la disposition des utilisateurs un environnement de d\u00e9veloppement et de d\u00e9ploiement sur internet, ce qui leur permet de d\u00e9velopper, ex\u00e9cuter et g\u00e9rer des applications sans se soucier de la complexit\u00e9 de l\u2019infrastructure.</p> <p>Software-as-a-service (SaaS) : fournit des applications logicielles via internet sur un mod\u00e8le d\u2019abonnement. Les utilisateurs acc\u00e8dent aux logiciels sans avoir \u00e0 les installer sur leur ordinateur personnel ou sur les serveurs de l\u2019entreprise.</p>"},{"location":"cloud/ovh/#object-storage-vs-block-storage-how-are-they-different","title":"Object storage vs. block storage: How are they different?","text":""},{"location":"cloud/ovh/#quest-ce-quun-registre-prive-de-conteneurs","title":"Qu\u2019est-ce qu\u2019un registre priv\u00e9 de conteneurs ?","text":"<p>Un registre priv\u00e9 de conteneurs est un syst\u00e8me de stockage et de distribution pour les images de conteneurs, c'est-\u00e0-dire des fichiers qui incluent tout le code, les biblioth\u00e8ques et les autres ressources n\u00e9cessaires pour ex\u00e9cuter une application dans un environnement conteneuris\u00e9. Accessible uniquement aux utilisateurs autoris\u00e9s, un registre de conteneurs priv\u00e9 constitue un espace s\u00e9curis\u00e9 pour stocker les images de conteneurs. Il permet aux organisations de g\u00e9rer et de contr\u00f4ler facilement leurs images de conteneurs, tout en offrant une int\u00e9gration simple avec les workflows existants d'int\u00e9gration continue et de d\u00e9ploiement continu (CI/CD) pour un d\u00e9veloppement, des tests et un d\u00e9ploiement d'applications fluides.</p>"},{"location":"cloud/ovh/#quest-ce-que-le-data-processing","title":"Qu\u2019est-ce que le data processing ?","text":"<p>Le data processing, ou traitement des donn\u00e9es, d\u00e9signe le processus d\u2019analyse des donn\u00e9es brutes. En effet, ces vastes quantit\u00e9s d\u2019informations s\u2019av\u00e8rent capitales pour les entreprises. Une fois trait\u00e9es, elles permettent une meilleure compr\u00e9hension des chiffres de ventes, de l\u2019efficacit\u00e9 d\u2019une campagne marketing ou encore d\u2019un risque financier. Cette op\u00e9ration s\u2019articule en plusieurs \u00e9tapes :</p> <ul> <li>La collecte des donn\u00e9es. La quantit\u00e9 d\u2019informations recueillies influence la qualit\u00e9 du r\u00e9sultat. Elles peuvent provenir de diff\u00e9rentes sources : fichiers client, inventaires, \u00e9tudes ant\u00e9rieures, etc. Pour \u00eatre utilisables, elles doivent \u00eatre fiables</li> <li>La pr\u00e9paration des informations. Il s\u2019agit d\u2019une phase de \u00ab nettoyage \u00bb de vos bases de donn\u00e9es. Elle vise \u00e0 \u00e9liminer les \u00e9l\u00e9ments de mauvaise qualit\u00e9 et/ou les erreurs.</li> <li>L\u2019importation des donn\u00e9es travaill\u00e9es et le d\u00e9marrage du traitement. L\u2019automatisation de cette analyse passe par un algorithme de machine learning.</li> <li>L\u2019interpr\u00e9tation des donn\u00e9es. Cette \u00e9tape vous permet de d\u00e9gager des informations lisibles et exploitables par tous.</li> <li>Le stockage des donn\u00e9es. Il sert \u00e0 conserver des donn\u00e9es qui pourront servir lors de futures \u00e9tudes.</li> </ul> <p>Notez que le stockage d\u2019informations est soumis \u00e0 certaines r\u00e9glementations. Par exemple, le RGPD exige une solution s\u00e9curis\u00e9e et conforme pour l\u2019ensemble de vos donn\u00e9es.</p>"},{"location":"cloud/ovh/#links","title":"Links","text":"<p>Ovhcloud Ovhcloud Blog Ovhcloud Learn OVHcloud Public Cloud Doc OVHcloud Public Cloud Status OVHcloud Market Place Sant\u00e9</p>"},{"location":"development/jhipster/","title":"JHipster : Installation et Configuration","text":""},{"location":"development/jhipster/#version-des-outils","title":"Version des outils","text":"Os / Tool Version JHipster 6.1.2"},{"location":"development/jhipster/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"development/jhipster/#note-en-vrac","title":"Note en vrac","text":"<p>G\u00e9n\u00e9rer une application front sans gestion des utilisateurs et sans back office :  </p> <pre><code>jh app --skip-server --skip-user-management --auth jwt --db h2\n</code></pre>"},{"location":"development/jhipster/#avant-propos","title":"Avant propos","text":"<p>JHipster is a development platform to generate, develop and deploy Spring Boot + Angular / React / Vue Web applications and Spring microservices.</p>"},{"location":"development/jhipster/#jhipster-quick-start","title":"JHipster Quick Start","text":"<ul> <li>Install JHipster <code>npm install -g generator-jhipster</code></li> <li>Create a new directory and go into it <code>mkdir myApp &amp;&amp; cd myApp</code></li> <li>Run JHipster and follow instructions on screen <code>jhipster</code></li> <li>Model your entities with JDL Studio and download the resulting <code>jhipster-jdl.jh</code> file</li> <li>Generate your entities with <code>jhipster import-jdl jhipster-jdl.jh</code></li> </ul>"},{"location":"development/jhipster/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":""},{"location":"development/jhipster/#source","title":"Source","text":"<p>JHipster </p>"},{"location":"devops/commit/","title":"Commit","text":""},{"location":"devops/commit/#commit-best-practice","title":"Commit Best Practice","text":""},{"location":"devops/commit/#tools-versions","title":"Tools versions","text":"Os / Tool Version Linux Mint 19.3 pre-commit 2.8.2 gitlint 0.14.0"},{"location":"devops/commit/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/commit/#bulk-note","title":"Bulk Note","text":"<p>N/A</p>"},{"location":"devops/commit/#about","title":"About","text":"<p>Pre-commit: A framework for managing and maintaining multi-language pre-commit hooks. Git-lint: Git commit message linter.</p>"},{"location":"devops/commit/#installation-procedure","title":"Installation procedure","text":""},{"location":"devops/commit/#pre-commit","title":"Pre-commit","text":"<pre><code>pip install --user --upgrade pre-commit\n</code></pre>"},{"location":"devops/commit/#git-lint","title":"Git-lint","text":"<pre><code>pip install --user --upgrade gitlint\n</code></pre>"},{"location":"devops/commit/#getting-start","title":"Getting start","text":""},{"location":"devops/commit/#pre-commit_1","title":"Pre-commit","text":"<p>Add a pre-commit configuration</p> <pre><code>cd MY_REPO &amp;&amp; \\\ntouch .pre-commit-config.yaml &amp;&amp; \\\npre-commit sample-config &gt; .pre-commit-config.yaml\n</code></pre> <p>Install the Git hook scripts</p> <pre><code>cd MY_REPO &amp;&amp; \\\npre-commit install\n</code></pre> <p>Run against all the files (first execution)</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>How does it work pre-commit is configured via the .pre-commit-config.yaml file. The configuration file is divided into three levels :</p> <ul> <li>Top level (see here).</li> <li>repos (see here).<ul> <li>hooks(see here).</li> </ul> </li> </ul> <p>Repos should be a Git URL to clone. Hooks represent the \"action\" to perfom. A repository can contain many hooks.</p> <p>Advanced Feature You can execute hooks from local scripts and/or local cli (see here). In my case cli must be installed before running pre-commit hooks.</p> <pre><code>pip install --user --upgrade ansible-lint yamllint &amp;&amp; \\\nsudo npm install -g eclint &amp;&amp; \\\nsudo curl -L https://github.com/hadolint/hadolint/releases/download/v1.18.2/hadolint-Linux-x86_64 --output /usr/local/bin/hadolint &amp;&amp; \\\nsudo chmod 755 /usr/local/bin/hadolint\n</code></pre> <p>This allows pre-commit to use certain linter with the configuration files located at the root of the projects.</p>"},{"location":"devops/commit/#git-lint_1","title":"Git-lint","text":"<p>Add a gitlint configuration</p> <pre><code>cd MY_REPO &amp;&amp; \\\ngitlint generate-config\n</code></pre> <p>Linting a range of commits</p> <pre><code>gitlint --commits \"019cf40...d6bc75a\"\n</code></pre>"},{"location":"devops/commit/#convetional-commit","title":"Convetional commit","text":"<p>The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. This convention dovetails with SemVer, by describing the features, fixes, and breaking changes made in commit messages.</p> <p>types:</p> <ul> <li>fix:</li> <li>feat:</li> <li>build:</li> <li>chore:</li> <li>ci:</li> <li>docs:</li> <li>style:</li> <li>refactor:</li> <li>perf:</li> <li>test:</li> </ul>"},{"location":"devops/commit/#source","title":"Source","text":"<p>Docs: convetional commit Docs: gitlint Docs: pre-commit GitHub: gitlint</p>"},{"location":"devops/docker-compose/","title":"Docker Compose : Installation et Configuration","text":""},{"location":"devops/docker-compose/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Debian 9.9 Docker Ce 18.09.0"},{"location":"devops/docker-compose/#note-en-vrac","title":"Note en vrac","text":""},{"location":"devops/docker-compose/#overview-of-docker-compose","title":"Overview of Docker Compose","text":"<p>Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose,</p> <p>Compose has commands for managing the whole lifecycle of your application:</p> <p>Start, stop, and rebuild services View the status of running services Stream the log output of running services Run a one-off command on a service  </p> <p>The features of Compose that make it effective are:</p> <p>Multiple isolated environments on a single host Preserve volume data when containers are created Only recreate containers that have changed Variables and moving a composition between environments  </p> <p>Compose has traditionally been focused on development and testing workflows</p>"},{"location":"devops/docker-compose/#install-docker-compose","title":"Install Docker Compose","text":"<pre><code>sudo curl -L \"https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\n# I needed\nsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n</code></pre>"},{"location":"devops/docker-compose/#en-vrac","title":"En vrac","text":"<p>https://docs.docker.com/compose/completion/ https://docs.docker.com/compose/compose-file/ https://docs.docker.com/compose/env-file/ https://docs.docker.com/release-notes/docker-compose/ </p>"},{"location":"devops/docker-compose/#source","title":"Source","text":"<p>Get Docker Get Docker CE for CentOS Post-installation steps for Linux Get Started Docker Machine Katacoda Docker</p>"},{"location":"devops/docker/","title":"Docker : Installation et Configuration","text":""},{"location":"devops/docker/#version-des-outils","title":"Version des outils","text":"Os / Tool Version CentOs 7.4.1708 Docker Ce 17.12.0"},{"location":"devops/docker/#avant-propos","title":"Avant propos","text":"<p>Docker is available in two editions: Community Edition (CE) and Enterprise Edition (EE). Docker Community Edition (CE) is ideal for developers and small teams looking to get started with Docker and experimenting with container-based apps. Docker CE has two update channels, stable and edge:</p> <ul> <li>Stable gives you reliable updates every quarter</li> <li>Edge gives you new features every month</li> </ul>"},{"location":"devops/docker/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>La proc\u00e9dure d'installation de Docker sur CentOs 7 se d\u00e9roule de la fa\u00e7on suivante : Installez les pr\u00e9requis.</p> <pre><code>sudo yum install -y yum-utils device-mapper-persistent-data lvm2\n</code></pre> <p>Assurez-vous qu'ils n'existent aucune autres versions de Docker sur la machine.</p> <pre><code>sudo yum remove docker docker-common docker-selinux docker-engine\n</code></pre> <p>Assurez-vous que le repository extras est bien activ\u00e9.</p> <pre><code>sudo yum-config-manager --enable extras\n</code></pre> <p>Ajoutez le repository Docker.</p> <pre><code>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n</code></pre> <p>Installez et v\u00e9rifiez que Docker fonctionne correctement.</p> <pre><code>sudo yum install docker-ce\nsudo systemctl start docker\nsudo docker run hello-world\n</code></pre>"},{"location":"devops/docker/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>V\u00e9rifiez les versions de Docker disponible et celle install\u00e9e.</p> <pre><code>yum list docker-ce --showduplicates | sort -r\n</code></pre> <p>Ajouter un utilisateur non sudoers qui aura les privil\u00e8ge d'utiliser Docker.</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\n</code></pre> <p>Activer Docker au d\u00e9marrage de Linux.</p> <pre><code>sudo systemctl enable docker\n</code></pre>"},{"location":"devops/docker/#docker-concepts","title":"Docker concepts","text":"<p>A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight. By contrast, a virtual machine (VM) runs a full-blown \u201cguest\u201d operating system with virtual access to host resources through a hypervisor. In general, VMs provide an environment with more resources than most applications need.  </p> Container Vms"},{"location":"devops/docker/#tutoriels-dockers","title":"Tutoriels Dockers","text":""},{"location":"devops/docker/#docker-get-started","title":"Docker : Get Started","text":""},{"location":"devops/docker/#part-1-orientation","title":"Part 1 Orientation","text":"<p>Ce qu'il faut retenir : Un simple <code>docker run nom-de-l'image</code> permet de \"tirer\" une image depuis un repository et de \"d\u00e9marrer\" un container. Un container est l'instanciation d'une image. Pour manager les containers on utilise la commande <code>docker container</code>. Pour manager les images on utilise la commande <code>docker image</code> manage les images.  </p> <p>Cheat sheet :</p> <pre><code>    ## List Docker CLI commands\n    docker\n    docker container --help\n\n    ## Display Docker version and info\n    docker --version\n    docker version\n    docker info\n\n    ## Excecute Docker image\n    docker run hello-world\n\n    ## List Docker images\n    docker image ls\n\n    ## List Docker containers (running, all, all in quiet mode)\n    docker container ls\n    docker container ls --all\n    docker container ls -a -q\n</code></pre>"},{"location":"devops/docker/#part-2-containers","title":"Part 2 Containers","text":"<p>Ce qu'il faut retenir : Pour cr\u00e9er un container on doit le builder depuis fichier nomm\u00e9 Dockerfile. Pour construire une image on utilise la commande <code>docker build</code>. Une image construite en local est disponible via la commande <code>docker image ls</code> Pour partager une image on commence par la taguer via <code>docker tag image username/repository:tag</code> On la push ensuite sur un repository via <code>docker push username/repository:tag</code> </p> <p>Cheat sheet : </p> <pre><code>    docker build -t friendlyhello .                  # Create image using this directory's Dockerfile\n    docker run -p 4000:80 friendlyhello              # Run \"friendlyname\" mapping port 4000 to 80\n    docker run -d -p 4000:80 friendlyhello           # Same thing, but in detached mode\n    docker container ls                              # List all running containers\n    docker container ls -a                           # List all containers, even those not running\n    docker container stop &lt;hash&gt;                     # Gracefully stop the specified container\n    docker container kill &lt;hash&gt;                     # Force shutdown of the specified container\n    docker container rm &lt;hash&gt;                       # Remove specified container from this machine\n    docker container rm $(docker container ls -a -q) # Remove all containers\n    docker image ls -a                               # List all images on this machine\n    docker image rm &lt;image id&gt;                       # Remove specified image from this machine\n    docker image rm $(docker image ls -a -q)         # Remove all images from this machine\n    docker login                                     # Log in this CLI session using your Docker credentials\n    docker tag &lt;image&gt; username/repository:tag       # Tag &lt;image&gt; for upload to registry\n    docker push username/repository:tag              # Upload tagged image to registry\n    docker run username/repository:tag               # Run image from a registry\n</code></pre>"},{"location":"devops/docker/#part-3-services","title":"Part 3 Services","text":"<p>Ce qu'il faut retenir : Un service est un container codifi\u00e9 par un fichier docker-compose.yml qui d\u00e9finit comment il doit \u00eatre instanci\u00e9. Un fichier docker-compose.yml se d\u00e9ploie via la commande <code>docker stack deploy -c docker-compose.yml nom_de_service</code>. Pour pouvoir d\u00e9ployer une stack il faut instancier un cluster Swarm (Minimun un node). On instancie un cluster Swarm avec la commande <code>docker swarm init</code>. Pour manager les services on utilise la commande  <code>docker service</code>.  </p> <p>Cheat sheet : </p> <pre><code>    docker stack ls                                  # List stacks or apps\n    docker stack deploy -c &lt;composefile&gt; &lt;appname&gt;   # Run the specified Compose file\n    docker service ls                                # List running services associated with an app\n    docker service ps &lt;service&gt;                      # List tasks associated with an app\n    docker inspect &lt;task or container&gt;               # Inspect task or container\n    docker container ls -q                           # List container IDs\n    docker stack rm &lt;appname&gt;                        # Tear down an application\n    docker swarm leave --force                       # Take down a single node swarm from the manager\n</code></pre>"},{"location":"devops/docker/#part-4-swarms","title":"Part 4 Swarms","text":"<p>Ce qu'il faut retenir : Un Swarm est un ensemble de machine qui forment un cluster docker. On initialise un Swarm avec la commande <code>docker swarm init</code> Celle ci retourne un ID qui permet aux autres instances de rejoindre le cluster Swarm via la commande <code>docker swarm join --token SWMTKN-1-sha256 @ip_du_master:2377</code> Swarm embarque tout un tas de service natif comme un load balanceur, simplifiant la scalabilit\u00e9 et la configuration d'application en mode distribu\u00e9.</p> <p>Cheat sheet : </p> <pre><code>    docker-machine create --driver virtualbox myvm1                                                     # Create a VM (Mac, Win7, Linux)\n    docker-machine create -d hyperv --hyperv-virtual-switch \"myswitch\" myvm1                            # Win10\n    docker-machine env myvm1                                                                            # View basic information about your node\n    docker-machine ssh myvm1 \"docker node ls\"                                                           # List the nodes in your swarm\n    docker-machine ssh myvm1 \"docker node inspect &lt;node ID&gt;\"                                            # Inspect a node\n    docker-machine ssh myvm1 \"docker swarm join-token -q worker\"                                        # View join token\n    docker-machine ssh myvm1                                                                            # Open an SSH session with the VM; type \"exit\" to end\n    docker node ls                                                                                      # View nodes in swarm (while logged on to manager)\n    docker-machine ssh myvm2 \"docker swarm leave\"                                                       # Make the worker leave the swarm\n    docker-machine ssh myvm1 \"docker swarm leave -f\"                                                    # Make master leave, kill swarm\n    docker-machine ls                                                                                   # list VMs, asterisk shows which VM this shell is talking to\n    docker-machine start myvm1                                                                          # Start a VM that is currently not running\n    docker-machine env myvm1                                                                            # show environment variables and command for myvm1\n    eval $(docker-machine env myvm1)                                                                    # Mac command to connect shell to myvm1\n    &amp; \"C:\\Program Files\\Docker\\Docker\\Resources\\bin\\docker-machine.exe\" env myvm1 | Invoke-Expression   # Windows command to connect shell to myvm1\n    docker stack deploy -c &lt;file&gt; &lt;app&gt;                                                                 # Deploy an app; command shell must be set to talk to manager (myvm1), uses local Compose file\n    docker-machine scp docker-compose.yml myvm1:~                                                       # Copy file to node's home dir (only required if you use ssh to connect to manager and deploy the app)\n    docker-machine ssh myvm1 \"docker stack deploy -c &lt;file&gt; &lt;app&gt;\"                                      # Deploy an app using ssh (you must have first copied the Compose file to myvm1)\n    eval $(docker-machine env -u)                                                                       # Disconnect shell from VMs, use native docker\n    docker-machine stop $(docker-machine ls -q)                                                         # Stop all running VMs\n    docker-machine rm $(docker-machine ls -q)                                                           # Delete all VMs and their disk images\n</code></pre>"},{"location":"devops/docker/#part-5-stacks","title":"Part 5 Stacks","text":"<p>Ce qu'il faut retenir : Une Stack est d\u00e9finit comme suit : A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together. A single stack is capable of defining and coordinating the functionality of an entire application (though very complex applications may want to use multiple stacks). Ajouter un service dans une Stack revient \u00e0 ajouter un payload dans la partie services du fichier docker-compose.yml.</p> <pre><code>version: \"3\"\nservices:\n  web:\n    # replace username/repo:tag with your name and image details\n    image: username/repo:tag\n    deploy:\n      replicas: 5\n      restart_policy:\n        condition: on-failure\n      resources:\n        limits:\n          cpus: \"0.1\"\n          memory: 50M\n    ports:\n      - \"80:80\"\n    networks:\n      - webnet\n  visualizer:\n    image: dockersamples/visualizer:stable\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n    networks:\n      - webnet\n  redis:\n    image: redis\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - \"/home/docker/data:/data\"\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n    command: redis-server --appendonly yes\n    networks:\n      - webnet\nnetworks:\n  webnet:\n</code></pre> <p>Pour manager les stacks on utilise la commande <code>docker stack</code>. Pour persister les donn\u00e9es d'un container on utilise les volumes. Il est n\u00e9c\u00e9ssaire de connaitre la mani\u00e8re dont ont \u00e9t\u00e9 build\u00e9s les images qu'on utilise pour persister les donn\u00e9es et exposer les services.</p>"},{"location":"devops/docker/#part-6-deploy-your-app","title":"Part 6 Deploy your app","text":"<p>Ce qu'il faut retenir : Rien de plus que ce qui est pr\u00e9sent\u00e9 en part 3/4/5. Il faut s'assurer que l'ensemble des machines est acc\u00e8s \u00e0 un registry.</p> <p>Cheat sheet : </p> <pre><code>    docker stack deploy -c docker-compose.yml getstartedlab\n\n    Creating network getstartedlab_webnet\n    Creating service getstartedlab_web\n    Creating service getstartedlab_visualizer\n    Creating service getstartedlab_redis\n\n    [getstartedlab] ~ $ docker node ls\n    ID                            HOSTNAME                                      STATUS              AVAILABILITY        MANAGER STATUS\n    9442yi1zie2l34lj01frj3lsn     ip-172-31-5-208.us-west-1.compute.internal    Ready               Active              \n    jr02vg153pfx6jr0j66624e8a     ip-172-31-6-237.us-west-1.compute.internal    Ready               Active              \n    thpgwmoz3qefdvfzp7d9wzfvi     ip-172-31-18-121.us-west-1.compute.internal   Ready               Active              \n    n2bsny0r2b8fey6013kwnom3m *   ip-172-31-20-217.us-west-1.compute.internal   Ready               Active              Leader   \n\n    [getstartedlab] ~/sandbox/getstart $ docker service ls\n    ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS\n    x3jyx6uukog9        dockercloud-server-proxy   global              1/1                 dockercloud/server-proxy          *:2376-&gt;2376/tcp\n    ioipby1vcxzm        getstartedlab_redis        replicated          0/1                 redis:latest                      *:6379-&gt;6379/tcp\n    u5cxv7ppv5o0        getstartedlab_visualizer   replicated          0/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp\n    vy7n2piyqrtr        getstartedlab_web          replicated          5/5                 sam/getstarted:part6    *:80-&gt;80/tcp\n\n    [getstartedlab] ~/sandbox/getstart $ docker service ps vy7n2piyqrtr\n    ID                  NAME                  IMAGE                            NODE                                          DESIRED STATE       CURRENT STATE            ERROR               PORTS\n    qrcd4a9lvjel        getstartedlab_web.1   sam/getstarted:part6   ip-172-31-5-208.us-west-1.compute.internal    Running             Running 20 seconds ago                       \n    sknya8t4m51u        getstartedlab_web.2   sam/getstarted:part6   ip-172-31-6-237.us-west-1.compute.internal    Running             Running 17 seconds ago                       \n    ia730lfnrslg        getstartedlab_web.3   sam/getstarted:part6   ip-172-31-20-217.us-west-1.compute.internal   Running             Running 21 seconds ago                       \n    1edaa97h9u4k        getstartedlab_web.4   sam/getstarted:part6   ip-172-31-18-121.us-west-1.compute.internal   Running             Running 21 seconds ago                       \n    uh64ez6ahuew        getstartedlab_web.5   sam/getstarted:part6   ip-172-31-18-121.us-west-1.compute.internal   Running             Running 22 seconds ago        \n</code></pre>"},{"location":"devops/docker/#annexe-sur-docker-machine","title":"Annexe sur docker-machine","text":"<p>Docker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like Azure, AWS, or Digital Ocean.</p> <p>Using docker-machine commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.</p>"},{"location":"devops/docker/#katacoda-get-started","title":"Katacoda : Get Started","text":""},{"location":"devops/docker/#scenario-1-deploying-your-first-docker-container","title":"Sc\u00e9nario 1 Deploying Your First Docker Container","text":"<p>Ce qu'il faut retenir : Pour chercher une image sur un registry on utilise la commande <code>docker search</code>. Un simple <code>docker run -d nom-de-l'image</code> permet de \"tirer\" une image depuis un repository et de \"d\u00e9marrer\" un container en mode d\u00e9mon. <code>docker inspect &lt;friendly-name|container-id&gt;</code> donne les d\u00e9tails du container. <code>docker logs &lt;friendly-name|container-id&gt;</code> donne les logs du container. <code>docker run -d --name &lt;friendly-name&gt; -p &lt;host-port&gt;:&lt;container-port&gt; &lt;image&gt;:&lt;tag&gt;</code> \"tire\" l'image depuis un repository \"d\u00e9marre\" un container en mode d\u00e9mon via un mapping de port entre l'host et le container. On peut manager le container via le friendly-name. <code>docker run -d --name &lt;friendly-name&gt; -p &lt;container-port&gt; &lt;image&gt;:&lt;tag&gt;</code> \"tire\" l'image depuis un repository \"d\u00e9marre\" un container en mode d\u00e9mon via un mapping de port dynamique. On peut manager le container via le friendly-name. <code>docker port &lt;friendly-name&gt; &lt;container-port&gt;</code> permet de connaitre le port dynamique utilis\u00e9. <code>docker run -d --name redisMapped -v /opt/docker/data/redis:/data redis</code> permet de persister les donn\u00e9es de redis dans /opt/docker/data/redis. <code>docker run ubuntu ps</code> \u00e9x\u00e9cute le container ubuntu et effectue la commande ps dans celui-ci. <code>docker run -it ubuntu bash</code> \u00e9x\u00e9cute le container ubuntu et ouvre un shell.  </p>"},{"location":"devops/docker/#scenario-2-deploy-static-html-site-as-container","title":"Sc\u00e9nario 2 Deploy Static HTML site as Container","text":"<p>Cr\u00e9er une \"image\" pour l'instancier comme un \"container\" cela revient \u00e0 partir d'une image \u00e9xistante et \u00e0 l'enrichir. Pour cela on va cr\u00e9er un fichier Dockerfile qu'on va \"builder\". Pour builder \"l'image\" on va utiliser la comamnde <code>docker build -t &lt;friendly-name&gt;:&lt;tag&gt; path_to_Dockerfile</code>. On peut voir \"l'image\" pr\u00e9c\u00e9demment cr\u00e9\u00e9 via la commande <code>docker images</code>. Pour instancier \"l'image\" en \"container\" on utilise la commande <code>docker run -d -p &lt;host-port&gt;:&lt;container-port&gt; &lt;friendly-name&gt;:&lt;tag&gt;</code>.  </p>"},{"location":"devops/docker/#scenario-3-building-container-images","title":"Sc\u00e9nario 3 Building Container Images","text":"<p>Tout  Dockerfile doit d\u00e9marrer depuis la commande <code>FROM &lt;image-name&gt;:&lt;tag&gt;</code> qu'on appel une \"base image\". On peut utiliser les commandes RUN ou COPY au sein de notre  Dockerfile. RUN permet d'\u00e9x\u00e9cuter une qqcnq commande. Par exemple on peut installer qqch ou compiler qqch. COPY permet de copier des fichiers du repertoire o\u00f9 se situe le  Dockerfile dans le container. La commande EXPOSE permet de d\u00e9finir le/les port(s) d'exposition du container. La commande CMD d\u00e9finit ce que fait le container au d\u00e9marrage. On l'utilise de de la mani\u00e8re suivante <code>CMD [\"nginx\",\"-g\",\"daemon off;\"]</code>. On peut aussi utiliser la commande ENTRYPOINT \u00e0 la place de  CMD. L\u00e0 o\u00f9 CMD peux \u00eatre surcharg\u00e9 au d\u00e9marrage du container, ENTRYPOINT d\u00e9finit une commande qui \u00e0 des arguments qu'on peux passer en ligne de commande.</p> <pre><code># This is your Editor pane. Write the Dockerfile here and\n# use the command line to execute commands\n# use the command line to execute commands\nFROM nginx:1.11-alpine\nCOPY ./index.html /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\",\"-g\",\"daemon off;\"]\n</code></pre>"},{"location":"devops/docker/#scenario-4-dockerizing-nodejs","title":"Sc\u00e9nario 4 Dockerizing Node.js","text":"<p>En cas de rebuild d'une image, Docker garde en cache tout ce qu'il s'est pass\u00e9. Si une modification survient dans les fichiers contenus dans le Dockerfile alors il ne rebuildera que ce qui pr\u00e9c\u00e8de la ligne du Dockerfile en question. Cela n\u00e9cessit\u00e9 d'orchestrer son Dockerfile correctement.  </p> <p>With NPM we only want to re-run npm install if something within our package.json file has changed. If nothing has changed then we can use the cache version to speed up deployment. By using <code>COPY package.json &lt;dest&gt;</code> we can cause the RUN npm install command to be invalidated if the package.json file has changed. If the file has not changed, then the cache will not be invalided, and the cached results of the npm install command will be used.  </p> <p>On peux s'affranchir du cache via l'option suivante de la commande build : <code>-no-cache=true</code>.  </p>"},{"location":"devops/docker/#scenario-5-optimise-builds-with-docker-onbuild","title":"Sc\u00e9nario 5 Optimise Builds With Docker OnBuild","text":"<p>La commande \"ONBUILD\" permet de d\u00e9finir des actions \u00e0 \u00e9x\u00e9cuter plus tard. Cette instruction est pratique d\u00e8s qu'on souhaite builder une image qui sera utilis\u00e9 comme \"base image\".</p> <pre><code>    FROM node:7\n    RUN mkdir -p /usr/src/app\n    WORKDIR /usr/src/app\n    ONBUILD COPY package.json /usr/src/app/\n    ONBUILD RUN npm install\n    ONBUILD COPY . /usr/src/app\n    CMD [ \"npm\", \"start\" ]\n\n    FROM node:7-onbuild\n    EXPOSE 3000\n</code></pre>"},{"location":"devops/docker/#scenario-6-ignoring-files-during-build","title":"Sc\u00e9nario 6 Ignoring Files During Build","text":"<p>Pour ignorer des fichiers au cours du build il suffit d'ajouter \u00e0 la racine du r\u00e9pertoire un fichier .dockerignore. Ce fichier est similaire au fichier .gitignore. Au passage on peux \u00e9x\u00e9cuter une commande dans le container via le passage de celle-ci apr\u00e8s la commande run comme ceci <code>docker run nopassword ls /app</code>. Une bonne pratique consiste \u00e0 ignorer le dossier <code>.git</code>.  </p>"},{"location":"devops/docker/#scenario-7-create-data-containers","title":"Sc\u00e9nario 7 Create Data Containers","text":"<p>On peux cr\u00e9er des container de donn\u00e9es qu'on appel des data containers. Pour cr\u00e9er un data containers, on utilise la commande <code>docker create -v /config --name dataContainer busybox</code>. L'option -v permet de connaitre le r\u00e9pertoire \"rw\". Pour copier des donner dans le container on utilise la commande <code>docker cp config.conf dataContainer:/config/</code>. Pour \u00e9x\u00e9cuter un autre container qui \"map\" le data container on utilise la commande <code>docker run --volumes-from dataContainer ubuntu ls /config</code>. Pour exporter le data container on utilise la commande <code>docker export dataContainer &gt; dataContainer.tar</code>.  </p>"},{"location":"devops/docker/#scenario-8-creating-networks-between-containers-using-links","title":"Sc\u00e9nario 8 Creating Networks Between Containers using Links","text":"<p>On peut cr\u00e9er des liens entre les containers pour faire communiquer deux service par exemple. On commence par \u00e9x\u00e9cuter un container cible par exemple <code>docker run -d --name redis-server redis</code>. Pour cr\u00e9er un lien on utilise la commande suivante : <code>docker run --link &lt;container-cible|id&gt;:&lt;alias&gt; &lt;container-source&gt;</code>. On peux via la commande pr\u00e9c\u00e9dante connecter une interface client \u00e0 son serveur. Dans le cas de Redis <code>docker run -it --link redis-server:redis redis redis-cli -h redis</code>. Ce qui est important dans tous \u00e7a c'est de comprendre ce qu'il se passe quand docker cr\u00e9\u00e9 un lien. Tout d'abbord il maj les variables d'environment de la machine source avec un certain nombre d'informations de la machine cible. Deuxiemement il met \u00e0 jour le fichier /etc/hosts de la machine source avec les informations de la machine cible.  </p>"},{"location":"devops/docker/#scenario-9-creating-networks-between-containers-using-networks","title":"Sc\u00e9nario 9 Creating Networks Between Containers using Networks","text":"<p>A la diff\u00e9rence des liens un docker network s'apparente \u00e0 un \"r\u00e9seau virtuel\" entre les containers. Pour cr\u00e9er un docker network on utilise la commande <code>docker network create backend-network</code>. Pour attacher un container \u00e0 ce docker network on utilise l'option <code>--net &lt;nom_du_rez&gt;</code> ex : <code>docker run -d --name=redis --net=backend-network redis</code>. Quand on attache des containers \u00e0 un docker network ceux-ci vont automatiquement partager leurs informations via le DNS embarqu\u00e9 dans docker. On peux voir ce DNS via la commande suivante <code>docker run --net=backend-network alpine cat /etc/resolv.conf</code>. On peux attacher un container \u00e0 un nouveau docker network via la commande <code>docker network connect frontend-network redis</code>. On peux voir les docker network via la commande <code>docker network ls</code>. Pour savoir quel container est rattach\u00e9 \u00e0 quel docker network on utilise la commande <code>docker network inspect frontend-network</code>. On peux d\u00e9connecter les containers connect\u00e9 au docker network via la commande <code>docker network disconnect frontend-network redis</code>.  </p>"},{"location":"devops/docker/#scenario-10-persisting-data-using-volumes","title":"Sc\u00e9nario 10 Persisting Data Using Volumes","text":"<p>Pour qu'un container persiste ses donner il faut les exporter dans un volume non-volatile (en dehors d'un qqcnq container). On utilise l'option \"-v\" de <code>docker run</code> pour sp\u00e9cifier le volume \"mapper\" qui va contenir les donn\u00e9es persist\u00e9es. Par exemple : <code>docker run  -v /docker/redis-data:/data --name r1 -d redis redis-server --appendonly yes</code>. On peut \"pipe\" les donn\u00e9es dans le container via la commande suivant : <code>cat data | docker exec -i r1 redis-cli --pipe</code>. On peut aussi \"mapper\" le volume d'un container A avec celui d'un container via l'option <code>--volume-from</code>. Par ex : <code>docker run --volumes-from r1 -it ubuntu ls /data</code>. On peut monter un volume avec des options comme dans l'exemple suivant : <code>docker run -v /docker/redis-data:/data:ro -it ubuntu rm -rf /data</code>.  </p>"},{"location":"devops/docker/#scenario-11-manage-container-log-files","title":"Sc\u00e9nario 11 Manage Container Log Files","text":"<p>Les logs d'un container sont accessibles via la commande <code>docker logs &lt;container-name|id&gt;</code>. Au d\u00e9marrage d'un container on peut choisir son \"log-driver\" via l'option <code>--log-driver=</code>. Par ex : <code>docker run -d --name redis-syslog --log-driver=syslog redis</code>. On peut d\u00e9sactiver les logs d'un container via l'option <code>--log-driver=none</code>. Par ex : <code>docker run -d --name redis-none --log-driver=none redis</code>. Pour connaitre la configuration des logs on utilise la comamnde suivante : <code>docker inspect --format '{{ .HostConfig.LogConfig }}' redis-server</code>.  </p>"},{"location":"devops/docker/#scenario-12-ensuring-container-uptime-with-restart-policies","title":"Sc\u00e9nario 12 Ensuring Container Uptime With Restart Policies","text":"<p>Un container peut comme n'importe quel process \"planter\". Docker via l'option <code>--restart</code> sp\u00e9cifer le nombre de red\u00e9marrage avant un v\u00e9ritable crash. Par exemple pour red\u00e9marrer 3 fois le container on utilise la commande : <code>docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example</code>. Pour red\u00e9marrer le container quoi qu'il arrive on utilisera la commande suivante : <code>docker run -d --name restart-always --restart=always scrapbook/docker-restart-example</code>.  </p>"},{"location":"devops/docker/#scenario-13-adding-docker-metadata-labels","title":"Sc\u00e9nario 13 Adding Docker Metadata &amp; Labels","text":"<p>On peut ajouter un certain nombre de metadata dans des containers ou des images. Les metadata sont g\u00e9r\u00e9es via ce qu'on appel des labels. Pour ajouter un label au d\u00e9marrage d'un container on utilise l'option <code>-l=&lt;value&gt;</code> de la commande <code>docker run</code>. Par exemple : <code>docker run -l user=12345 -d redis</code>. On peut ajouter un fichier entier contenant des lables via  <code>docker run --label-file=labels -d redis</code>. On peut cr\u00e9er des labels dans une image directement via le Dockerfile via la commande <code>LABEL</code>.  </p> <pre><code>  LABEL vendor=Katacoda  \n  LABEL vendor=Katacoda \\ com.katacoda.version=0.0.5 \\ com.katacoda.build-date=2016-07-01T10:47:29Z \\ com.katacoda.course=Docker  \n</code></pre> <p>Pour connaitre les labels d'une image ou d'un container on utilise la commande <code>docker inspect rd</code>. On peut alors \"filtrer\" les containers ou les images sur des labels via l'option <code>--filter</code>. Par exemple <code>docker ps --filter \"label=user=scrapbook\"</code> ou encore <code>docker images --filter \"label=vendor=Katacoda\"</code>. Pour ajouter des labels directement dans le d\u00e9mon Unix :</p> <pre><code>    docker -d \\\n    -H unix:///var/run/docker.sock \\\n    --label com.katacoda.environment=\"production\" \\\n    --label com.katacoda.storage=\"ssd\"\n</code></pre>"},{"location":"devops/docker/#scenario-14-load-balancing-containers","title":"Sc\u00e9nario 14 Load Balancing Containers","text":"<p>Ce sc\u00e9nario utilise un container sp\u00e9cifique \"nginx_proxy\" pour proxyfier et load-balancer des containers web. Le seul int\u00e9r\u00eat de ce tuto est qu'il aborde la notion de \"Service Discovery\" via les \"Docker's API.\".  </p>"},{"location":"devops/docker/#scenario-15-orchestration-using-docker-compose","title":"Sc\u00e9nario 15 Orchestration using Docker Compose","text":"<p>Docker Compose permet d'orchestrer des applications mutli-containers. Docker Compose est bas\u00e9 sur un fichier YAML docker-compose.yml qui \u00e0 la forme suivante.  </p> <pre><code>container_name:\n  property: value\n    - or options\n</code></pre> <p>Pour d\u00e9marrer l'application on utilise la commande <code>docker-compose up -d</code>. Pour voir les containers d\u00e9marr\u00e9 on utilise la commande <code>docker-compose ps</code>. Pour acc\u00e9der au log <code>docker-compose log</code>. Pour scaler un service pr\u00e9cis on utilise la commande suivante : <code>docker-compose scale web=3</code>. Pour arr\u00e9ter une application on utilisera <code>docker-compose stop</code>. Pour supprimer tous les containers <code>docker-compose rm</code>. Attention docker-compose n'est pas la meme chose que docker stack (voir le lien suivant https://nickjanetakis.com/blog/docker-tip-23-docker-compose-vs-docker-stack )</p>"},{"location":"devops/docker/#scenario-16-see-container-metrics-with-docker-stats","title":"Sc\u00e9nario 16 See Container Metrics With Docker Stats","text":"<p>Pour connaitre les m\u00e9trics d'un container on utilise la commande <code>docker stats nginx</code>. Pour connaitre les m\u00e9trics de plusieurs containers on utilise la commande <code>docker ps -q | xargs docker stats</code>.  </p>"},{"location":"devops/docker/#scenario-17-creating-optimised-docker-images-using-multi-stage-builds","title":"Sc\u00e9nario 17 Creating Optimised Docker Images using Multi-Stage Builds","text":"<p>The Multi-Stage feature allows a single Dockerfile to contain multiple stages in order to produce the desired, optimised, Docker Image. Previously, the problem would have been solved with two Dockerfiles. One file would have the steps to build the binary and artifacts using a development container, the second would be optimised for production and not include the development tools. By removing development tooling in the production image, you reproduce the attack surface and improve the deployment time.  </p> <p>Un exemple de fichier Multi-Stage.</p> <pre><code># First Stage\nFROM golang:1.6-alpine\n\nRUN mkdir /app\nADD . /app/\nWORKDIR /app\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n\n# Second Stage\nFROM alpine\nEXPOSE 80\nCMD [\"/app\"]\n\n# Copy from first stage\nCOPY --from=0 /app/main /app\n</code></pre> <p>Pour Builder l'image on utilise la commande suivant <code>docker build -f Dockerfile.multi -t golang-app .</code></p>"},{"location":"devops/docker/#scenario-18-formatting-ps-output","title":"Sc\u00e9nario 18 Formatting PS Output","text":"<p>On peux formatter la sortie de <code>docker ps</code> via l'option <code>--format</code>. <code>docker ps --format '{{.Names}} container is using {{.Image}} image'</code>. <code>docker ps --format 'table {{.Names}}\\t{{.Image}}'</code>. <code>docker ps -q | xargs docker inspect --format '{{ .Id }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}'</code>.  </p>"},{"location":"devops/docker/#scenario-19-learn-docker-swarm","title":"Sc\u00e9nario 19 Learn Docker Swarm","text":"<p>On peux \"clusteriser\" docker avec le mode Swarm. On d\u00e9marre un cluster Swarm avec la commande <code>docker swarm init</code>. Le premier cluster devient un cluster \"manager\" il renseigne un token qu'il faut garder dans un endroit s\u00e9curis\u00e9. Ce token permet aux autres \"node\" de s'enregistrer au cluster. Depuis un \"node\" externe au cluster, on peut utiliser la commande suivante pour r\u00e9cup\u00e9rer le token : <code>token=$(docker -H 172.17.0.33:2345 swarm join-token -q worker) &amp;&amp; echo $token</code>. on rejoit alors le cluster avec la commande suivante : <code>docker swarm join 172.17.0.33:2377 --token $token</code>. Pour connaitre la liste des \"nodes\" qui composent le cluster on utilise la commande <code>docker node ls</code> depuis le \"master\".</p> <p>Le mode Swarm d\u00e9ploie un model de r\u00e9seau nouveau et plus robuste, permettant le dialogue et la scalabilit\u00e9 entre chaque \"node\" du cluste. Pour cr\u00e9er un reseau \"overlay\" dans le cluster swarm on utilise la commande : <code>docker network create -d overlay skynet</code> depuis le \"master\".  </p> <p>Le mode Swarm ajoute la notion de \"service\" qui est \"au dessus\" du container. Celle-ci permet de d\u00e9fnir un ou plusieurs container composant un \"service\" avec des r\u00e8gles de deploiement de r\u00e9plication de scalabilit\u00e9 ... Dans cet exemple on va cr\u00e9er un service qui contient deux serveur http. Ceux-ci sont automatiquement r\u00e9pliqu\u00e9 et load balanc\u00e9 sur l'ensemble du cluster swarm. <code>docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server</code>. Pour connaitre les services en cours on utilise la commande <code>docker service ls</code> depuis le \"master\". Pour connaitre l'\u00e9tat d'un service en particulier <code>docker service ps &lt;service_name&gt;</code>. Pour inspecter un service en particulier <code>docker service inspect --pretty &lt;service_name&gt;</code>. Pour connaitre les containers du \"node\" <code>docker node ps self</code>. Depuis le master pour connaitre les containers d'un \"node\" <code>docker node ps $(docker node ls -q | head -n1)</code>. Pour scaler le nombre d'instance d'un service en cli : <code>docker service scale &lt;service_name&gt;=&lt;nombre_d'instance&gt;</code> ex : <code>docker service scale http=5</code></p>"},{"location":"devops/docker/#source","title":"Source","text":"<p>Get Docker Get Docker CE for CentOS Post-installation steps for Linux Get Started Docker Machine Katacoda Docker</p>"},{"location":"devops/git/","title":"Git : Installation et Configuration","text":""},{"location":"devops/git/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Windows 10 Professionnel 1709 Git For Windows 2.16.2.windows.1"},{"location":"devops/git/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>La proc\u00e9dure d'installation de Git for Windows sur Windows 10 se d\u00e9roule de la fa\u00e7on suivante :</p> <p>Commencez par ex\u00e9cuter l'installeur Git-2.16.2-64-bit.exe.  </p> <p>Dans la fen\u00eatre Information cliquez sur Next </p> <p>Dans la fen\u00eatre Select Destination Location laissez par d\u00e9fault et cliquez sur Next </p> <p>Dans la fen\u00eatre Select Components on choisira de ne pas associer les fichiers .sh pour ne pas interf\u00e9rer avec le bash Ubuntu. On veillera aussi \u00e0 ce que les mises \u00e0 jour soient v\u00e9rifi\u00e9es quotidiennement. Apr\u00e8s avoir coch\u00e9/d\u00e9coch\u00e9 les checkbox en question, cliquez sur  Next </p> <p>Dans la fen\u00eatre Select Start Menu Folder laissez par d\u00e9fault et cliquez sur Next </p> <p>Dans la fen\u00eatre Choosing the default editor used by Git on continuera d'utiliser vim du fait d'une mauvaise int\u00e9gration de Notepad++ en cas d'installation personnalis\u00e9e. Apr\u00e8s avoir choisit Vim, cliquez sur  Next </p> <p>Dans la fen\u00eatre Adjusting your Path environment laissez par d\u00e9fault et cliquez sur Next </p> <p>Dans la fen\u00eatre Choosing HTTPS transport backend laissez par d\u00e9fault et cliquez sur Next </p> <p>Dans la fen\u00eatre Configuring the line ending conversions choisissez la deuxi\u00e8me optionset cliquez sur Next </p> <p>Dans la fen\u00eatre Configuring the terminal emulator to use with Git Bash laissez par d\u00e9fault et cliquez sur Next </p> <p>Dans la fen\u00eatre Configuring extra options laissez par d\u00e9fault et cliquez sur Next </p>"},{"location":"devops/git/#procedure-de-configuration","title":"Proc\u00e9dure de configuration","text":"<p>La proc\u00e9dure de confiuguration de Git for Windows sur Windows 10 se d\u00e9roule de la fa\u00e7on suivante :  </p> <p>Commencez par \u00e9x\u00e9cuter les deux commandes suivantes :</p> <pre><code>git config --global user.email \"prenom.nom@gmail.com\"\ngit config --global user.name \"GitHub account name\"\n</code></pre>"},{"location":"devops/gitlab/","title":"Gitlab","text":""},{"location":"devops/gitlab/#gitlab-installation-et-configuration","title":"Gitlab : Installation et Configuration","text":""},{"location":"devops/gitlab/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Gitlab 11.10.4 Docker 18.09.5"},{"location":"devops/gitlab/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/gitlab/#note-en-vrac","title":"Note en vrac","text":"<p>Leur id\u00e9e r\u00e9duire le temps pass\u00e9 \u00e0 manager les outils et leurs interconnexions en ce concentrant sur la valeur clef\u00a0: Le code source et les feature business.  </p> <p>A propos de leur site web : Certaine fonctionnalit\u00e9s sont mise en avant via une page d\u00e9di\u00e9 comme : Source Code Management D'autres sont des rappels \u00e0 la documentation : Wiki Les \"missing feature\" peuvent appara\u00eetre comme des \u00e9pics dans gitlab : Design Management</p> <p>Tests :</p> <ul> <li>Runners</li> <li>Auto DevOps</li> </ul>"},{"location":"devops/gitlab/#avant-propos","title":"Avant propos","text":"<p>GitLab is a single application that provides everything you need to Manage, Plan, Create, Verify, Package, Release, Configure, Monitor, and Secure your applications.</p>"},{"location":"devops/gitlab/#about-gitlab","title":"About Gitlab","text":"<p>The entire DevOps lifecycle in one application. Manage -&gt; Plan -&gt; Create -&gt; Verify -&gt; Package -&gt; Release -&gt; Configure -&gt; Monitor -&gt; Secure </p>"},{"location":"devops/gitlab/#devops-lifecycle","title":"DevOps lifecycle","text":"<p>Manage : Gain visibility and insight into how your business is performing.  </p> <p>Authentication and Authorization</p> <ul> <li>Protected tags</li> <li>Enforced Two-factor Authentication (2FA)</li> </ul> <p>Workflow Policies</p> <ul> <li>Custom header and footer system message in web and email</li> </ul> <p>Plan : Regardless of your process, GitLab provides powerful planning tools to keep everyone synchronized.  </p> <p>Project Management</p> <ul> <li>Issues</li> <li>Description Templates</li> <li>Task Lists</li> <li>Labels</li> <li>Jira Integration</li> </ul> <p>Kanban Boards</p> <ul> <li>Project Issue Board</li> <li>Group Issue Board</li> </ul> <p>Time Tracking</p> <ul> <li>Time Tracking</li> </ul> <p>Agile Portfolio Management</p> <ul> <li>Scrum</li> <li>DevOps Pipeline</li> <li>Kanban</li> </ul> <p>Create : Create, view, and manage code and project data through powerful branching tools.  </p> <p>Source Code Management</p> <ul> <li>Commit graph and reporting tools</li> <li>Task Lists</li> <li>Discussions</li> <li>Merge Requests</li> <li>Protected branches</li> <li>Private profile page</li> <li>Merge request reviews (Premium Ultimate)</li> <li>Git is fast</li> <li>Git LFS 2.0 support</li> <li>Project badges</li> <li>Keep personal email private</li> </ul> <p>Code Review</p> <ul> <li>Assignee</li> <li>Suggest changes</li> </ul> <p>Web IDE</p> <ul> <li>Web IDE</li> </ul> <p>Verify : Keep strict quality standards for production code with automatic testing and reporting.  </p> <p>Continuous Integration (CI)</p> <ul> <li>Built-in CI/CD</li> <li>CI/CD Horizontal Autoscaling</li> <li>See JUnit test summaries in merge request widget</li> <li>Free CI/CD with shared or personal Runners</li> <li>Scheduled triggering of pipelines</li> <li>Group-level variables</li> </ul> <p>Package : Create a consistent and dependable software supply chain with built-in universal package management.  </p> <p>Container Registry</p> <ul> <li>Built-in Container Registry</li> </ul> <p>Secure : Security capabilities, integrated into your development lifecycle.  </p> <p>SAST, DAST, Dependency Scanning, Container Scanning, License Management</p> <ul> <li>Ultimate feature</li> </ul> <p>Release : GitLab's integrated CD solution allows you to ship code with zero-touch, be it on one or one thousand servers.  </p> <p>Continuous Delivery (CD)</p> <ul> <li>Comprehensive pipeline graphs</li> <li>Browsable artifacts</li> <li>Deploy Tokens</li> </ul> <p>Release Orchestration</p> <ul> <li>Keep track of releases using GitLab Releases</li> <li>Environments history</li> </ul> <p>Pages</p> <ul> <li>Publish static sites for free with GitLab Pages</li> </ul> <p>Configure : Configure your applications and infrastructure.  </p> <p>Kubernetes Configuration</p> <ul> <li>Easy Deployment of Helm, Ingress, and Prometheus on Kubernetes</li> <li>Easy integration of existing Kubernetes clusters</li> <li>Easy creation of Kubernetes clusters on GKE</li> </ul> <p>ChatOps</p> <ul> <li>Deploy from Chat</li> <li>Create, search and view issues from chat</li> </ul> <p>Serverless</p> <ul> <li>Serverless</li> <li>Serverless Monitoring</li> </ul> <p>Monitor : Automatically monitor metrics so you know how any change in code impacts your production environment.  </p> <p>Metrics</p> <ul> <li>Application performance monitoring</li> <li>GitLab server monitoring</li> <li>Log Correlation</li> <li>Cloud Native Monitoring</li> </ul> <p>Logging, Cluster Monitoring, Tracing</p> <ul> <li>Ultimate</li> </ul> <p>Defend : Defend your apps and infrastructure from security intrusions.  </p> <p>The DevOps lifecycle cross the arguments </p> <p>DevOps Tools Landscape </p>"},{"location":"devops/gitlab/#procedure-dinstallation-docker","title":"Proc\u00e9dure d'installation docker","text":"<p>Quick start </p> <p>Advanced deployment </p>"},{"location":"devops/gitlab/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":""},{"location":"devops/gitlab/#source","title":"Source","text":"<p>A beginner's guide to continuous integration About Agile Planning All features Auto DevOps Ce Or Ee Concurrent Devops Continuous Integration &amp; Delivery DevOps Lifeycle DevOps Tools Landscape DevSecOps Feature Comparison Handbook Resources Source Code Management Status Page Value Stream Management </p> <p>YouTube - Reseller Enablement - GitLab101</p>"},{"location":"devops/jenkins/","title":"Jenkins","text":""},{"location":"devops/jenkins/#jenkins-installation-et-configuration","title":"Jenkins : Installation et Configuration","text":""},{"location":"devops/jenkins/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Jenkins 2.176.2 Docker 19.03.1"},{"location":"devops/jenkins/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/jenkins/#note-en-vrac","title":"Note en vrac","text":"<p>Le hub docker de jenkins est divis\u00e9 en deux :  </p> <ul> <li>https://hub.docker.com/u/jenkinsci </li> <li>https://hub.docker.com/u/jenkins </li> </ul> <p>Le hub officiel est \"jenkins\" mais certain projet reste maintenu dans jenkinsci comme l'image \"blueocean\". La diff\u00e9rence entre jenkins/jenkins et jenkinsci/blueocean r\u00e9side dans l'installation par d\u00e9faut de blueocean.</p>"},{"location":"devops/jenkins/#avant-propos","title":"Avant propos","text":"<p>The leading open source automation server, Jenkins provides hundreds of plugins to support building, deploying and automating any project.</p>"},{"location":"devops/jenkins/#procedure-dinstallation-docker","title":"Proc\u00e9dure d'installation docker","text":"<p>Quick start </p> <pre><code>docker run \\\n  -u root \\\n  --rm \\\n  -d \\\n  -p 8080:8080 \\\n  -p 50000:50000 \\\n  -v jenkins-data:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  jenkinsci/blueocean\n</code></pre> <p>Advanced deployment </p>"},{"location":"devops/jenkins/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":""},{"location":"devops/jenkins/#source","title":"Source","text":"<p>Jenkins Jenkins CasC Demos Jenkins Configuration as Code Plugin Jenkins Docker GitHub Jenkins Docker Jenkins Documentation Jenkins Features Controlled with System Properties Jenkins on Jenkins </p>"},{"location":"devops/kubernetes/","title":"Containers Fundamentals","text":""},{"location":"devops/kubernetes/#virtualization-fundamentals","title":"Virtualization Fundamentals","text":""},{"location":"devops/kubernetes/#control-groups-cgroups","title":"Control Groups (cgroups)","text":"<p>Control groups, known as cgroups, are a feature of the Linux kernel allowing the limitation, accounting, and isolation of resources used by groups of processes and their subgroups.</p> <p></p>"},{"location":"devops/kubernetes/#namespaces","title":"Namespaces","text":"<p>Namespaces are a feature of the Linux kernel allowing groups of processes to have limited visibility of the host system resources.</p>"},{"location":"devops/kubernetes/#unification-file-system-unionfs","title":"Unification File System (UnionFS)","text":"<p>UnionFS is a feature found in the Linux, FreeBSD and NetBSD kernels, allowing the overlay of separate transparent file systems to produce an apparent single unified file system.</p> <p></p>"},{"location":"devops/kubernetes/#virtualization-mechanisms","title":"Virtualization Mechanisms","text":""},{"location":"devops/kubernetes/#full-virtualization-vs-operating-system-level-virtualization","title":"Full Virtualization vs. Operating System-Level Virtualization","text":"<p>Although Containers are not considered to be Virtual Machines (VMs), not even light-weight VMs, their similarities cannot be overlooked.  </p> <p></p> <p>A Virtual Machine is created on top of a hypervisor software. A typical application runs inside such a VM, and it requires extensive overhead to reach the physical hardware or the outside world considering that it has to go through so many layers of abstraction - the guest OS, then the hypervisor, and finally the host OS.</p> <p>A container is a light-weight environment that virtualizes and isolates resources for a running application. A container allows an application to be boxed and shipped with all its dependencies. Once deployed, a container runs directly on the host operating system. As a result, the user space component of the container should be compatible with the host operating system</p>"},{"location":"devops/kubernetes/#operating-system-level-virtualization","title":"Operating System-Level Virtualization","text":"<p>Operating system-level virtualization refers to a kernel\u2019s capability to allow the creation and existence of multiple isolated virtual environments on the same host. As opposed to programs running inside a real environment, where they see all resources such as CPU, network, connected devices, and files, programs running inside a virtual environment are limited to its content and assigned devices. Operating System level virtualization is typically used to limit usage and securely isolate resources shared between multiple programs or users, and to separate programs to run in their own assigned virtual environments for better security and resource management. While Operating System level virtualization requires less overhead than full virtualization because everything is managed at the kernel level without the need to install a guest OS, it limits the OS of the virtual environments to the host Operating System.  </p>"},{"location":"devops/kubernetes/#mechanisms-implementing-operating-system-level-virtualization","title":"Mechanisms Implementing Operating System-Level Virtualization","text":"<p>While one of the first known mechanisms to implement operating system-level virtualization dates from the early 1980s, the majority of such mechanisms known today were released after the turn of the 21st century</p>"},{"location":"devops/kubernetes/#chroot","title":"Chroot","text":"<p>Chroot is a mechanism implementing OS-level virtualization. It was first introduced on Unix Version 7 in 1979, then in 1982 it was added to BSD. Any user process and/or its children running inside this virtual chrooted directory tree runs under the false impression that it is in the real root directory tree of the operating system.  </p>"},{"location":"devops/kubernetes/#freebsd-jails","title":"FreeBSD Jails","text":"<p>A FreeBSD jail is a mechanism implementing OS-level virtualization with very little overhead. It was first introduced in 2000 on FreeBSD systems. FreeBSD jail allows for the partitioning of a FreeBSD system into many independent systems, called jails. They share the same kernel, but virtualize the system\u2019s files and resources for improved security and administration through clean isolation between services. Jails become virtual environments running on the host system with virtualized filesystem, processes, and users. Jails share the host kernel instead.</p>"},{"location":"devops/kubernetes/#solaris-zones","title":"Solaris Zones","text":"<p>Solaris zone is a mechanism implementing OS-level virtualization. It was first introduced in 2004 on Solaris 10 systems. Solaris zones represent securely isolated Virtual Machines on a single host system. Zones may host single or multiple applications, services, and their children. Each zone on a host system virtualizes its hostname, network, IP address, and it has assigned storage.</p>"},{"location":"devops/kubernetes/#openvz","title":"OpenVZ","text":"<p>OpenVZ is a mechanism implementing OS-level virtualization; it was first introduced in 2005 on Linux systems. OpenVZ allows a physical host to run multiple isolated virtual instances - called containers, virtual environments or virtual private servers. OpenVZ containers share the same kernel, and can only run Linux.</p>"},{"location":"devops/kubernetes/#linux-containers-lxc","title":"Linux Containers (LXC)","text":"<p>LXC, or Linux Containers, is a mechanism implementing OS-level virtualization; it was first introduced in 2008 on Linux systems. LXC allows multiple isolated systems to run on a single Linux host, using chroot and cgroups, together with namespace isolation features of the Linux kernel to limit resources, set priorities, and isolate processes, the filesystem, network and users from the host operating system.</p>"},{"location":"devops/kubernetes/#systemd-nspawn","title":"Systemd-nspawn","text":"<p>Systemd-nspawn is a mechanism implementing OS-level virtualization; it was first introduced in 2010 on Linux systems. Systemd-nspawn may be used to run a simple script or boot an entire Linux-like operating system in a container.</p>"},{"location":"devops/kubernetes/#container-standards-and-runtimes","title":"Container Standards and Runtimes","text":""},{"location":"devops/kubernetes/#container","title":"Container","text":"<p>Most times, to ease the understanding of the concept, a container is compared to a regular Virtual Machine - which is a result of full virtualization. A container, however, is the product of several OS-level virtualization features of the Linux kernel used in conjunction to build a lightweight isolated environment There are quite a few differences as well, between VMs and containers: containers use the host kernel and are bound to boot the host OS only, and are processes running on the host system managed individually or in groups.</p>"},{"location":"devops/kubernetes/#what-are-containers","title":"What Are Containers?","text":"<p>Application centric environments ensure an application\u2019s efficiency, performance, and responsiveness to external factors such as load spikes and in response the application scales accordingly. Containers are the application centric method to deliver high performing and scalable applications on any infrastructure. This ideal solution is, in fact, a container image, and the isolated and secure execution environment created from it at runtime is a container.</p>"},{"location":"devops/kubernetes/#where-are-containers-deployed","title":"Where Are Containers Deployed?","text":"<p>Containers are able to run anywhere.  </p>"},{"location":"devops/kubernetes/#standards","title":"Standards","text":"<p>Being able to move container images between different container runtime environments, by enabling containers to be interoperable and allowing them to integrate with third-party storage and network plugins of various projects and vendors, are just a few of the benefits of container standards. The App Container standard has not received any new features  </p>"},{"location":"devops/kubernetes/#container-image-standards-app-container-appc","title":"Container Image Standards - App Container (appc)","text":"<p>The App Container (appc) specification was introduced in 2014 by CoreOS in collaboration with Google and RedHat One of the container runtimes implementing the appc specification is rkt  </p> <p>App Container Image The App Container Image (ACI) defines the packaging, compression and extraction methods of files that make up the container image together with the validation of container image\u2019s integrity.  </p> <p>App Container Image Discovery The App Container Image Discovery defines how a container image name is linked to a downloadable container image.  </p> <p>App Container Pod The App Container Pod defines a Pod as the deployment and execution unit for one or a group of container images.  </p> <p>App Container Executor The App Container Executor (ACE) defines how to run an app container image, more specifically environment configuration for the running app, and the app\u2019s interaction with the environment.  </p>"},{"location":"devops/kubernetes/#container-image-standards-open-container-initiative-oci","title":"Container Image Standards - Open Container Initiative (OCI)","text":"<p>The Open Container Initiative (OCI) was introduced in 2015 by Docker together with other leaders in the container industry One of the container runtimes implementing the OCI specification is runC.  </p> <p>Runtime Specification The Runtime Specification defines how to run a \"filesystem bundle\" that is unpacked on disk. An OCI implementation would download and unpack an OCI image into an OCI Runtime filesystem bundle. Then, an OCI Runtime would run the OCI Runtime Bundle.</p> <p>The Image Specification helps with the development of compatible tools to ensure consistent container image conversion into containers.</p>"},{"location":"devops/kubernetes/#unification-process","title":"Unification Process","text":"<p>To ease the unification process between the two existing standards, features of the appc standard are being merged into OCI</p>"},{"location":"devops/kubernetes/#container-runtimes","title":"Container Runtimes","text":"<p>A container runtime is guided by a runtime specification, which describes the configuration, execution environment and the lifecycle of the container. he role of a container runtime is to provide an environment supporting basic operations with images and the running containers, that is both configurable and consistent, where container processes are able to run. A container runtime is designed to perform some default operations under the hood as a response to user commands.  </p> <ul> <li>Extracts the container image</li> <li>Stores it on an overlay filesystem</li> <li>When the runtime executes a container, it interacts with the kernel to set resource limits via control groups and namespaces.</li> </ul>"},{"location":"devops/kubernetes/#runc","title":"runc","text":"<ul> <li>Developed by Docker.</li> <li>runc implements the OCI specification.</li> <li>does not expose an API.</li> <li>does not provide container image management.</li> <li>does not support image build operations.</li> <li>does not provide image download or image integrity check capabilities.</li> <li>does not include a centralized daemon, it may be integrated with the Linux service manager - systemd.</li> </ul>"},{"location":"devops/kubernetes/#containerd","title":"containerd","text":"<ul> <li>Adopt by Docker engine, Kubernetes services of IBM (IKS) and Google Cloud (GKE), Cloud Foundry, and Kata Containers.</li> <li>implements the OCI specification.</li> <li>support several container operations, (storage and transfer  executing, attaching storage and network to containers.).</li> <li>used runc at low level</li> <li>containerd adds implementation for some missing, yet desired, capabilities of runc</li> </ul>"},{"location":"devops/kubernetes/#docker","title":"Docker","text":"<p>Docker is one of the most robust and complex container development and management platforms in the container industry. Although we list it here as a container runtime, Docker is a lot more than just a simple, or not so simple, container platform. The Docker platform supports a wide range of operations, from container image management to container lifecycle and runtime management. Earlier Docker versions were based on the Linux Containers (LXC). Then ocker decided to develop in-house its own proprietary runtime, libcontainer, its a component of runc. Docker\u2019s complexity is not only reflected by the multitude of operations it supports on container images and running containers, but also by its architecture. Docker is powered by the Docker Engine - a client-server application. The Docker Engine is composed of the Docker host running the Docker daemon, a REST API to communicate with the daemon, and a Docker client </p> <p>Docker client This is the command-line tool that allows users to run docker commands against a Docker daemon running on a Docker host. The client and daemon communicate through REST API, over Unix sockets or a network interface. Also, the client is capable of communicating with more than one daemon running on different Docker hosts.</p> <p>Docker host The Docker host is a system running the Docker daemon - called dockerd. The daemon is responsible for building, running, and distributing Docker containers. He manage docker objects such as images, containers, networks, and volumes. He can interact with other daemons to manage distributed Docker services across multiple Docker hosts clustered together.</p> <p>Docker registries The Docker registries store Docker container images.</p> <p>Docker Desktop The Docker Desktop is the installer for the Docker framework components on Windows and Mac systems.</p>"},{"location":"devops/kubernetes/#rkt","title":"rkt","text":"<p>Developed by CoreOS, rkt is a simpler application container runtime that implements the modern App Container (appc) specification and relies on the Application Container Image (ACI) format. rkt is an application container engine developed for modern production cloud-native environments.  </p> <ul> <li>It features a pod-native approach.  </li> <li>rkt may be integrated with systemd.  </li> </ul> <p>When running containers in pods, rkt goes through three different stages in execution:</p> <ul> <li>Stage 0 invokes the rkt binary in order to perform various initialization steps.</li> <li>Stage 1 isolates the pod\u2019s environment using chroot, cgroups and namespaces.</li> <li>Stage 2 runs the applications in the environment setup by Stage 1.</li> </ul> <p>rkt also supports full hardware virtualization, specific to Virtual Machines. rkt is interoperable as well, being able to query multiple container image registries, such as quay of CoreOS, Docker Hub. Despite its initial popularity, the rkt project reached its end.  </p>"},{"location":"devops/kubernetes/#cri-o","title":"CRI-O","text":"<p>CRI-O is a minimal implementation of the Container Runtime Interface (CRI) to enable the usage of any  Open Container Initiative (OCI) compatible runtime with Kubernetes, a popular container orchestrator. CRI-O supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in instead. The CRI-O runtime has been optimized for Kubernetes, and it also implements the Container Network Interface (CNI) for networking and supports CNI plugins. CRI-O is packed with libraries that pull container images from registries and create container filesystems. CRI-O also supports container security such as SELinux, capabilities or seccomp.  </p>"},{"location":"devops/kubernetes/#image-operations","title":"Image Operations","text":""},{"location":"devops/kubernetes/#container-operations","title":"Container Operations","text":""},{"location":"devops/kubernetes/#building-container-images","title":"Building Container Images","text":""},{"location":"devops/kubernetes/#container-networking","title":"Container Networking","text":""},{"location":"devops/kubernetes/#container-storage","title":"Container Storage","text":""},{"location":"devops/kubernetes/#course-completion","title":"Course Completion","text":""},{"location":"devops/longhorn/","title":"Longhorn","text":""},{"location":"devops/longhorn/#longhorn","title":"Longhorn","text":""},{"location":"devops/longhorn/#tools-versions","title":"Tools versions","text":"Os / Tool Version Sles 15sp6 Rke2 1.28.14 Longhorn 1.7.2"},{"location":"devops/longhorn/#what-is-longhorn","title":"What is Longhorn?","text":"<p>Longhorn is a lightweight, reliable, and powerful distributed block storage system for Kubernetes.</p> <p>Longhorn implements distributed block storage using containers and microservices. Longhorn creates a dedicated storage controller for each block device volume and synchronously replicates the volume across multiple replicas stored on multiple nodes. The storage controller and replicas are themselves orchestrated using Kubernetes.</p> <p>With Longhorn, you can:</p> <ul> <li>Use Longhorn volumes as persistent storage for the distributed stateful applications in your Kubernetes cluster</li> <li>Partition your block storage into Longhorn volumes so that you can use Kubernetes volumes with or without a cloud provider</li> <li>Replicate block storage across multiple nodes and data centers to increase availability</li> <li>Store backup data in external storage such as NFS or AWS S3</li> <li>Create cross-cluster disaster recovery volumes so that data from a primary Kubernetes cluster can be quickly recovered from backup in a second Kubernetes cluster</li> <li>Schedule recurring snapshots of a volume, and schedule recurring backups to NFS or S3-compatible secondary storage</li> <li>Restore volumes from backup</li> <li>Upgrade Longhorn without disrupting persistent volumes</li> </ul> <p>Longhorn comes with a standalone UI, and can be installed using Helm, kubectl, or the Rancher app catalog.</p>"},{"location":"devops/minikube/","title":"Minikube : Installation et Configuration","text":""},{"location":"devops/minikube/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Debian 9.5.0 Windows 10 18.03 Kubernetes 1.12.2 Kubectl 1.12.2 Minikube v0.30.0"},{"location":"devops/minikube/#avant-propos","title":"Avant propos","text":"<p>Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.</p> <p>Minikube est une sorte de \"vagrant\" d\u00e9di\u00e9 \u00e0 kubernetes. Il permet d'instancier un kubernetes d'un seul noeud via un provider virtuel. L'installation de Minikube fournit un simple CLI. On peut :</p> <ul> <li>Instancier un cluster (de un noeud) de</li> <li>l'arr\u00e9ter,</li> <li>le configurer,</li> <li>si connecter etc ...  </li> <li>Y installer des modules.</li> </ul> <p>Minikube n'installe pas kubenetes et n'op\u00e8re pas kubenetes. Au premier minikube start celui ci va t\u00e9l\u00e9charger une machine virtuel choisit en fonction d'un provider et la configur\u00e9 comme on le souhaite. On manage donc la vm et la configuration du kubernetes de minikube via le cli, mais on op\u00e8re le cluster via kubectl comme sur une instance classique.</p> <p>La liste des providers fournit disponible pour minikube :</p> <ul> <li>virtualbox</li> <li>vmwarefusion</li> <li>KVM2</li> <li>hyperkit</li> <li>xhyve</li> <li>hyperv</li> <li>none</li> </ul>"},{"location":"devops/minikube/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>Comme expliqu\u00e9 pr\u00e9c\u00e9demment minikube n'est qu'un \"simple\" CLI. Vous pouvez l'installer sur plusieur OS de diff\u00e9rentes mani\u00e8re.  </p> <ul> <li>Via des packages binaire natif \u00e0 l'os  </li> <li>En t\u00e9l\u00e9charchant et en \u00e9x\u00e9cutant l'\u00e9x\u00e9cutable</li> </ul>"},{"location":"devops/minikube/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>Verifying the Installation :  </p>"},{"location":"devops/minikube/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"devops/minikube/#kubernetes-basics","title":"Kubernetes Basics","text":"Commands Description"},{"location":"devops/minikube/#kubernetes-essential","title":"Kubernetes Essential","text":"Commands Description"},{"location":"devops/minikube/#kubernetes-advanced","title":"Kubernetes Advanced","text":"Commands Description"},{"location":"devops/minikube/#tutoriels-kubernetes","title":"Tutoriels Kubernetes","text":""},{"location":"devops/minikube/#source","title":"Source","text":"<p>Minikube Setup Minikube Tutorials Minikube Release Minikube Setup Without Vm Support Minikube Issue 2575 Minikube Issue 2762</p>"},{"location":"devops/molecule/","title":"Molecule","text":""},{"location":"devops/molecule/#molecule-installation-and-configuration","title":"Molecule: Installation and Configuration","text":""},{"location":"devops/molecule/#tools-versions","title":"Tools versions","text":"Os / Tool Version Linux Mint 19.3 Ansible 2.9.6 Molecule 3.0.3"},{"location":"devops/molecule/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/molecule/#bulk-note","title":"Bulk Note","text":"<p>N/A</p>"},{"location":"devops/molecule/#about","title":"About","text":"<p>Molecule is designed to aid in the development and testing of Ansible roles. Molecule provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule encourages an approach that results in consistently developed roles that are well-written, easily understood and maintained.</p>"},{"location":"devops/molecule/#installation-procedure","title":"Installation procedure","text":"<p>Centos requirements</p> <pre><code>sudo yum install -y epel-release\nsudo yum install -y gcc python3-pip python3-devel openssl-devel libselinux-python\n</code></pre> <p>Debian requirements</p> <pre><code>sudo apt-get update\nsudo apt-get install -y python3-pip libssl-dev\n</code></pre> <p>Molecule</p> <pre><code>python3 -m pip install --upgrade --user setuptools\npython3 -m pip install --user \"molecule[lint]\"\n</code></pre> <p>Docker</p> <pre><code>docker pull quay.io/ansible/molecule:3.0.3\n</code></pre>"},{"location":"devops/molecule/#getting-start","title":"Getting start","text":"<p>Creating a new role</p> <pre><code>molecule init role -r my-new-role\n</code></pre> <p>Molecule Scenario</p> <p>After init role there is a molecule folder wich containes a default folder wich is the default scenario. In this scenario we can found :</p> <ul> <li>Dockerfile.j2 Jinja2 template file in place. Molecule will use this file to build a docker image to test your role against.</li> <li>INSTALL.rst contains instructions on what additional software or setup steps you will need to take in order to allow Molecule to successfully interface with the driver.</li> <li>molecule.yml is the central configuration entrypoint for Molecule.</li> <li>converge.yml is the playbook file that contains the call site for your role.</li> <li>tests is the tests directory created because Molecule uses TestInfra as the default Verifier.</li> </ul> <p>Molecule.yml</p> <p>The molecule.yml is for configuring Molecule. It is a YAML file whose keys represent the high level components that Molecule provides. These are:</p> <ul> <li>The Dependency manager.</li> <li>The Driver provider.</li> <li>The Lint provider.</li> <li>The Platforms definitions.</li> <li>The Provisioner. Molecule only provides an Ansible provisioner.</li> <li>The Scenario definition.</li> <li>The Verifier framework.</li> </ul> <p>Run test sequence commands</p> <pre><code># Create instance\nmolecule create\n\n# List instance\nmolecule list\n\n# Execute playbook\nmolecule converge\n\n# Inspect instance\nmolecule login\n\n# Destroy instance\nmolecule destroy\n</code></pre> <p>Run a full test sequence</p> <pre><code>molecule test\n</code></pre>"},{"location":"devops/molecule/#usage","title":"Usage","text":"<ul> <li>check        Use the provisioner to perform a Dry-Run...</li> <li>cleanup      Use the provisioner to cleanup any changes...</li> <li>converge     Use the provisioner to configure instances...</li> <li>create       Use the provisioner to start the instances.</li> <li>dependency   Manage the role's dependencies.</li> <li>destroy      Use the provisioner to destroy the instances.</li> <li>idempotence  Use the provisioner to configure the...</li> <li>init         Initialize a new role or scenario.</li> <li>lint         Lint the role.</li> <li>list         Lists status of instances.</li> <li>login        Log in to one instance.</li> <li>matrix       List matrix of steps used to test instances.</li> <li>prepare      Use the provisioner to prepare the instances...</li> <li>side-effect  Use the provisioner to perform side-effects...</li> <li>syntax       Use the provisioner to syntax check the role.</li> <li>test         Test (lint, cleanup, destroy, dependency,...</li> <li>verify       Run automated tests against instances.</li> </ul>"},{"location":"devops/molecule/#config","title":"Config","text":""},{"location":"devops/molecule/#dependency","title":"Dependency","text":"<p>Testing roles may rely upon additional dependencies. Molecule handles managing these dependencies by invoking configurable dependency managers.  </p> <ul> <li>Ansible Galaxy</li> <li>Gilt</li> <li>Shell</li> </ul> <p>Example:</p> <pre><code>dependency:\n  name: galaxy\n  options:\n    ignore-certs: True\n    ignore-errors: True\n    role-file: requirements.yml\n</code></pre>"},{"location":"devops/molecule/#driver","title":"Driver","text":"<p>Molecule uses Ansible to manage instances to operate on. Molecule supports any provider Ansible supports. This work is offloaded to the provisioner. The driver\u2019s name is specified in molecule.yml, and can be overridden on the command-line. Molecule will remember the last successful driver used, and continue to use the driver for all subsequent subcommands, or until the instances are destroyed by Molecule.</p> <ul> <li>Delegated</li> <li>Docker (default)</li> <li>Podman</li> </ul> <p>Example:</p> <pre><code>driver:\n  name: docker\nplatforms:\n  - name: instance\n    hostname: instance\n    image: image_name:tag\n    dockerfile: Dockerfile.j2\n    pull: True|False\n    pre_build_image: True|False\n    registry:\n      url: registry.example.com\n      credentials:\n        username: $USERNAME\n        password: $PASSWORD\n        email: user@example.com\n        user: root\n    override_command: True|False\n    command: sleep infinity\n    tty: True|False\n    pid_mode: host\n    privileged: True|False\n    security_opts:\n      - seccomp=unconfined\n    devices:\n      - /dev/fuse:/dev/fuse:rwm\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\n    keep_volumes: True|False\n    tmpfs:\n      - /tmp\n      - /run\n    capabilities:\n      - SYS_ADMIN\n    sysctls:\n      net.core.somaxconn: 1024\n      net.ipv4.tcp_syncookies: 0\n    exposed_ports:\n      - 53/udp\n      - 53/tcp\n    published_ports:\n      - 0.0.0.0:8053:53/udp\n      - 0.0.0.0:8053:53/tcp\n    ulimits:\n      - nofile:262144:262144\n    dns_servers:\n      - 8.8.8.8\n    etc_hosts: \"{'host1.example.com': '10.3.1.5'}\"\n    networks:\n      - name: foo\n      - name: bar\n    network_mode: host\n    purge_networks: true\n    docker_host: tcp://localhost:12376\n    cacert_path: /foo/bar/ca.pem\n    cert_path: /foo/bar/cert.pem\n    key_path: /foo/bar/key.pem\n    tls_verify: true\n    env:\n      FOO: bar\n    restart_policy: on-failure\n    restart_retries: 1\n    buildargs:\n        http_proxy: http://proxy.example.com:8080/\n</code></pre>"},{"location":"devops/molecule/#lint","title":"Lint","text":"<p>Starting with v3, Molecule handles project linting by invoking and external lint commands as exemplified below.</p> <p>The decision to remove the complex linting support was not easily taken as we do find it very useful. The issue was that molecule runs on scenarios and linting is usually performed at repository level.</p> <p>It makes little sense to perform linting in more than one place per project. Molecule was able to use up to three linters and while it was aimed to flexible about them, it ended up creating more confusions to the users. We decided to maximize flexibility by just calling an external shell command.</p> <p>Example:</p> <pre><code>lint: |\n  set -e\n  yamllint .\n  ansible-lint\n  flake8\n</code></pre>"},{"location":"devops/molecule/#platforms","title":"Platforms","text":"<p>Platforms define the instances to be tested, and the groups to which the instances belong.</p> <p>Example:</p> <pre><code>platforms:\n  - name: instance-1\n    groups:\n      - group1\n      - group2\n    children:\n      - child_group1\n</code></pre>"},{"location":"devops/molecule/#provisioner","title":"Provisioner","text":"<p>Molecule handles provisioning and converging the role.</p> <ul> <li>Ansible</li> </ul> <p>Important</p> <p>Ansible is the default provisioner. No other provisioner will be supported.  </p> <p>Molecule\u2019s provisioner manages the instances lifecycle. However, the user must provide the create, destroy, and converge playbooks. Molecule\u2019s init subcommand will provide the necessary files for convenience.  </p> <p>Molecule will skip tasks which are tagged with either molecule-notest or notest. With the tag molecule-idempotence-notest tasks are only skipped during the idempotence action step.</p> <p>Reserve the create and destroy playbooks for provisioning. Do not attempt to gather facts or perform operations on the provisioned nodes inside these playbooks.  </p> <p>Molecule will remove any options matching \u2018^[v]+$\u2019, and pass -vvv to the underlying ansible-playbook command when executing molecule \u2013debug.  </p> <p>The playbook loading order is:</p> <ul> <li>provisioner.playbooks.$driver_name.$action</li> <li>provisioner.playbooks.$action</li> <li>bundled_playbook.$driver_name.$action</li> </ul> <p>side_effect playbook</p> <p>The side effect playbook executes actions which produce side effects to the instances(s). Intended to test HA failover scenarios or the like. It is not enabled by default. Add the following to the provisioner\u2019s playbooks section to enable.</p> <pre><code>provisioner:\n  name: ansible\n  playbooks:\n    side_effect: side_effect.yml\n</code></pre> <p>prepare playbook</p> <p>The prepare playbook executes actions which bring the system to a given state prior to converge. It is executed after create, and only once for the duration of the instances life.  </p> <pre><code>provisioner:\n  name: ansible\n  playbooks:\n    prepare: prepare.yml\n</code></pre> <p>cleanup playbook</p> <p>The cleanup playbook is for cleaning up test infrastructure that may not be present on the instance that will be destroyed. The primary use-case is for \u201ccleaning up\u201d changes that were made outside of Molecule\u2019s test environment. For example, remote database connections or user accounts. Intended to be used in conjunction with prepare to modify external resources when required.</p> <p>The cleanup step is executed directly before every destroy step. Just like the destroy step, it will be run twice. An initial clean before converge and then a clean before the last destroy step. This means that the cleanup playbook must handle failures to cleanup resources which have not been created yet.</p> <p>Add the following to the provisioner\u2019s playbooks section to enable.</p> <pre><code>provisioner:\n  name: ansible\n  playbooks:\n    cleanup: cleanup.yml\n</code></pre>"},{"location":"devops/molecule/#scenario","title":"Scenario","text":"<p>A scenario is a self-contained directory containing everything necessary for testing the role in a particular way. The default scenario is named default, and every role should contain a default scenario.</p> <pre><code>scenario:\n  create_sequence:\n    - dependency\n    - create\n    - prepare\n  check_sequence:\n    - dependency\n    - cleanup\n    - destroy\n    - create\n    - prepare\n    - converge\n    - check\n    - destroy\n  converge_sequence:\n    - dependency\n    - create\n    - prepare\n    - converge\n  destroy_sequence:\n    - dependency\n    - cleanup\n    - destroy\n  test_sequence:\n    - dependency\n    - lint\n    - cleanup\n    - destroy\n    - syntax\n    - create\n    - prepare\n    - converge\n    - idempotence\n    - side_effect\n    - verify\n    - cleanup\n    - destroy\n</code></pre>"},{"location":"devops/molecule/#verifier","title":"Verifier","text":"<p>Molecule handles role testing by invoking configurable verifiers.</p> <ul> <li>Ansible (default)</li> <li>Testinfra</li> </ul> <p>Ansible Molecule executes a playbook (verify.yml) located in the role\u2019s scenario.directory.</p> <p>Testinfra Testinfra is no longer the default test verifier since version 3.0.</p>"},{"location":"devops/molecule/#testing","title":"Testing","text":""},{"location":"devops/molecule/#gitlab-ci","title":"Gitlab CI","text":"<p>Here is an example setting up a virtualenv and testing an Ansible role via Molecule. User-level pip is cached and so is the virtual environment to save time. And this is run over a runner tagged pip36 and docker, because its a minimal CentOS 7 VM installed with pip36 from IUS repository and docker.</p> <pre><code>---\nimage: docker:latest\n\nservices:\n  - docker:dind\n\nbefore_script:\n  - apk update &amp;&amp; apk add --no-cache\n    python3-dev py3-pip gcc git curl build-base\n    autoconf automake py3-cryptography linux-headers\n    musl-dev libffi-dev openssl-dev openssh\n  - docker info\n  - python3 --version\n  - python3 -m pip install ansible molecule[docker] ansible-lint\n  - ansible --version\n  - molecule --version\n\nmolecule:\n  stage: test\n  script:\n    - molecule test\n</code></pre>"},{"location":"devops/molecule/#source","title":"Source","text":"<p>Molecule 2 to 3 migration script Molecule breakable changes Molecule Changelog Molecule Docker images Molecule Driver Molecule GitHub Molecule Read the docs TestInfra Yamllint</p>"},{"location":"devops/nexus3/","title":"Nexus3","text":""},{"location":"devops/nexus3/#nexus-3-installation-et-configuration","title":"Nexus 3 : Installation et Configuration","text":""},{"location":"devops/nexus3/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Nexus 3 3.15.0+ Nexus 2 2.14.5-02"},{"location":"devops/nexus3/#todo","title":"Todo","text":"<ul> <li>Apt plugin</li> <li>Docker repositories</li> <li>Content Selector</li> <li>Cleanup Policies</li> <li>Capabilities</li> <li>Tasks</li> </ul>"},{"location":"devops/nexus3/#note-en-vrac","title":"Note en vrac","text":"<p>Am\u00e9lioration de Repository Health Check (RHC) depuis la 3.3. Fonctionnalit\u00e9 d'analyse des licences.</p> <p>Concernant les scripts de configuration : Syst\u00e8me tr\u00e8s curieux m\u00ealant rest API et groovy script. On upload un script groovy qu'on \u00e9x\u00e9cute ensuite via rest API (curl). Techno encore jeune la javadoc montre qu'on peut \"cr\u00e9er\" mais pas deleter. Il possible de faire un playbook d'installation/configuration mais cela semble tr\u00e8s couteux. Pas mal pour avoir un d\u00e9marrage rAPIde mais n\u00e9anmoins certain option manque (exemple sur les repository maven les valeurs \"Max component Age\" et \"Max medata age\" ) La rest API ne semble pas faire mieux.</p>"},{"location":"devops/nexus3/#procedure-dinstallation-docker","title":"Proc\u00e9dure d'installation docker","text":""},{"location":"devops/nexus3/#etape-1-deployer-le-nexus-3-depuis-limage-officielle","title":"Etape 1: Deployer le Nexus 3 depuis l'image officielle","text":"<pre><code>mkdir /some/dir/nexus-data &amp;&amp; chown -R 200 /some/dir/nexus-data\ndocker run -d -p 8081:8081 --name nexus -v /some/dir/nexus-data:/nexus-data sonatype/nexus3\n</code></pre>"},{"location":"devops/nexus3/#etape-2-supprimer-les-repositories-par-defaut","title":"Etape 2: Supprimer les repositories par d\u00e9faut","text":""},{"location":"devops/nexus3/#etape-3-executer-les-groovy-scripts-pour-peupler-le-nexus-3","title":"Etape 3: Executer les groovy scripts pour peupler le Nexus 3","text":"<pre><code>cd $nexusgit-dir/provisioning\n./provision.sh all\n</code></pre>"},{"location":"devops/nexus3/#etape-4-configurer-le-non-automatisable","title":"Etape 4: Configurer le non automatisable","text":"<p>Note : L'activation du RPC entraine automatiquement la cr\u00e9ation de tasks </p>"},{"location":"devops/nexus3/#etape-5-modifier-la-conf-de-nexus","title":"Etape 5: Modifier la conf de nexus","text":"<pre><code>cd $data-dir/etc\necho \"nexus.assetdownloads.enabled=true\" &gt;&gt; nexus.properties\nsudo docker service update nexus_nexus\n</code></pre>"},{"location":"devops/nexus3/#etape-6-faire-les-recommandations-de-la-doc","title":"Etape 6: Faire les recommandations de la doc","text":"<ul> <li>Changer le mot de passe administrateur et l'adresse email.</li> <li>Faire la configuration SMTP.</li> <li>Configurer les proxy HTTP et HTTPS.</li> <li>Cr\u00e9er une proc\u00e9dure de backup.</li> </ul>"},{"location":"devops/nexus3/#comparatif-artifactory-nexus-proget","title":"Comparatif Artifactory Nexus ProGet","text":"Artifactory Nexus 3 ProGet Bower X X X Docker X X X GitLFS X X Maven X X X .NET/NuGet X X X npm X X X PyPi X X Raw X X X RubyGems X X X Yum X X Apt X C Conan X C CPAN C Helm X C P2 X C R X C Chef X CocoaPods X Go X Gradle X ? Ivy X ? Opkg X PHP Composer X Puppet X SBT X Vagrant X VCS X Powershell X Chocolatey X Romp X VSIX X Upack X"},{"location":"devops/nexus3/#de-nexus-2-a-nexus-3","title":"De Nexus 2 \u00e0 Nexus 3","text":"Fonctionnalit\u00e9s 2.x OSS 2.x Pro 3.x OSS 3.x Pro Bower X X Docker X X Maven X X X X npm Limit\u00e9 Limit\u00e9 X X NuGet X X X X PyPI X X Ruby Gems X X Yum X X X X Clusters haute disponibilit\u00e9 X Plug-in S3 Blobstore X X Passage de 2x \u00e0 3x X X D\u00e9ploiement illimit\u00e9 X X X X Recherche de composants Limit\u00e9 X X X T\u00e9l\u00e9chargement artefact tiers dans IU Limit\u00e9 Limit\u00e9 X X Repository Health Check (RHC) X X X X M\u00e9tadonn\u00e9es personnalis\u00e9es X Pr\u00e9vu Sauvegarde et reprise am\u00e9lior\u00e9e X X API d'approvisionnement X X REST X X X X Plug-ins X X Nexus Exchange Nexus Exchange Int\u00e9gration open source X X X X Support pour jeton d'authentication X X X Contr\u00f4les acc\u00e8s personnalis\u00e9s X X X X Cible repository / S\u00e9lecteurs de contenu X X X X LDAP d'entreprise X X P2 X Communaut\u00e9 Communaut\u00e9 OBR X Pr\u00e9vu Pr\u00e9vu Crowd X X Proxy intelligent X Pr\u00e9vu Staging et builds X X Aide de la communaut\u00e9 X X X X Support pour entreprises X X"},{"location":"devops/nexus3/#tarifications","title":"Tarifications","text":"Artifactory Nexus 3 Pro 2950 $ par an 10$ par utilisateurs par mois Pro X 14400 $ par an Entreprise 29500 $ par an Entreprise + $$$$$ <p>Note artifactory est \"Unlimited number of users\"</p>"},{"location":"devops/nexus3/#sonatype-nexus-3-documentation","title":"Sonatype Nexus 3 documentation","text":""},{"location":"devops/nexus3/#download","title":"Download","text":"<p>Derni\u00e8re version de Nexus 3 : 3.15.0-01 (2019-01-15) Disponible sous trois formes :</p> <ul> <li>Archives (Unix/Windows/OSX).</li> <li>Docker Image.</li> <li>Cloud Templates.</li> </ul>"},{"location":"devops/nexus3/#system-requirements","title":"System Requirements","text":"<ul> <li>Avoir java 8 et un utilisateur d\u00e9di\u00e9.</li> <li>Augmenter la limit du nombre de fichier ouvrable par l'utilisateur nexus \"nexus - nofile 65536\". L'image docker est configur\u00e9 comme il faut mais si besoin on peut la d\u00e9marrer avec le flags : \"--ulimit nofile=65536:65536\"</li> <li>Les param\u00e8tres de la JVM sont d\u00e9pendants de la RAM disponible sans toutefois d\u00e9passer les 4GB Max.</li> <li>Laisser les params de la JVM par d\u00e9faut \u00e0 min 1200MB et max &lt;4GB.</li> </ul> Physical Memory Example Maximum Memory Configuration 4GB -Xms1200M-Xmx1200M-XX:MaxDirectMemorySize=2G 8GB -Xms2703M-Xmx2703M-XX:MaxDirectMemorySize=2703M 12GB -Xms4G-Xmx4G-XX:MaxDirectMemorySize=4014M 16GB -Xms4G-Xmx4G-XX:MaxDirectMemorySize=6717M 32GB -Xms4G-Xmx4G-XX:MaxDirectMemorySize=17530M 64GB -Xms4G-Xmx4G-XX:MaxDirectMemorySize=39158M <p>Pas de NFS pour les blobstore. Si il n'y a pas le choix il faut du NFS v4 car NFS v3 est connu pour des probl\u00e8mes de compatibilit\u00e9s.</p>"},{"location":"devops/nexus3/#upgrade-compatibility-repository-manager-2-to-3","title":"Upgrade Compatibility - Repository Manager 2 to 3","text":"<p>On ne peut qu\u2019upgrader que d'une version 2.14.1 + vers une 3.1 +. On ne peut qu\u2019upgrader que d'une OSS vers une OSS ou d'une PRO vers une PRO. On ne peut upgrader que sur une version vanilla de la 3.y (fresh install). Version recommand\u00e9e pour l'upgrade 2.14.5+</p> <p>Matrice de migration :</p> Version 2 Version 3 Minimum Lifecycle Firewall compatible IQ Server 2.14.8 3.12.1 1.46.0 2.14.6 3.8.0 1.42.0 2.14.5 3.7.1 1.33.0 2.14.4 3.4.0 Not Supported 2.14.3 3.2.1 Not Supported 2.14.2 3.2 Not Supported 2.14.1 3.1 Not Supported"},{"location":"devops/nexus3/#repository-manager-pro-features","title":"Repository Manager Pro Features","text":"<p>Atlassian Crowd Support Il s'agit d'un syst\u00e8me de sso vendu par Atlassian. La version pro supporte nativement cette fonctionnalit\u00e9.</p> <p>Staging and Build Promotion Permet d'automatiser directement au sein de Nexus 3 la promotion de component entre les phases de builds pour passer d'un repository de 'staging' \u00e0 un repository de 'production'.</p> <p>Tagging Permet de \"taguer\" un ensemble de component pour les associer les uns autres. Fonctionne de pair avec la fonctionnalit\u00e9 de staging de Nexus 3.</p> <p>User Token Support Permet d'utiliser des token en lieu et place des mots de passes.</p> <p>High Availability Permet de clusteriser Nexus 3.</p> <p>Repository Health Check Fonctionnalit\u00e9 de check des components pour notamment l'int\u00e9grer dans un pipeline DevSecOps. Permet de d\u00e9tecter automatiquement les risques de s\u00e9curit\u00e9 des projets opensource. Fonctionne \"\u00e0 la mani\u00e8re\" d'un xray.</p> <p>Customer Success Coaching sp\u00e9cifique d\u00e9di\u00e9 au souscripteur.</p> <p>Enterprise Support Support.</p>"},{"location":"devops/nexus3/#repository-manager-concepts","title":"Repository Manager Concepts","text":"<p>Components Le terme employ\u00e9 par Sonatype pour d\u00e9signer un \"artifact, package, bundle, archive\".</p> <p>Assets El\u00e9ments constituant un composant. Par exemples un jar et son pom.xml forme un artifiact maven.</p> <p>Components in Repositories Le repository est l'endroit qui rend accessible les composants.</p>"},{"location":"devops/nexus3/#installation","title":"Installation","text":"<p>Nexus 3 est une webapps java totalement autonome. Il existe deux m\u00e9thodes \"d'installation\" de nexus :</p> <ul> <li>Via l'archive de distribution classique.   Il suffit de d\u00e9zipper le contenu de l'archive (/opt/nexus) et d'ex\u00e9cuter bin/nexus start.   L'application est d\u00e9marr\u00e9e lorsque le message de log de la console \"Started Sonatype Nexus\".</li> <li>Via l'image docker.   Ex\u00e9cuter la commande suivante :</li> </ul> <pre><code>mkdir /some/dir/nexus-data &amp;&amp; chown -R 200 /some/dir/nexus-data\ndocker run -d -p 8081:8081 --name nexus -v /some/dir/nexus-data:/nexus-data sonatype/nexus3\n</code></pre>"},{"location":"devops/nexus3/#directories","title":"Directories","text":"<p>Apr\u00e8s l'installation deux r\u00e9pertoires apparaissent :</p> <ul> <li>nexus-3.XX.X-XX   Contient l'application Nexus Repository Manager.</li> <li>sonatype-work   Contient l'ensemble des repositories, components et autres.   Le dossier sonatype-work/nexus3/blobs est le plus important, c'est lui qui contient les repositories.</li> </ul>"},{"location":"devops/nexus3/#configuring-the-runtime-environment","title":"Configuring the Runtime Environment","text":"<p>La configuration du produit se situe dans trois r\u00e9pertoires distinct :</p> <ul> <li>$data-dir/etc   Contient le fichier nexus.properties qui poss\u00e8de quelques variables de configuration notamment l'ip et le port.</li> <li>$install-dir/bin   Contient le fichier nexus.vmoptions qui poss\u00e8de quelques variables de configuration notamment la localisation du data directories.</li> <li>$install-dir/etc   Contient les configuration des middlexare utilis\u00e9 par Nexus 3.   Pour savoir \"o\u00f9\" modifier \"quoi\" autant se rendre sur la documentation officielle.</li> </ul>"},{"location":"devops/nexus3/#post-install-checklist","title":"Post Install Checklist","text":"<ul> <li>Etape 1: Changer le mot de passe administrateur et l'adresse email.</li> <li>Etape 2: Faire la configuration SMTP.</li> <li>Etape 3: Configurer les proxy HTTP et HTTPS.</li> <li>Etape 4: Cr\u00e9er une proc\u00e9dure de backup.</li> </ul>"},{"location":"devops/nexus3/#upgrading","title":"Upgrading","text":""},{"location":"devops/nexus3/#why-upgrade-to-nexus-repository-manager-3","title":"Why Upgrade to Nexus Repository Manager 3 ?","text":"<ul> <li>Nouveau format de repository.</li> <li>Interface utilisateur am\u00e9lior\u00e9e</li> <li>Recherche de composants performante</li> <li>Navigation dans le r\u00e9f\u00e9rentiel universel</li> <li>M\u00e9tadonn\u00e9es am\u00e9lior\u00e9es</li> </ul>"},{"location":"devops/nexus3/#upgrade-process-and-expectations","title":"Upgrade Process and Expectations","text":"<p>What Is Upgraded</p> <ul> <li>Stockage des compoent des fichiers aux blobs :   On passe du syst\u00e8me de fichier \u00e0 plat de Nexus 2 au blob de Nexus 3. C'est le plus gros changement.</li> <li>Les metadata associ\u00e9es aux components passe de multiple fichier \u00e0 la base OrientDB.</li> <li>Les URLs de d\u00e9ploiement et d'acc\u00e8s sont maintenant diff\u00e9rentes.</li> </ul> <p>What Is Not Upgraded Parmis les plus impactant :</p> <ul> <li>virtual repositories</li> <li>Java VM settings, including custom system properties or variables</li> <li>operating system nexus service scripts</li> <li>operating system optimization, such as increasing allowable open file handles</li> <li>environment variables affecting values used to control the repository manager</li> <li>third-party or custom-developed plugins</li> </ul> <p>Repository Format Support</p> <ul> <li>npm</li> <li>NuGet</li> <li>Site/Raw</li> <li>Maven2</li> <li>RubyGems</li> </ul>"},{"location":"devops/nexus3/#data-transfer-methods","title":"Data Transfer Methods","text":"<ul> <li>HTTP Downloading   N\u00e9cessaire si le Nexus 2 et 3 sont sur deux machines diff\u00e9rentes.</li> <li>File System Copying   Possible si et seulement si le Nexus 2 et 3 sont configur\u00e9s pour acc\u00e9der au m\u00eame espace de stockage.   C'est la m\u00e9thode alternative</li> <li>File System Hard Linking   Possible si et seulement si le Nexus 2 et 3 sont configur\u00e9s pour acc\u00e9der au m\u00eame espace de stockage.   C'est la m\u00e9thode la plus rAPIde.</li> </ul>"},{"location":"devops/nexus3/#upgrade-details-for-specific-elements","title":"Upgrade Details for Specific Elements","text":"<ul> <li>Repository IDs   Attention les repository ID n'existe plus dans Nexus 3.   Ils sont n\u00e9anmoins \"case sensitive\" pendant le process de migration.</li> <li>Repository Groups   S'assurer que dans un groupe l'ensemble des repositories soient \u00e9ligibles.   Au risque de ne pas upgrader le groupe entier...</li> <li>HTTP(S) Proxy Configuration   Ils sont \u00e0 configurer manuellement dans chaque environment.</li> </ul>"},{"location":"devops/nexus3/#security-compatibility","title":"Security Compatibility","text":"<ul> <li>Version 2 Roles   Les roles sont upgrad\u00e9 du Nexus 2 vers le Nexus 3. Ils sont pr\u00e9fix\u00e9s de nx2-.</li> <li>Version 2 Repository Targets and Target Privileges   Les Repository targets de Nexus 2 deviennent des Content Selector dans Nexus 3.</li> </ul>"},{"location":"devops/nexus3/#upgrade-procedure","title":"Upgrade Procedure","text":"<p>Lire la documentation officielle.</p>"},{"location":"devops/nexus3/#etape-0-indisponibilite-du-nexus","title":"Etape 0 : Indisponibilit\u00e9 du nexus","text":"<p>Pendant la phase de migration le Nexus 2 reste disponible. Une \u00e9tape de \"synchronisation\" permet de r\u00e9cup\u00e9rer les \u00e9ventuelles \"push\" sur le nexus pendant la migration jusqu'\u00e0 une certaine \u00e9tape. A la fin le Nexus 2 doit \u00eatre d\u00e9sactiv\u00e9 au profit du Nexus 3, il est pr\u00e9f\u00e9rable d\u00e8s lors d\u2019effectuer l\u2019op\u00e9ration en HNO et s\u2019assurer que Jenkins n\u2019effectue aucun deploy pendant la migration.</p> <p>Designing Your Upgrade Plan</p> <ul> <li> Identification of a maintenance window for version 2, allowing the upgrade to proceed without interruption.</li> <li> Selection of an installation scenario that best supports your upgrade plan.</li> <li> Selection of an upgrade method.</li> <li> Getting access to a system storage , as well as location for content to be transferred to.</li> <li> Identification of configurations that may result in failure, and prevent upgrade of certain components.</li> <li> Review of security settings , and associated differences between version 2 and version 3.</li> <li> Considerations for optimization.</li> </ul>"},{"location":"devops/nexus3/#etape-1-sassurer-que-le-nexus-2-est-en-21411-01","title":"Etape 1 : S'assurer que le nexus 2 est en 2.14.11-01","text":"<p>Le nexus 2 doit \u00eatre dans la plus haute version disponible au moment de l'update. Ceci permet de s\u2019affranchir d\u2019\u00e9ventuelle probl\u00e8me d\u2019incompatibilit\u00e9 de migration entre les versions et corrige d\u2019\u00e9ventuelle bug concernant la proc\u00e9dure d\u2019update automatique.</p>"},{"location":"devops/nexus3/#etape-2-optimisation-pre-migration","title":"Etape 2 : Optimisation pre-migration","text":"<ul> <li> Nexus 2 : D\u00e9sactiver les \"System feeds\".</li> <li> Nexus 2 : Supprimer les Snapshots.</li> <li> Nexus 2 : D\u00e9sactiver les \"Scheduled task for releases\".</li> </ul>"},{"location":"devops/nexus3/#etape-3-activer-les-upgrade-capability-de-nexus-2-et-nexus-3","title":"Etape 3 : Activer les \"Upgrade Capability\" de Nexus 2 et Nexus 3","text":"<p>Dans Nexus 2 / 3 en tant qu'admin </p> <p></p> <p></p>"},{"location":"devops/nexus3/#etape-4-creer-les-blobstore-cible","title":"Etape 4 : Cr\u00e9er les blobstore cible","text":"<p>Si ce n'est pas d\u00e9j\u00e0 fait (\u00e0 l'instanciation du Nexus 3). Il faut cr\u00e9er les blobstores via le sript de provionning.</p> <p></p>"},{"location":"devops/nexus3/#etape-5-lancer-la-procedure-de-migration","title":"Etape 5 : Lancer la proc\u00e9dure de migration","text":"<p>Dans Nexus 3 en tant qu'admin. Dans Administation --&gt; System --&gt; Upgrade.</p>"},{"location":"devops/nexus3/#etape-x","title":"Etape X","text":"<p>La migration est termin\u00e9e, passons aux post migration tasks :</p> <ul> <li> Nexus 3 : Checker les repositories group.</li> <li> Nexus 3 : Checker les RHC.</li> <li> Nexus 3 : Checker les settings (Imap, Proxy).</li> <li> Nexus 3 : Nettoyer les roles et la security.</li> <li> Nexus 3 : V\u00e9rifier les roles des utilisateurs admin et anonymous.</li> <li> Nexus 3 : Configurer le mot de passe admin.</li> <li> Nexus 3 : Cr\u00e9er les taches de clean des blobstores maven et de backup.</li> <li> Nexus 3 : Retirer la capabilities upgrade.</li> <li> Nexus 2 : Stopper le service.</li> <li> Client : Modifier toutes les URLs des .m2/settings.xml et .npmrc</li> </ul>"},{"location":"devops/nexus3/#configurationnavigation","title":"Configuration/Navigation","text":""},{"location":"devops/nexus3/#user-interface","title":"User Interface","text":"<p>En se connectant on arrive sur l'interface de base.  En tant qu'utilisateur anonyme on peut acc\u00e9der \u00e0 l'interface de recherche rAPIde et avanc\u00e9 et naviguer dans les repositories. En tant qu'admin on peut manager via le bouton engrenage. En tant qu'utilisateur identifi\u00e9 poss\u00e9dant les droits suffisants, on peut uploader depuis l'interface un component.</p>"},{"location":"devops/nexus3/#administration-menu","title":"Administration Menu","text":"<p>L'interface d'administration est d\u00e9coup\u00e9e en 5 sous menu. </p> <ul> <li>Repository, g\u00e8re les repositories, blob stores, cleanup policies et Content Selector.</li> <li>IQ Server, g\u00e8re la connexion au IQ Server.</li> <li>Security, g\u00e8re tous ce qui est relatif \u00e0 la s\u00e9curit\u00e9 (Authentication, Authorization, Privileges... ).</li> <li>Support, fonctionnalit\u00e9 de monitoring du serveur.</li> <li>System, la configuration g\u00e9n\u00e9rale de l'outil.</li> </ul> <p>Dans l'interface Repository la notion de Blob stores et Content selectors est importante.</p>"},{"location":"devops/nexus3/#source","title":"Source","text":"<p>Sonatype Global Nexus 3 Dowloads Docker Sonatype/Nexus 3</p> <p>Nexus 3 OSS Global Nexus 3 Communaut\u00e9 Nexus Comparatif</p> <p>Nexus 3 Release Notes 2019 Nexus 3 Run as a Service Nexus 3 Run Behind a Reverse Proxy Nexus 3 Upgrade Procedures</p> <p>Nexus 3 Quick Start Maven/Npm Nexus 3 Storage Guide Nexus 3 Scripting Examples</p>"},{"location":"devops/portainer/","title":"Portainer","text":""},{"location":"devops/portainer/#portainer-installation-et-configuration","title":"Portainer : Installation et Configuration","text":""},{"location":"devops/portainer/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Portainer 1.20.2 Docker 18.09.5"},{"location":"devops/portainer/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/portainer/#note-en-vrac","title":"Note en vrac","text":"<p>N/A</p>"},{"location":"devops/portainer/#avant-propos","title":"Avant propos","text":"<p>Portainer is a simple management solution for Docker. It consists of a web UI that allows you to easily manage your Docker containers, images, networks and volumes.</p>"},{"location":"devops/portainer/#procedure-dinstallation-docker","title":"Proc\u00e9dure d'installation docker","text":"<p>Quick start </p> <pre><code>docker volume create portainer_data\ndocker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer\n</code></pre> <p>Advanced deployment </p> <p>Deploy Portainer via docker-compose :  </p> <pre><code>version: '2'\n\nservices:\n  portainer:\n    image: portainer/portainer\n    command: -H unix:///var/run/docker.sock\n    restart: always\n    ports:\n      - 9000:9000\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - portainer_data:/data\n\nvolumes:\n  portainer_data:\n</code></pre> <p>Declaring the Docker environment to manage upon deployment :  </p> <pre><code>docker run -d -p 9000:9000 --name portainer --restart always -v portainer_data:/data portainer/portainer -H tcp://&lt;REMOTE_HOST&gt;:&lt;REMOTE_PORT&gt;\ndocker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer -H unix:///var/run/docker.sock\n</code></pre> <p>Secure Portainer using SSL :  </p> <pre><code>docker run -d -p 443:9000 --name portainer --restart always -v ~/local-certs:/certs -v portainer_data:/data portainer/portainer --ssl --sslcert /certs/portainer.crt --sslkey /certs/portainer.key\n</code></pre>"},{"location":"devops/portainer/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>Configuration Admin password</p> <pre><code>htpasswd -nb -B admin &lt;password&gt; | cut -d \":\" -f 2\ndocker run --rm httpd:2.4-alpine htpasswd -nbB admin &lt;password&gt; | cut -d \":\" -f 2\n\ndocker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer --admin-password='$2y$05$qFHAlNAH0A.6oCDe1/4W.ueCWC/iTfBMXIHBI97QYfMWlMCJ7N.a6'\n</code></pre> <p>Available flags</p> <ul> <li>--admin-password: Admin password in the form admin:&lt;hashed_password&gt;</li> <li>--admin-password-file: Path to the file containing the password for the admin user</li> <li>--bind, -p: Address and port to serve Portainer (default: :9000)</li> <li>--data, -d: Directory where Portainer data will be stored (default: /data on Linux, C:\\data on Windows)</li> <li>--external-endpoints: Enable external endpoint management by specifying the path to a JSON endpoint source in a file</li> <li>--hide-label, -l: Hide containers with a specific label in the UI</li> <li>--host, -H: Docker daemon endpoint</li> <li>--logo: URL to a picture to be displayed as a logo in the UI, use Portainer logo if not specified</li> <li>--no-analytics: Disable analytics (default: false)</li> <li>--no-auth: Disable internal authentication mechanism (default: false)</li> <li>--no-snapshot: Disable periodic endpoint snapshot (default: false)</li> <li>--snapshot-interval: Time interval between two endpoint snapshot jobs expressed as a string, e.g. 30s, 5m, 1h\u2026 as supported by the time.ParseDuration method (default: 5m)</li> <li>--ssl: Secure Portainer instance using SSL (default: false)</li> <li>--sslcert: Path to the SSL certificate used to secure the Portainer instance (default: /certs/portainer.crt, C:\\certs\\portainer.crt on Windows)</li> <li>--sslkey: Path to the SSL key used to secure the Portainer instance (default: /certs/portainer.key, C:\\certs\\portainer.key on Windows)</li> <li>--sync-interval: Time interval between two endpoint synchronization requests expressed as a string, e.g. 30s, 5m, 1h\u2026 as supported by the time.ParseDuration method (default:** 60s)</li> <li>--templates, -t: URL to templates (apps) definitions</li> <li>--template-file: Path on disk to templates (apps) definitions (default: /templates.json)</li> <li>--tlscacert: Path to the CA (default: /certs/ca.pem on Linux, C:\\certs\\ca.pem on Windows)</li> <li>--tlscert: Path to the TLS certificate file (default: /certs/cert.pem, C:\\certs\\cert.pem on Windows)</li> <li>--tlskey: Path to the TLS key (default: /certs/key.pem, C:\\certs\\key.pem on Windows)</li> <li>--tlsverify: TLS support (default: false)</li> </ul>"},{"location":"devops/portainer/#source","title":"Source","text":"<p>Portainer Portainer Documentation Portainer Docker Portainer Swagger </p>"},{"location":"devops/traefik/","title":"Traefik : Installation et Configuration","text":""},{"location":"devops/traefik/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Gitlab 1.7.X Docker 18.09.6"},{"location":"devops/traefik/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"devops/traefik/#note-en-vrac","title":"Note en vrac","text":""},{"location":"devops/traefik/#avant-propos","title":"Avant propos","text":"<p>Traefik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy. Traefik integrates with your existing infrastructure components (Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS, ...) and configures itself automatically and dynamically. Pointing Traefik at your orchestrator should be the only configuration step you need.  .</p>"},{"location":"devops/traefik/#getting-started","title":"Getting Started","text":"<p>Ex\u00e9cuter simplement le docker-compose suivant :  </p> <pre><code>version: '3'\n\nservices:\n  reverse-proxy:\n    image: traefik # The official Traefik docker image\n    command: --api --docker # Enables the web UI and tells Traefik to listen to docker\n    ports:\n      - \"80:80\"     # The HTTP port\n      - \"8080:8080\" # The Web UI (enabled by --api)\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events\n  whoami:\n    image: containous/whoami # A container that exposes an API to show its IP address\n    labels:\n      - \"traefik.frontend.rule=Host:whoami.docker.localhost\"\n</code></pre> <p>Testons avec curl :</p> <pre><code>curl -H Host:whoami.docker.localhost http://127.0.0.1\n</code></pre> <p>En scalant :</p> <pre><code>docker-compose scale whoami=2\ncurl -H Host:whoami.docker.localhost http://127.0.0.1\n</code></pre>"},{"location":"devops/traefik/#features","title":"Features","text":"<ul> <li>Continuously updates its configuration (No restarts!)</li> <li>Supports multiple load balancing algorithms</li> <li>Provides HTTPS to your microservices by leveraging Let's Encrypt (wildcard certificates support)</li> <li>Circuit breakers, retry</li> <li>High Availability with cluster mode (beta)</li> <li>See the magic through its clean web UI</li> <li>Websocket, HTTP/2, GRPC ready</li> <li>Provides metrics (Rest, Prometheus, Datadog, Statsd, InfluxDB)</li> <li>Keeps access logs (JSON, CLF)</li> <li>Fast</li> <li>Exposes a Rest API</li> <li>Packaged as a single binary file (made with \u2764\ufe0f with go) and available as a tiny official docker image</li> </ul>"},{"location":"devops/traefik/#supported-providers","title":"Supported Providers","text":"<ul> <li>Docker / Swarm mode</li> <li>Kubernetes</li> <li>Mesos / Marathon</li> <li>Rancher (API, Metadata)</li> <li>Azure Service Fabric</li> <li>Consul Catalog</li> <li>Consul / Etcd / Zookeeper / BoltDB</li> <li>Eureka</li> <li>Amazon ECS</li> <li>Amazon DynamoDB</li> <li>File</li> <li>Rest</li> </ul>"},{"location":"devops/traefik/#basics","title":"Basics","text":"<p>Quick overview </p> <p>Zoom into traefik </p> <ul> <li>Incoming requests end on entrypoints, as the name suggests, they are the network entry points into Traefik (listening port, SSL, traffic redirection...).</li> <li>Traffic is then forwarded to a matching frontend. A frontend defines routes from entrypoints to backends. Routes are created using requests fields (Host, Path, Headers...) and can match or not a request.</li> <li>The frontend will then send the request to a backend. A backend can be composed by one or more servers, and by a load-balancing strategy.</li> <li>Finally, the server will forward the request to the corresponding microservice in the private network.</li> </ul>"},{"location":"devops/traefik/#entrypoints","title":"Entrypoints","text":"<p>Entrypoints are the network entry points into Traefik. They can be defined using:</p> <ul> <li>a port (80, 443...)</li> <li>SSL (Certificates, Keys, authentication with a client certificate signed by a trusted CA...)</li> <li>redirection to another entrypoint (redirect HTTP to HTTPS)</li> </ul>"},{"location":"devops/traefik/#frontends","title":"Frontends","text":"<p>A frontend consists of a set of rules that determine how incoming requests are forwarded from an entrypoint to a backend.</p>"},{"location":"devops/traefik/#backends","title":"Backends","text":"<p>A backend is responsible to load-balance the traffic coming from one or more frontends to a set of http servers.</p>"},{"location":"devops/traefik/#configuration","title":"Configuration","text":"<p>Traefik's configuration has two parts:</p> <ul> <li>The static Traefik configuration which is loaded only at the beginning.</li> <li>The dynamic Traefik configuration which can be hot-reloaded (no need to restart the process).</li> </ul>"},{"location":"devops/traefik/#config","title":"Config","text":""},{"location":"devops/traefik/#docker-provider","title":"Docker Provider","text":"<pre><code>################################################################\n# Docker Provider\n################################################################\n\n# Enable Docker Provider.\n[docker]\n\n# Docker server endpoint. Can be a tcp or a unix socket endpoint.\n#\n# Required\n#\nendpoint = \"unix:///var/run/docker.sock\"\n\n# Default base domain used for the frontend rules.\n# Can be overridden by setting the \"traefik.domain\" label on a container.\n#\n# Optional\n#\ndomain = \"docker.localhost\"\n\n# Enable watch docker changes.\n#\n# Optional\n#\nwatch = true\n\n# Expose containers by default in Traefik.\n# If set to false, containers that don't have `traefik.enable=true` will be ignored.\n#\n# Optional\n# Default: true\n#\nexposedByDefault = true\n\n# Use the IP address from the binded port instead of the inner network one.\n#\n# In case no IP address is attached to the binded port (or in case\n# there is no bind), the inner network one will be used as a fallback.     \n#\n# Optional\n# Default: false\n#\nusebindportip = true\n\n# Use Swarm Mode services as data provider.\n#\n# Optional\n# Default: false\n#\nswarmMode = false\n\n# Polling interval (in seconds) for Swarm Mode.\n#\n# Optional\n# Default: 15\n#\nswarmModeRefreshSeconds = 15\n</code></pre>"},{"location":"devops/traefik/#on-container","title":"On container","text":"Label Description traefik.docker.network Overrides the default docker network to use for connections to the container. traefik.domain Sets the default base domain for the frontend rules. For more information, check the Container Labels section's of the user guide \"Let's Encrypt &amp;  Docker\". traefik.enable=false Disables this container in Traefik. traefik.port=80 Registers this port. Useful when the container exposes multiples ports. traefik.tags=foo,bar,myTag Adds Traefik tags to the Docker container/service to be used in constraints. traefik.protocol=https Overrides the default http protocol. traefik.weight=10 Assigns this weight to the container. traefik.backend=foo Overrides the container name by foo in the generated name of the backend. traefik.frontend.rule=EXPR Overrides the default frontend rule. Default: Host:{containerName}.{domain} or Host:{service}.{project_name}.{domain} if you are using docker-compose."},{"location":"devops/traefik/#on-containers-with-multiple-ports-segment-labels","title":"On containers with Multiple Ports (segment labels)","text":"Label Description traefik.&lt;segment_name&gt;.backend=BACKEND Same as traefik.backend traefik.&lt;segment_name&gt;.domain=DOMAIN Same as traefik.domain traefik.&lt;segment_name&gt;.port=PORT Same as traefik.port traefik.&lt;segment_name&gt;.protocol=http Same as traefik.protocol traefik.&lt;segment_name&gt;.weight=10 Same as traefik.weight traefik.&lt;segment_name&gt;.frontend.rule=EXP Same as traefik.frontend.rule"},{"location":"devops/traefik/#source","title":"Source","text":"<p>Traefik Traefik Documentation Traefik Docker Traefik Security Challenge with the Docker Socket </p>"},{"location":"operating-system/debian/","title":"Debian 9 : Installation et Configuration","text":""},{"location":"operating-system/debian/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Debian 9.5.0"},{"location":"operating-system/debian/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>Classique de l'install d'une vm.</p>"},{"location":"operating-system/debian/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":""},{"location":"operating-system/debian/#install-virtualbox-guest-additions","title":"Install VirtualBox Guest Additions","text":"<pre><code>apt install build-essential module-assistant dkms\nm-a prepare\nreboot\n# Insert Guest Additions CD image.\nmount /dev/cdrom /mnt\ncd /mnt\n./VBoxLinuxAdditions.run\nreboot\n</code></pre>"},{"location":"operating-system/debian/#packages","title":"Packages","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt update &amp;&amp; sudo apt install build-essential cowsay curl dkms dnsutils fonts-powerline git gitk htop libfortune-perl man mlocate module-assistant net-tools nmap powerline sudo tmux tree unzip vim zsh\n</code></pre>"},{"location":"operating-system/debian/#packages-in-debian-backports","title":"Packages in Debian backports","text":"<pre><code>sudo su -c 'echo \"deb http://ftp.debian.org/debian stretch-backports main\" &gt; /etc/apt/sources.list.d/backports.list'\nsudo apt -t stretch-backports install git ansible tmux\n</code></pre>"},{"location":"operating-system/debian/#tools","title":"Tools","text":"<pre><code>cd\ncurl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\nsudo apt-get install -y nodejs\nsudo npm install -g hads\nsh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\ngit clone https://github.com/jimeh/tmuxifier.git ~/.tmuxifier\n</code></pre>"},{"location":"operating-system/debian/#configurer-le-reseaux","title":"Configurer le r\u00e9seaux","text":"<pre><code>vim /etc/network/interfaces\n\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nallow-hotplug enp0s3\niface enp0s3 inet dhcp\n\n# The secondary network interface\nallow-hotplug enp0s8\niface enp0s8 inet static\n        address 192.168.100.12\n        netmask 255.255.255.0\n        getway 192.168.100.2\n</code></pre>"},{"location":"operating-system/mint/","title":"Linux Mint 19.1 : Installation et Configuration","text":""},{"location":"operating-system/mint/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Linux Mint Tessa 19.1"},{"location":"operating-system/mint/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":""},{"location":"operating-system/mint/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":""},{"location":"operating-system/mint/#one-line-installer","title":"One Line installer","text":"<pre><code>sudo apt install apt-transport-https build-essential ca-certificates cowsay curl dkms dnsutils fonts-powerline git gnupg2 htop libfortune-perl man mlocate module-assistant net-tools nmap powerline python-dev software-properties-common sudo tmux tree unzip vim zsh pip\n</code></pre>"},{"location":"operating-system/mint/#7zip","title":"7zip","text":"<pre><code>sudo apt install p7zip\n</code></pre>"},{"location":"operating-system/mint/#atom","title":"Atom","text":"<pre><code>wget -qO - https://packagecloud.io/AtomEditor/atom/gpgkey | sudo apt-key add -\nsudo sh -c 'echo \"deb [arch=amd64] https://packagecloud.io/AtomEditor/atom/any/ any main\" &gt; /etc/apt/sources.list.d/atom.list'\nsudo apt-get install atom\n</code></pre>"},{"location":"operating-system/mint/#cryptomator","title":"Cryptomator","text":"<pre><code>sudo add-apt-repository ppa:sebastian-stenzel/cryptomator\nsudo apt-get update\nsudo apt-get install cryptomator\n</code></pre>"},{"location":"operating-system/mint/#git","title":"Git","text":"<pre><code>add-apt-repository ppa:git-core/ppa &amp;&amp; apt update &amp;&amp; apt install git\n</code></pre>"},{"location":"operating-system/mint/#virtualbox","title":"VirtualBox","text":"<pre><code>wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -\nwget -q https://www.virtualbox.org/download/oracle_vbox.asc -O- | sudo apt-key add -\nsudo sh -c 'echo \"deb [arch=amd64] https://download.virtualbox.org/virtualbox/debian bionic contrib\" &gt; /etc/apt/sources.list.d/virtualbox.list'\nsudo apt update &amp;&amp; sudo apt-get install virtualbox-6.0\n</code></pre>"},{"location":"operating-system/mint/#vagrant","title":"Vagrant","text":"<pre><code>cd ~/T\u00e9l\u00e9chargements\nwget https://releases.hashicorp.com/vagrant/2.2.4/vagrant_2.2.4_x86_64.deb\nsudo apt install ./vagrant_2.2.4_x86_64.deb\n</code></pre>"},{"location":"operating-system/mint/#ansible","title":"Ansible","text":"<pre><code>pip install --usser molecule\n</code></pre>"},{"location":"operating-system/mint/#signal","title":"Signal","text":"<pre><code>curl -s https://updates.signal.org/desktop/apt/keys.asc | sudo apt-key add -\necho \"deb [arch=amd64] https://updates.signal.org/desktop/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/signal-xenial.list\nsudo apt update &amp;&amp; sudo apt install signal-desktop\n</code></pre>"},{"location":"operating-system/mint/#forticlient","title":"Forticlient","text":"<pre><code>wget -O - https://repo.fortinet.com/repo/ubuntu/DEB-GPG-KEY | sudo apt-key add -\nsudo sh -c 'echo \"deb [arch=amd64] https://repo.fortinet.com/repo/ubuntu/ /bionic multiverse\" &gt; /etc/apt/sources.list.d/forticlient.list'\n</code></pre>"},{"location":"operating-system/mint/#steam","title":"Steam","text":"<pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys B05498B7\nsudo sh -c 'echo \"deb http://repo.steampowered.com/steam/ precise steam\" &gt; /etc/apt/sources.list.d/steam.list'\n</code></pre>"},{"location":"operating-system/mint/#keepassxc","title":"Keepassxc","text":"<pre><code>sudo add-apt-repository ppa:phoerious/keepassxc\nsudo apt-get update\nsudo apt install keepassxc\n</code></pre>"},{"location":"operating-system/mint/#knowledge-base","title":"Knowledge Base","text":""},{"location":"operating-system/mint/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"operating-system/mint/#source","title":"Source","text":"<p>Battery drains Cinnamon Settings Cinnamon Spices Linux Mint UFW </p>"},{"location":"operating-system/wsl/","title":"Wsl : Installation et Configuration","text":""},{"location":"operating-system/wsl/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Windows 10 Professionnel 1803 Wsl - Debian GNU/Linux X.X.X"},{"location":"operating-system/wsl/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>Activer le WSL de Windows 10. Installer Debian GNU/Linux depuis le Microsoft Store.  </p>"},{"location":"operating-system/wsl/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>Installer les awesomes font sur Windows 10. Installer les dotfiles Pandemonium1986. Utiliser l'outil colortool de windows pour modifier la couleur du WSL.  </p> <pre><code>start cmd /k \"D:\\Downloads\\Colortool\\colortool.exe -b Argonaut.itermcolors &amp;&amp; echo Argonaut.itermcolors\"\n</code></pre> <p>Configurer la console WSL comme ceci :  </p> <p>Options : </p> <p>Police : </p> <p>Configuration : </p> <p>Couleurs : </p>"},{"location":"operating-system/wsl/#activer-le-drvfs","title":"Activer le DrvFs","text":"<p>Depuis le Wsl (Cette action est \u00e0 faire pour chaque sous system linux install\u00e9 sur la machine Windows 10).  </p> <pre><code>sudo vim /etc/wsl.conf\n</code></pre> <pre><code>[automount]\nenabled = true\nroot = /mnt/\noptions = \"metadata,uid=1000,gid=1000,umask=22,fmask=11\"\nmountFsTab = false\n</code></pre>"},{"location":"operating-system/wsl/#source","title":"Source","text":"<p>Nxi - Wsl How To Introducing the Windows Console Colortool iTerm Color Schemes Manage and configure Windows Subsystem for Linux Chmod/Chown WSL Improvements</p>"},{"location":"operating-system/wsl2/","title":"Wsl2","text":""},{"location":"operating-system/wsl2/#wsl2-installation-et-configuration","title":"Wsl2 : Installation et Configuration","text":""},{"location":"operating-system/wsl2/#tool-versions","title":"Tool Versions","text":"Os / Tool Version Windows 10 Professionnel 20H2 Wsl - Ubuntu GNU/Linux 20.4"},{"location":"operating-system/wsl2/#installation-procedure","title":"Installation procedure","text":""},{"location":"operating-system/wsl2/#vanilla-the-hardest-way","title":"Vanilla (the hardest way)","text":"<p>The vanilla installation is made to free you from the use of a third party tool. However you must scrupulously respect the execution of the commands and the different steps (being an admin user, reboot ...).</p> <p>Step 1</p> <pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart;\ndism.exe /online /enable-feature /featurename:VirtualM5achinePlatform /all /norestart;\n</code></pre> <p>Step 2</p> <p>Reboot</p> <p>Step 3</p> <pre><code>Invoke-WebRequest -Uri \"&lt;https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi&gt;\" -Outfile \"$Env:tmp\\\\wsl_update_x64.msi\";\nmsiexec /i $Env:tmp\\\\wsl_update_x64.msi /qn;\nsleep 15;\nwsl --set-default-version 2;\nsleep 15;\n$ProgressPreference = 'SilentlyContinue';\nInvoke-WebRequest -Uri &lt;https://aka.ms/wslubuntu2004&gt; -OutFile $Env:tmp\\\\wsl-ubuntu-2004.appx -UseBasicParsing;\nAdd-AppxPackage $Env:tmp\\\\wsl-ubuntu-2004.appx;\n</code></pre>"},{"location":"operating-system/wsl2/#chocolatey-the-lazy-way","title":"Chocolatey (the lazy way)","text":"<p>Step 1</p> <pre><code>$amIAdmin = (([System.Security.Principal.WindowsIdentity]::GetCurrent()).groups -match \"S-1-5-32-544\");\nif ( $amIAdmin )\n{\n\n  ############################`n###  Chocolatey install  ###`n############################`n\";\n  Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'));\n  ############################`n###     Wsl2 install     ###`n############################`n\";\n  choco install -y wsl2 -params \"/Version:2 /Retry:true\";\n  ############################`n###        Reboot        ###`n############################\";\n  restart-computer -Confirm;\n\n}\nelse {\n  ###############################`n###  Requires admin rights  ###`n###############################`n\";\n}\n</code></pre> <p>Step 2</p> <p>Red\u00e9marrage fin d'installation du wsl</p> <p>Step 3</p> <pre><code>$amIAdmin = (([System.Security.Principal.WindowsIdentity]::GetCurrent()).groups -match \"S-1-5-32-544\");\nif ( $amIAdmin )\n{\n\n  ########################`n###  Configure Wsl2  ###`n########################`n\";\n  wsl --set-default-version 2\n  ###############################`n###  Download Linux images  ###`n###############################`n\";\n  $ProgressPreference = 'SilentlyContinue';\n  Invoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile $Env:tmp\\\\wsl-ubuntu-2004.appx -UseBasicParsing;\n  ##############################`n###  Install Linux images  ###`n##############################`n\";\n  Add-AppxPackage $Env:tmp\\\\wsl-ubuntu-2004.appx;\n\n}\nelse {\n  ###############################`n###  Requires admin rights  ###`n###############################`n\";\n}\n</code></pre> <p>Step 4</p> <pre><code>$amIAdmin = (([System.Security.Principal.WindowsIdentity]::GetCurrent()).groups -match \"S-1-5-32-544\");\nif ( $amIAdmin )\n{\n  ########################`n###  Vcxsrv Install   ###`n########################`n\";\n  choco install -y vcxsrv\n  ###############################`n###  Docker desktop install  ###`n###############################`n\";\n  choco install -y docker-desktop\n  ############################`n###        Reboot        ###`n############################\";\n  restart-computer -Confirm;\n}\nelse {\n  ###############################`n###  Requires admin rights  ###`n###############################`n\";\n}\n</code></pre>"},{"location":"operating-system/wsl2/#source","title":"Source","text":"<p>Enable Nested Virtualization In VirtualBox </p>"},{"location":"software/atom/","title":"Atom : Installation et Configuration","text":""},{"location":"software/atom/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Windows 10 Professionnel 1803 Atom 1.28.1"},{"location":"software/atom/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>Dl + Clic + Install</p>"},{"location":"software/atom/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>Installation des packages suppl\u00e9mentaires :</p> <pre><code>apm install atom-beautify busy-signal compare-files file-icons highlight-selected intentions language-ansible language-docker language-groovy linter linter-ansible-linting linter-ansible-syntax linter-js-yaml linter-ui-default linter-vagrant-validate minimap minimap-highlight-selected minimap-linter minimap-split-diff sort-lines split-diff\n</code></pre>"},{"location":"software/atom/#knowledge-base","title":"Knowledge Base","text":""},{"location":"software/atom/#atom_home","title":"ATOM_HOME","text":"<p>Alternatively you can set the ATOM_HOME environment variable to point wherever you want (you can write a .sh or .cmd script to temporarily set it and launch it from that)  </p>"},{"location":"software/atom/#proxy-and-firewall-settings","title":"Proxy and firewall settings","text":"<pre><code>apm config set strict-ssl false\napm config set https-proxy YOUR_PROXY_ADDRESS\n</code></pre>"},{"location":"software/atom/#apm-install-package","title":"Apm Install package","text":"<pre><code>apm install &lt;package_name&gt; # to install the latest version.\napm install &lt;package_name&gt;@&lt;package_version&gt;  # to install a specific version.\n</code></pre>"},{"location":"software/atom/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"software/atom/#atom-basics","title":"Atom Basics","text":"Actions Shortcuts Command Palette <code>Ctrl+Shift+P</code> Settings view <code>Ctrl+,</code> Opening a file <code>Ctrl+O</code> Save file <code>Ctrl+S</code> Open directory <code>Ctrl+Shift+A</code> Open a file in a project <code>Ctrl+T or Ctrl+P</code> Open a file in a project recently <code>Ctrl+B</code>"},{"location":"software/atom/#moving-in-atom","title":"Moving in Atom","text":"Actions Shortcuts Navigate with symbol <code>Ctrl+R</code> Bookmarks line <code>Maj+Ctrl+F2</code> Next Bookmarks <code>F2</code> Previous Bookmarks <code>Shift+F2</code> Open Bookmarks <code>Ctrl+F2</code> Move to the beginning of word <code>Ctrl+Left</code> Move to the end of word <code>Ctrl+Right</code> Move to the first character of the current line <code>Home</code> Move to the end of the line <code>End</code> Move to the top of the file <code>Ctrl+Home</code> Move to the bottom of the file <code>Ctrl+End</code> Move directly <code>Ctrl+G</code>"},{"location":"software/atom/#editing-and-deleting-text","title":"Editing and Deleting Text","text":"Actions Shortcuts Upper case the current word <code>Ctrl+K Ctrl+U</code> Lower case the current word <code>Ctrl+K Ctrl+L</code> Delete current line <code>Ctrl+Shift+K</code> Delete to beginning of word <code>Ctrl+Backspace</code> Delete to end of word <code>Ctrl+Delete</code> Add a new cursor at the clicked location <code>Ctrl+Click</code> Add another cursor above/below the current cursor <code>Alt+Ctrl+Up/Down</code> Select the next word in the document that is the same as the currently selected word <code>Ctrl+D</code> Select all words in the document that are the same as the currently selected word <code>Alt+F3</code> Jump to the bracket matching the one adjacent to the cursor. <code>Ctrl+M</code> Select all the text inside the current brackets <code>Alt+Ctrl+,</code> Close the current XML/HTML tag <code>Alt+Ctrl+.</code>"},{"location":"software/atom/#find-and-replace","title":"Find and Replace","text":"Actions Shortcuts Search within a buffer <code>Ctrl+F</code> Search the entire project <code>Ctrl+Shift+F</code>"},{"location":"software/atom/#panes","title":"Panes","text":"Actions Shortcuts Split Pane <code>Ctrl+K Up/Down/Left/Right</code> Move Pane <code>Ctrl+K Ctrl+Up/Down/Left/Right</code> Close Pane <code>Ctrl+K Ctrl+W</code>"},{"location":"software/atom/#grammar","title":"Grammar","text":"Actions Shortcuts Select grammar <code>Ctrl+Shift+L</code>"},{"location":"software/atom/#snippets","title":"Snippets","text":"Actions Shortcuts Show Snippets <code>Ctrl+Shift+P</code>"},{"location":"software/atom/#plugins","title":"Plugins","text":"Actions Shortcuts Beautify <code>Ctrl+Alt+B</code> Sort Lines <code>F5</code>"},{"location":"software/atom/#source","title":"Source","text":"<p>Atom Flight Manual</p>"},{"location":"software/msys2/","title":"Msys2 : Installation et Configuration","text":""},{"location":"software/msys2/#tool-versions","title":"Tool Versions","text":"Os / Tool Version Windows 10 Professionnel 20H2 Msys2 20210419"},{"location":"software/msys2/#installation-procedure","title":"Installation procedure","text":"<p>Start by getting and installing the Msys2 binary</p> <p>Run the installer and follow the steps. Make sure to install MSYS2 in your user directory. <code>C:\\Users\\USER_NAME\\.msys64</code></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"software/msys2/#post-installation-procedure","title":"Post installation procedure","text":"<p>Run <code>MSYS2 MinGW 64-bit</code> </p> <p>If you are behind a proxy configure it from the command-line to start:</p> <pre><code>export http_proxy=\"http://LOGIN:PASSWORD@PROXY:PORT\" &amp;&amp; export https_proxy=$http_proxy &amp;&amp; curl -ivks https://github.com\n</code></pre> <p>In case of TLS proxy get the root and intermediate authorities and add them.</p> <pre><code>mkdir ~/certs\n# Retrieving certificates\ntrust anchor --store ~/certs/cert001.crt\n# [...]\ntrust anchor --store ~/certs/certXXX.crt\nupdate-ca-trust\n</code></pre> <p>Update the system</p> <pre><code>pacman -Syu\n</code></pre> <p>Install the base-devel and mingw-w64-x86_64-toolchain packages</p> <pre><code>pacman -S --needed --noconfirm base-devel mingw-w64-x86_64-toolchain\n</code></pre>"},{"location":"software/msys2/#customisation","title":"Customisation","text":""},{"location":"software/msys2/#basic-tools","title":"Basic tools","text":"<pre><code>pacman -S --needed --noconfirm ansible curl gcc git make man-db tmux tree unzip vim zsh\n</code></pre>"},{"location":"software/msys2/#programming-language-and-co","title":"Programming language and Co","text":"<pre><code>pacman -S --needed --noconfirm libcrypt-devel libffi-devel libyaml-devel mingw-w64-x86_64-libffi mingw-w64-x86_64-libsodium mingw-w64-x86_64-openssl mingw-w64-x86_64-pkg-config mingw-w64-x86_64-python mingw-w64-x86_64-python-pip mingw-w64-x86_64-ruby openssh openssl-devel\n</code></pre>"},{"location":"software/msys2/#python-tools","title":"Python tools","text":"<pre><code>export CRYPTOGRAPHY_DONT_BUILD_RUST=1\nexport SODIUM_INSTALL=system CFLAGS=`pkg-config --cflags libffi`\nexport LDFLAGS=`pkg-config --libs libffi`\npip install cffi --no-binary :all:\nexport C_INCLUDE_PATH=/mingw64/include\nexport LIBRARY_PATH=/mingw64/lib\npip install pynacl\npip install beautysh gita gitlint httpie pre-commit yamllint\n</code></pre>"},{"location":"software/msys2/#smoke-tests","title":"Smoke Tests","text":"<p>Check that everything is correctly installed</p> <pre><code>tools=( ansible beautysh curl gcc git gita gitlint http locate make pip  pre-commit tmux tree unzip vim yamllint zsh )\nfor i in \"${tools[@]}\"\ndo\n  which $i &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"$i ok\" || echo \"$i ko\"\ndone\n</code></pre>"},{"location":"software/msys2/#advanced-customisation","title":"Advanced customisation","text":"<p>Generate your SSH key</p> <pre><code>ssh-keygen -t ed25519 -f $HOME/.ssh/id_ed25519 -C $(whoami)@$HOSTNAME\n</code></pre> <p>Install ohmyzsh</p> <pre><code>sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n</code></pre> <p>Install additional ohmyzsh plugins</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions.git ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\n</code></pre> <p>Change msys2 launch to switch from bash to Zsh edit the <code>C:\\Users\\USER_NAME\\.msys64\\msys2_shell.cmd</code></p> <pre><code>@echo off\nsetlocal EnableDelayedExpansion\n\nset \"WD=%__CD__%\"\nif NOT EXIST \"%WD%msys-2.0.dll\" set \"WD=%~dp0usr\\bin\\\"\nset \"LOGINSHELL=zsh\"\n[...] truncate output\n</code></pre> <p>Clone and install the powerline font and tmuxifier</p> <pre><code>mkdir -p /opt/github &amp;&amp; cd /opt/github\ngit clone https://github.com/powerline/fonts.git\ngit clone https://github.com/jimeh/tmuxifier.git\n</code></pre> <p>Install powerline font by running the script <code>C:\\Users\\USER_NAME\\.msys64\\opt\\github\\fonts\\install.ps1</code></p> <pre><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nC:\\Users\\USER_NAME\\.msys64\\opt\\github\\fonts\\install.ps1\n</code></pre> <p>Finally</p> <ul> <li>Install dotfiles</li> <li>Configure mingw color</li> </ul>"},{"location":"software/msys2/#source","title":"Source","text":"<p>GitHub - Msys2 installer Microsoft - Maximum Path Length Limitation Msys2 - site GitHub - Install Ansible on msys2 </p>"},{"location":"software/reveal.js/","title":"Reveal","text":""},{"location":"software/reveal.js/#revealjs","title":"Reveal.js","text":""},{"location":"software/reveal.js/#tools-versions","title":"Tools versions","text":"Os / Tool Version Linux Mint 19.3 reveal.js 4.1.0"},{"location":"software/reveal.js/#todo","title":"Todo","text":"<p>N/A</p>"},{"location":"software/reveal.js/#bulk-note","title":"Bulk Note","text":"<p>N/A</p>"},{"location":"software/reveal.js/#about","title":"About","text":"<p>reveal.js is an open source HTML presentation framework. It's a tool that enables anyone with a web browser to create fully-featured and beautiful presentations for free.</p>"},{"location":"software/reveal.js/#installation-procedure","title":"Installation procedure","text":"<p>Full Setup</p> <pre><code>git clone https://github.com/hakimel/reveal.js.git\ncd reveal.js &amp;&amp; npm install\n</code></pre>"},{"location":"software/reveal.js/#getting-start","title":"Getting start","text":"<p>To quickly execute reveal.js</p> <pre><code>cd reveal.js &amp;&amp; npm start\n</code></pre>"},{"location":"software/reveal.js/#content","title":"Content","text":""},{"location":"software/reveal.js/#markup","title":"Markup","text":"<p>Simple example :</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"dist/reveal.css\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"dist/theme/white.css\"&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"reveal\"&gt;\n      &lt;div class=\"slides\"&gt;\n        &lt;section&gt;Horizontal Slide&lt;/section&gt;\n        &lt;section&gt;\n          &lt;section&gt;Vertical Slide 1&lt;/section&gt;\n          &lt;section&gt;Vertical Slide 2&lt;/section&gt;\n        &lt;/section&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=\"dist/reveal.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n      Reveal.initialize();\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"software/reveal.js/#markdown","title":"Markdown","text":"<p>Simple example :</p> <pre><code>&lt;section data-markdown&gt;\n  &lt;textarea data-template&gt;\n    ## Slide 1\n    A paragraph with some text and a [link](http://hakim.se).\n    ---\n    ## Slide 2\n    ---\n    ## Slide 3\n  &lt;/textarea&gt;\n&lt;/section&gt;\n</code></pre> <p>Include plugin</p> <pre><code>&lt;script src=\"plugin/markdown/markdown.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMarkdown ]\n  });\n&lt;/script&gt;\n</code></pre> <p>External Markdown</p> <pre><code>&lt;section data-markdown=\"example.md\"\n  data-separator=\"^\\n\\n\\n\"\n  data-separator-vertical=\"^\\n\\n\"\n  data-separator-notes=\"^Note:\"\n  data-charset=\"iso-8859-15\"&gt;\n&lt;/section&gt;\n</code></pre> <p>Note :</p> <ul> <li>data-markdown: defines the external Markdown</li> <li>data-separator: defines a regular expression for horizontal slides</li> <li>data--separator-vertical: defines a regular expression for vertical slides</li> <li>data--separator-notes: specifying the beginning of the current slide's speaker notes</li> </ul> <p>Element and Slide Attributes Can be added through HTML comments</p> <pre><code>&lt;section data-markdown&gt;\n  &lt;script type=\"text/template\"&gt;\n    - Item 1 &lt;!-- .element: class=\"fragment\" data-fragment-index=\"2\" --&gt;\n    - Item 2 &lt;!-- .element: class=\"fragment\" data-fragment-index=\"1\" --&gt;\n  &lt;/script&gt;\n&lt;/section&gt;\n&lt;section data-markdown&gt;\n  &lt;script type=\"text/template\"&gt;\n  &lt;!-- .slide: data-background=\"#ff0000\" --&gt;\n    Markdown content\n  &lt;/script&gt;\n&lt;/section&gt;\n</code></pre> <p>Using bracket</p> <pre><code>&lt;section data-markdown&gt;\n  &lt;textarea data-template&gt;\n    ``js [1-2|3|4]\n    let a = 1;\n    let b = 2;\n    let c = x =&gt; 1 + 2 + x;\n    c(3);\n    ``\n  &lt;/textarea&gt;\n&lt;/section&gt;\n</code></pre>"},{"location":"software/reveal.js/#backgrounds","title":"Backgrounds","text":"<p>Simply add a data-background in a section</p> Attribute Default Description data-background-color All CSS color formats are supported. data-background-image URL of the image to show. data-background-size cover See background-size on MDN. data-background-position center See background-position on MDN. data-background-repeat no-repeat See background-repeat on MDN. data-background-opacity 1 0 is transparent and 1 is fully opaque. data-background-video A single video source, or a comma separated list of video sources. data-background-video-loop false Flags if the video should play repeatedly. data-background-video-muted false Flags if the audio should be muted. data-background-size cover Use cover for full screen or contain for letterboxing. data-background-opacity 1 0 is transparent and 1 is fully opaque. data-background-iframe URL of the iframe to load data-background-interactive false Make it possible to interact with the iframe contents. <p>Use <code>backgroundTransition</code> to set background transition.  </p> <p>If you want to use a parallax scrolling background configure it when initializing reveal.js</p>"},{"location":"software/reveal.js/#code","title":"Code","text":"<p>Simple example</p> <pre><code>&lt;section&gt;\n  &lt;pre&gt;&lt;code data-trim data-noescape&gt;\n  (def lazy-fib\n    (concat\n    [0 1]\n    ((fn rfib [a b]\n      (lazy-cons (+ a b) (rfib b (+ a b)))) 0 1))\n  )\n  &lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n</code></pre> <p>Note :</p> <ul> <li>data-trim: surrounding whitespace within the <code>&lt;code&gt;</code> is automatically removed</li> <li>data-noescape: defines a regular expression for horizontal slides</li> </ul>"},{"location":"software/reveal.js/#theming","title":"Theming","text":"<p>Simple example</p> <pre><code>&lt;link rel=\"stylesheet\" href=\"lib/css/monokai.css\"&gt;\n&lt;script src=\"plugin/highlight/highlight.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealHighlight ]\n  });\n&lt;/script&gt;\n</code></pre> <p>Line Numbers &amp; Highlights &amp; Step-by-setp</p> <p>To highlight code use data-line-numbers. To use step by step put a <code>|</code> betweend each lines</p> <pre><code>&lt;pre&gt;&lt;code data-line-numbers=\"3-5|8-10|13-15\"&gt;\n&lt;table&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Apples&lt;/td&gt;\n    &lt;td&gt;$1&lt;/td&gt;\n    &lt;td&gt;7&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Oranges&lt;/td&gt;\n    &lt;td&gt;$2&lt;/td&gt;\n    &lt;td&gt;18&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Kiwi&lt;/td&gt;\n    &lt;td&gt;$3&lt;/td&gt;\n    &lt;td&gt;1&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n&lt;/code&gt;&lt;/pre&gt;\n</code></pre>"},{"location":"software/reveal.js/#fragment","title":"Fragment","text":"<p>Simple example</p> <pre><code>&lt;p class=\"fragment\"&gt;Fade in&lt;/p&gt;\n&lt;p class=\"fragment fade-out\"&gt;Fade out&lt;/p&gt;\n&lt;p class=\"fragment highlight-red\"&gt;Highlight red&lt;/p&gt;\n&lt;p class=\"fragment fade-in-then-out\"&gt;Fade in, then out&lt;/p&gt;\n&lt;p class=\"fragment fade-up\"&gt;Slide up while fading in&lt;/p&gt;\n</code></pre> Name Effect fade-out Start visible, fade out fade-up Slide up while fading in fade-down Slide down while fading in fade-left Slide left while fading in fade-right Slide right while fading in fade-in-then-out Fades in, then out on the next step fade-in-then-semi-out Fades in, then to 50% on the next step grow Scale up shrink Scale down strike Strike through highlight-red Turn text red highlight-green Turn text green highlight-blue Turn text blue highlight-current-red Turn text red, then back to original on next step highlight-current-green Turn text green, then back to original on next step highlight-current-blue Turn text blue, then back to original on next step <p>Nested Fragments</p> <pre><code>&lt;span class=\"fragment fade-in\"&gt;\n  &lt;span class=\"fragment highlight-red\"&gt;\n    &lt;span class=\"fragment fade-out\"&gt;\n      Fade in &gt; Turn red &gt; Fade out\n    &lt;/span&gt;\n  &lt;/span&gt;\n&lt;/span&gt;\n</code></pre> <p>Fragments order</p> <pre><code>&lt;p class=\"fragment\" data-fragment-index=\"3\"&gt;Appears last&lt;/p&gt;\n&lt;p class=\"fragment\" data-fragment-index=\"1\"&gt;Appears first&lt;/p&gt;\n&lt;p class=\"fragment\" data-fragment-index=\"2\"&gt;Appears second&lt;/p&gt;\n</code></pre>"},{"location":"software/reveal.js/#links","title":"Links","text":"<pre><code>&lt;section&gt;\n  &lt;a href=\"#/grand-finale\"&gt;Go to the last slide&lt;/a&gt;\n&lt;/section&gt;\n&lt;section&gt;\n  &lt;h2&gt;Slide 2&lt;/h2&gt;\n&lt;/section&gt;\n&lt;section id=\"grand-finale\"&gt;\n  &lt;h2&gt;The end&lt;/h2&gt;\n  &lt;a href=\"#/0\"&gt;Back to the first&lt;/a&gt;\n&lt;/section&gt;\n&lt;section&gt;\n  &lt;a href=\"#/2\"&gt;Go to 2nd slide&lt;/a&gt;\n  &lt;a href=\"#/3/2\"&gt;Go to the 2nd vertical slide inside of the 3rd slide&lt;/a&gt;\n&lt;/section&gt;\n</code></pre> <p>Simple example</p>"},{"location":"software/reveal.js/#layout","title":"Layout","text":"<p>Layout provide a few different helper classes for controlling the layout and styling your content.</p> <ul> <li>r-stack: The r-stack layout helper lets you center and place multiple elements on top of each other.</li> <li>r-fit-text: The r-fit-text class makes text as large as possible without overflowing the slide.</li> <li>r-stretch: The r-stretch layout helper lets you resize an element, like an image or video, to cover the remaining vertical space in a slide.</li> </ul>"},{"location":"software/reveal.js/#slide-visibility","title":"Slide Visibility","text":"<p>The data-visibility attribute can be used to hide slides.</p> <pre><code>&lt;section&gt;Slide 1&lt;/section&gt;\n&lt;section data-visibility=\"hidden\"&gt;Slide 2&lt;/section&gt;\n&lt;section&gt;Slide 3&lt;/section&gt;\n&lt;section&gt;Slide 1&lt;/section&gt;\n&lt;section&gt;Slide 2&lt;/section&gt;\n&lt;section data-visibility=\"uncounted\"&gt;Slide 3&lt;/section&gt;\n</code></pre>"},{"location":"software/reveal.js/#customization","title":"Customization","text":""},{"location":"software/reveal.js/#themes","title":"Themes","text":"Name Effect black Black background, white text, blue links (default) white White background, black text, blue links league Gray background, white text, blue links beige Beige background, dark text, brown links sky Blue background, thin dark text, blue links night Black background, thick white text, orange links serif Cappuccino background, gray text, brown links simple White background, black text, blue links solarized Cream-colored background, dark green text, blue links blood Dark background, thick white text, red links moon Dark blue background, thick grey text, blue links"},{"location":"software/reveal.js/#transitions","title":"Transitions","text":"<p>Simple example</p> <pre><code>&lt;section data-transition=\"zoom\"&gt;\n  &lt;h2&gt;This slide will override the presentation transition and zoom!&lt;/h2&gt;\n&lt;/section&gt;\n\n&lt;section data-transition-speed=\"fast\"&gt;\n  &lt;h2&gt;Choose from three transition speeds: default, fast or slow!&lt;/h2&gt;\n&lt;/section&gt;\n</code></pre> <p>Backgrounds Transitions</p> <pre><code>Reveal.initialize({\n  backgroundTransition: 'slide'\n});\n</code></pre> Name Effect -none Switch backgrounds instantly -fade Cross fade \u2014 default for background transitions -slide Slide between backgrounds \u2014 default for slide transitions -convex Slide at a convex angle -concave Slide at a concave angle -zoom Scale the incoming slide up so it grows in from the center of the screen"},{"location":"software/reveal.js/#configuration-options","title":"Configuration Options","text":"<p>See Configuration Options</p>"},{"location":"software/reveal.js/#presentation-size","title":"Presentation Size","text":"<p>We can customize presentation layout</p> <pre><code>Reveal.initialize({\n  width: 960,\n  height: 700,\n  margin: 0.04,\n  minScale: 0.2,\n  maxScale: 2.0,\n  center: false\n});\n</code></pre>"},{"location":"software/reveal.js/#features","title":"Features","text":""},{"location":"software/reveal.js/#vertical-slides","title":"Vertical Slides","text":"<p>Simple example</p> <pre><code>&lt;section&gt;Horizontal Slide&lt;/section&gt;\n&lt;section&gt;\n  &lt;section&gt;Vertical Slide 1&lt;/section&gt;\n  &lt;section&gt;Vertical Slide 2&lt;/section&gt;\n&lt;/section&gt;\n</code></pre> <p>Navigation mode can be tuned by setting <code>navigationMode</code> config option</p> <ul> <li>default</li> <li>linear</li> <li>grid</li> </ul>"},{"location":"software/reveal.js/#auto-animate","title":"Auto-Animate","text":"<p>reveal.js can automatically animate elements across slides. All you need to do is add data-auto-animate to two adjacent slide <code>&lt;section&gt;</code> elements and Auto-Animate will animate all matching elements between the two.</p> <p>Complex example with highlighting code. For more information </p> <pre><code>&lt;section data-auto-animate&gt;\n  &lt;ul&gt;\n    &lt;li&gt;Mercury&lt;/li&gt;\n    &lt;li&gt;Jupiter&lt;/li&gt;\n    &lt;li&gt;Mars&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/section&gt;\n&lt;section data-auto-animate&gt;\n  &lt;ul&gt;\n    &lt;li&gt;Mercury&lt;/li&gt;\n    &lt;li&gt;Earth&lt;/li&gt;\n    &lt;li&gt;Jupiter&lt;/li&gt;\n    &lt;li&gt;Saturn&lt;/li&gt;\n    &lt;li&gt;Mars&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/section&gt;\n</code></pre> <pre><code>&lt;section data-auto-animate&gt;\n  &lt;pre data-id=\"code-animation\"&gt;&lt;code data-trim data-line-numbers&gt;\n    let planets = [\n      { name: 'mars', diameter: 6779 },\n    ]\n  &lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section data-auto-animate&gt;\n  &lt;pre data-id=\"code-animation\"&gt;&lt;code data-trim data-line-numbers&gt;\n    let planets = [\n      { name: 'mars', diameter: 6779 },\n      { name: 'earth', diameter: 12742 },\n      { name: 'jupiter', diameter: 139820 }\n    ]\n  &lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section data-auto-animate&gt;\n  &lt;pre data-id=\"code-animation\"&gt;&lt;code data-trim data-line-numbers&gt;\n    let circumferenceReducer = ( c, planet ) =&gt; {\n      return c + planet.diameter * Math.PI;\n    }\n\n    let planets = [\n      { name: 'mars', diameter: 6779 },\n      { name: 'earth', diameter: 12742 },\n      { name: 'jupiter', diameter: 139820 }\n    ]\n\n    let c = planets.reduce( circumferenceReducer, 0 )\n  &lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n</code></pre>"},{"location":"software/reveal.js/#auto-slide","title":"Auto-Slide","text":"<p>Presentations can be configured to step through slides automatically, without any user input. To enable this you will need to specify an interval for slide changes using the autoSlide config option. The interval is provided in milliseconds.</p> <pre><code>// Slide every five seconds\nReveal.initialize({\n  autoSlide: 5000,\n  loop: true\n});\n</code></pre>"},{"location":"software/reveal.js/#speaker-view","title":"Speaker View","text":"<pre><code>&lt;section&gt;\n  &lt;h2&gt;Some Slide&lt;/h2&gt;\n\n  &lt;aside class=\"notes\"&gt;\n    Shhh, these are your private notes \ud83d\udcdd\n  &lt;/aside&gt;\n&lt;/section&gt;\n&lt;section \n  data-markdown=\"example.md\"\n  data-separator=\"^\\n\\n\\n\"\n  data-separator-vertical=\"^\\n\\n\"\n  data-separator-notes=\"^Note:\"&gt;\n# Title\n## Sub-title\n\nHere is some content...\n\nNote:\nThis will only display in the notes window.\n&lt;/section&gt;\n</code></pre> <p>Create simple note</p>"},{"location":"software/reveal.js/#slide-numbers","title":"Slide Numbers","text":"<p>Simple example</p> <pre><code>Reveal.initialize({ slideNumber: true });\n&lt;/section&gt;\n</code></pre> <p>Format</p> Value Description h.v Horizontal . Vertical slide number (default) h/v Horizontal / Vertical slide number c Flattened slide number, including both horizontal and vertical slides c/t Flattened slide number / total slides"},{"location":"software/reveal.js/#pdf-export","title":"PDF Export","text":"<p>Simple example  </p> <pre><code>// http://localhost:8000/demo?print-pdf\nReveal.configure({\n  showNotes: true,\n  showNotes: 'separate-page',\n  pdfMaxPagesPerSlide: 1,\n  pdfSeparateFragments: false\n});\n</code></pre>"},{"location":"software/reveal.js/#overview-mode","title":"Overview Mode","text":"<p>Press Esc button</p>"},{"location":"software/reveal.js/#fullscreen-mode","title":"Fullscreen Mode","text":"<p>Press F button</p>"},{"location":"software/reveal.js/#plugins","title":"Plugins","text":"<ol> <li>Include the plugin script in the document. (Some plugins may require styles as well.)</li> <li>Tell reveal.js about the plugin by including it in the plugins array when initializing.</li> </ol> <pre><code>&lt;script src=\"plugin/markdown/markdown.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  Reveal.initialize({\n    plugins: [ RevealMarkdown ]\n  });\n&lt;/script&gt;\n</code></pre>"},{"location":"software/reveal.js/#source","title":"Source","text":"<p>Docs: creating a theme Docs: highlight.js Docs: reveal.js Plugins: Built-in Plugins: Third Party Plugins</p>"},{"location":"software/vagrant/","title":"Vagrant","text":""},{"location":"software/vagrant/#vagrant-installation-et-configuration","title":"Vagrant : Installation et Configuration","text":""},{"location":"software/vagrant/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Windows 10 Professionnel 1803 Vagrant 2.1.5"},{"location":"software/vagrant/#naming-convention","title":"Naming convention","text":"Type Name Example Base box operating_system_namecurrent_version debian10 Vagrant box ff7_city_name-base_box_trigram midgar-deb Vagrant box Tools tools-ff7_city_name-base_box_trigram cicd-midgar-deb"},{"location":"software/vagrant/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>La proc\u00e9dure d'installation de Vagrant sur Windows 10 se d\u00e9roule de la fa\u00e7on suivante :</p> <p>Commencez par ex\u00e9cuter l'installeur vagrant_2.1.5_x86_64.msi.  </p> <p>Dans la fen\u00eatre Welcome to the Vagrant Setup Wizard cliquez sur Next </p> <p>Dans la fen\u00eatre End User Licence Agreement cochez I  accept the terms in License Agreement et cliquez sur Next </p> <p>Dans la fen\u00eatre Destination Folder laissez par d\u00e9faut et cliquez sur Next </p> <p>Dans la fen\u00eatre Ready to install Vagrant cliquez sur Install </p> <p>Dans la fen\u00eatre Installing Vagrant attendez la fin des op\u00e9rations et cliquez sur  Next </p> <p>Dans la fen\u00eatre Completed the Vagrant Setup Wizard cliquez sur Finish </p>"},{"location":"software/vagrant/#procedure-de-post-installation","title":"Proc\u00e9dure de post-installation","text":"<p>Verifying the Installation :  </p> <pre><code>C:\\Users\\micha&gt;vagrant\nUsage: vagrant [options] &lt;command&gt; [&lt;args&gt;]\n\n  -v, --version                    Print the version and exit.\n  -h, --help                       Print this help.\n  [...]\n</code></pre>"},{"location":"software/vagrant/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"software/vagrant/#vagrant-basics","title":"Vagrant Basics","text":"Commands Description box This is the command used to manage (add, remove, etc.) boxes. init This initializes the current directory to be a Vagrant environment by creating an initial Vagrantfile if one does not already exist. up This command creates and configures guest machines according to your Vagrantfile. halt This command shuts down the running machine Vagrant is managing. destroy This command stops the running machine Vagrant is managing and destroys all resources that were created during the machine creation process. suspend This suspends the guest machine Vagrant is managing, rather than fully shutting it down or destroying it. resume This resumes a Vagrant managed machine that was previously suspended, perhaps with the suspend command. reload The equivalent of running a halt followed by an up. status This will tell you the state of the machines Vagrant is managing. snapshot This is the command used to manage snapshots with the guest machine."},{"location":"software/vagrant/#vagrant-essential","title":"Vagrant Essential","text":"Commands Description SSH This will SSH into a running Vagrant machine and give you access to a shell. port The port command displays the full list of guest ports mapped to the host machine ports. provision Runs any configured provisioners against the running Vagrant managed machine. package This packages a currently running VirtualBox or Hyper-V environment into a re-usable box. global-status This command will tell you the state of all active Vagrant environments on the system for the currently logged in user. validate This command validates your Vagrantfile. ssh-config This will output valid configuration for an SSH config file to SSH into the running Vagrant machine from SSH directly (instead of using vagrant SSH). version This command tells you the version of Vagrant you have installed as well as the latest version of Vagrant that is currently available."},{"location":"software/vagrant/#vagrant-advanced","title":"Vagrant Advanced","text":"Commands Description connect The connect command complements the share command by enabling access to shared environments. login The login command is used to authenticate with the HashiCorp's Vagrant Cloud server. plugin This is the command used to manage plugins. powershell This will open a PowerShell prompt on the host into a running Vagrant guest machine. rdp This will start an RDP client for a remote desktop session with the guest. share The share command initializes a Vagrant Share session, allowing you to share your Vagrant environment with anyone in the world."},{"location":"software/vagrant/#tutoriels-vagrant","title":"Tutoriels Vagrant","text":""},{"location":"software/vagrant/#vagrant-getting-started","title":"Vagrant : Getting Started","text":""},{"location":"software/vagrant/#introduction","title":"Introduction","text":"<p>Vagrant is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the \"works on my machine\" excuse a relic of the past.</p>"},{"location":"software/vagrant/#vagrant-vs-other-software","title":"Vagrant vs. Other Software","text":"<p>Command-line tools : Virtualization software like VirtualBox and VMware come with command-line utilities for managing the lifecycle of machines on their platform. Many people make use of these utilities to write their own automation. Vagrant actually uses many of these utilities internally.  </p> <p>Docker : Vagrant is a tool focused on providing a consistent development environment workflow across multiple operating systems. Docker is a container management that can consistently run software as long as a containerization system exists.  </p> <p>Terraform : Vagrant and Terraform are both projects from HashiCorp. Vagrant is a tool focused for managing development environments and Terraform is a tool for building infrastructure.</p>"},{"location":"software/vagrant/#quickly-start-and-try","title":"Quickly start and try","text":"<pre><code>vagrant init hashicorp/precise64\nvagrant up\n</code></pre> <p>After running the above two commands, you will have a fully running virtual machine in VirtualBox running Ubuntu 12.04 LTS 64-bit. You can SSH into this machine with <code>vagrant ssh</code>, and when you are done playing around, you can terminate the virtual machine with vagrant destroy.</p>"},{"location":"software/vagrant/#project-setup","title":"Project Setup","text":"<p>The first step in configuring any Vagrant project is to create a Vagrantfile. The purpose of the Vagrantfile is twofold:</p> <ul> <li> <p>Mark the root directory of your project. Many of the configuration options in Vagrant are relative to this root directory.</p> </li> <li> <p>Describe the kind of machine and resources you need to run your project, as well as what software to install and how you want to access it.</p> </li> </ul> <pre><code>mkdir vagrant_getting_started\ncd vagrant_getting_started\nvagrant init\n</code></pre>"},{"location":"software/vagrant/#boxes","title":"Boxes","text":"<p>Boxes are added to Vagrant with vagrant box add. This stores the box under a specific name so that multiple Vagrant environments can re-use it. If you have not added a box yet, you can do so now:</p> <pre><code>vagrant box add hashicorp/precise64\n</code></pre> <p>Using a Box :  </p> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.box = \"hashicorp/precise64\"\nend\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"hashicorp/precise64\"\n  config.vm.box_version = \"1.1.0\"\nend\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"hashicorp/precise64\"\n  config.vm.box_url = \"https://vagrantcloud.com/hashicorp/precise64\"\nend\n</code></pre>"},{"location":"software/vagrant/#up-and-ssh","title":"Up And SSH","text":"<p>Boot vagrant Box :  </p> <pre><code>vagrant up\n</code></pre> <p>SSH connection :  </p> <pre><code>vagrant ssh\n</code></pre>"},{"location":"software/vagrant/#synced-folders","title":"Synced Folders","text":"<p>By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.</p> <pre><code>$ vagrant up\n...\n$ vagrant ssh\n...\nvagrant@precise64:~$ ls /vagrant\nVagrantfile\n</code></pre>"},{"location":"software/vagrant/#provisioning","title":"Provisioning","text":"<p>Vagrant has built-in support for automated provisioning. Using this feature, Vagrant will automatically install software when you vagrant up so that the guest machine can be repeatably created and ready-to-use.</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.box = \"hashicorp/precise64\"\n  config.vm.provision :shell, path: \"bootstrap.sh\"\nend\n</code></pre> <p>To force provisioning :</p> <pre><code>vagrant reload --provision\nvagrant provision\n</code></pre>"},{"location":"software/vagrant/#networking-port-forwarding","title":"Networking Port Forwarding","text":"<pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.box = \"hashicorp/precise64\"\n  config.vm.provision :shell, path: \"bootstrap.sh\"\n  config.vm.network :forwarded_port, guest: 80, host: 4567\nend\n</code></pre>"},{"location":"software/vagrant/#share","title":"Share","text":"<p>Vagrant Share lets you share your Vagrant environment to anyone around the world with an internet connection. It will give you a URL that will route directly to your Vagrant environment from any device in the world that is connected to the internet.</p> <pre><code>vagrant share\n...\n==&gt; default: Creating Vagrant Share session...\n==&gt; default: HTTP URL: http://b1fb1f3f.ngrok.io\n...\n</code></pre>"},{"location":"software/vagrant/#teardown","title":"Teardown","text":"<p>Suspending the virtual machine by calling vagrant suspend will save the current running state of the machine and stop it. Internet.</p> <pre><code>vagrant suspend\n</code></pre> <p>Halting the virtual machine by calling vagrant halt will gracefully shut down the guest operating system and power down the guest machine</p> <pre><code>vagrant halt\n</code></pre> <p>Destroying the virtual machine by calling vagrant destroy will remove all traces of the guest machine from your system. It'll stop the guest machine, power it down, and remove all of the guest hard disks.</p> <pre><code>vagrant destroy\n</code></pre>"},{"location":"software/vagrant/#rebuild","title":"Rebuild","text":"<p>Simply :</p> <pre><code>vagrant up\n</code></pre>"},{"location":"software/vagrant/#vagrant-base-box-creation","title":"Vagrant Base Box Creation","text":"<pre><code>vagrant package --base pandama-vanilla --output debvanilla.box\n</code></pre> <p>Update Base Box : Connect to base box as root.</p> <pre><code>apt update &amp;&amp; apt upgrade &amp;&amp; apt dist-upgrade\n</code></pre> <p>Mount last virtualbox guest additions cd.</p> <pre><code>mount /dev/cdrom /mnt\nsh /mnt/VBoxLinuxAdditions.run\numount /mnt\n</code></pre> <p>Eject the disc tray. Then update it :</p> <pre><code>vagrant box outdated --global\nvagrant box update --box pandemonium/debvanilla\n</code></pre>"},{"location":"software/vagrant/#base-box-centos","title":"Base Box : Centos","text":"<p>Download latest CentOS release. Create a virtualbox machine :</p> <ul> <li>Name : ctsvanilla</li> <li>Memory : 1024</li> <li>Cpu : 2</li> <li>Syst\u00e8me de pointage : Souris PS/2</li> <li>Ordre d'amor\u00e7age : Optique, Disque Dur</li> <li>Acceleration : VT-x/AMD-V, Pagination Imbriqu\u00e9e, PAE/NX, Paravirtualisation KVM</li> <li>M\u00e9moire vid\u00e9o : 16 Mo</li> <li>Contr\u00f4leur graphique : VBoxVGA</li> <li>Son : D\u00e9sactiv\u00e9</li> <li>Usb : D\u00e9sactiv\u00e9</li> </ul> <p>As root user </p> <pre><code># Install yum priorities and epel repo\nyum install yum-plugin-priorities\nyum --enablerepo=extras install epel-release\n\n# Password-less sudo\nvisudo # vagrant ALL=(ALL) NOPASSWD: ALL\n\n# SSHH Tweaks\nvi /etc/ssh/sshd_config #UseDNS no\n\n# Install packages for virtualbox guest additions\nyum upgrade\nyum groupinstall \"Development Tools\"\nyum install kernel-devel\nyum install dkms wget\n\n# Mount last virtualbox guest additions cd.\nmount /dev/cdrom /mnt\nsh /mnt/VBoxLinuxAdditions.run\n\n# Umount and dont forget to remove tray then poweroff\numount /mnt\npoweroff\n\n# Reconnect and purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n# Poweroff via vbox\n</code></pre> <p>As vagrant user </p> <pre><code># To correctly generate .ssh folder\nssh-keygen\n\n# Grab the ssh insecure key\nwget https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub -O ~/.ssh/authorized_keys\n\n# Edit permissions\nchmod 600 ~/.ssh/authorized_keys &amp;&amp; rm ~/.ssh/id_rsa*\n\n# Purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n# Poweroff via vbox\n</code></pre>"},{"location":"software/vagrant/#base-box-ubuntu","title":"Base Box : Ubuntu","text":"<p>Download latest Ubuntu release. Create a virtualbox machine :</p> <ul> <li>Name : ubtvanilla</li> <li>Memory : 1024</li> <li>Cpu : 2</li> <li>Syst\u00e8me de pointage : Souris PS/2</li> <li>Ordre d'amor\u00e7age : Optique, Disque Dur</li> <li>Acceleration : VT-x/AMD-V, Pagination Imbriqu\u00e9e, PAE/NX, Paravirtualisation KVM</li> <li>M\u00e9moire vid\u00e9o : 16 Mo</li> <li>Contr\u00f4leur graphique : VBoxVGA</li> <li>Son : D\u00e9sactiv\u00e9</li> <li>Usb : D\u00e9sactiv\u00e9</li> </ul> <p>As root user </p> <pre><code># Update alternative to have vim instead of nano\nupdate-alternatives --config editor\n\n# Password-less sudo\nvisudo # vagrant ALL=(ALL) NOPASSWD:ALL at the enf of file\n\n# SSH Tweaks\nvi /etc/ssh/sshd_config #UseDNS no\n\n# Install packages for virtualbox guest additions\nsudo apt install linux-headers-$(uname -r) build-essential dkms\n\n# Reboot\nreboot\n\n# Mount last virtualbox guest additions cd.\nmount /dev/cdrom /mnt\nsh /mnt/VBoxLinuxAdditions.run\n\n# Umount and dont forget to remove tray then poweroff\numount /mnt\npoweroff\n\n# Reconnect and purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n# Poweroff via vbox\n</code></pre> <p>As vagrant user </p> <pre><code># To correctly generate .ssh folder\nssh-keygen\n\n# Grab the ssh insecure key\nwget https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub -O ~/.ssh/authorized_keys\n\n# Edit permissions\nchmod 600 ~/.ssh/authorized_keys &amp;&amp; rm ~/.ssh/id_rsa*\n\n# Purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n#Poweroff via vbox\n</code></pre>"},{"location":"software/vagrant/#base-box-mint","title":"Base Box : Mint","text":"<p>Download latest Linux Mint release. Create a virtualbox machine :</p> <ul> <li>Name : mntvanilla</li> <li>Memory : 2048</li> <li>Cpu : 2</li> <li>Syst\u00e8me de pointage : Souris PS/2</li> <li>Ordre d'amor\u00e7age : Optique, Disque Dur</li> <li>Acceleration : VT-x/AMD-V, Pagination Imbriqu\u00e9e, PAE/NX, Paravirtualisation KVM</li> <li>M\u00e9moire vid\u00e9o : 64 Mo</li> <li>Contr\u00f4leur graphique : VBoxVGA</li> <li>Acc\u00e9l\u00e9ration : 3D</li> <li>Son : D\u00e9sactiv\u00e9</li> <li>Usb : D\u00e9sactiv\u00e9</li> </ul> <p>As root user </p> <pre><code># Update alternative to have vim instead of nano\nupdate-alternatives --config editor\n\n# Password-less sudo\nvisudo # vagrant ALL=(ALL) NOPASSWD:ALL at the enf of file\n\n# SSH Tweaks\nvi /etc/ssh/sshd_config #UseDNS no\n\n# Install packages for virtualbox guest additions\napt update &amp;&amp; apt upgrade\napt install linux-headers-$(uname -r) build-essential dkms openssh-server\n\n# Reboot\nreboot\n\n# Mount last virtualbox guest additions cd.\nmount /dev/cdrom /mnt\nsh /mnt/VBoxLinuxAdditions.run\n\n# Umount and dont forget to remove tray then poweroff\numount /mnt\npoweroff\n\n# Reconnect and purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n# Poweroff via vbox\n</code></pre> <p>As vagrant user </p> <pre><code># To correctly generate .ssh folder\nssh-keygen\n\n# Grab the ssh insecure key\nwget https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub -O ~/.ssh/authorized_keys\n\n# Edit permissions\nchmod 600 ~/.ssh/authorized_keys &amp;&amp; rm ~/.ssh/id_rsa*\n\n# Purge history\ncat /dev/null &gt; .bash_history &amp;&amp; history -c\n\n#Poweroff via vbox\n</code></pre>"},{"location":"software/vagrant/#vagrant-provisioners","title":"Vagrant Provisioners","text":"<p>Provisioners can also be named :</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  # ... other configuration\n\n  config.vm.provision \"bootstrap\", type: \"shell\" do |s|\n    s.inline = \"echo hello\"\n  end\nend\n</code></pre> <p>Running Provisioners : Provisioners are run in three cases: the initial vagrant up, vagrant provision, and vagrant reload --provision.</p> <p>The --provision-with flag can be used if you only want to run a specific provisioner if you have multiple provisioners specified.  The arguments to --provision-with can be the provisioner type (such as \"shell\") or the provisioner name (such as \"bootstrap\" from above).</p> <p>Run Once, Always or Never :</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.provision \"bootstrap\", type: \"shell\", run: \"never\" do |s|\n    s.inline = \"echo hello\"\n  end\nend\n</code></pre>"},{"location":"software/vagrant/#source","title":"Source","text":"<p>Vagrant Getting Started</p>"},{"location":"software/virtualbox/","title":"VirtualBox : Installation et Configuration","text":""},{"location":"software/virtualbox/#version-des-outils","title":"Version des outils","text":"Os / Tool Version Windows 10 Professionnel 1709 VirtualBox 5.2.6"},{"location":"software/virtualbox/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":"<p>En cas de modification d'une vm suite \u00e0 un clone.: vim /etc/sysconfig/network-scripts/ifcfg-enp0s8 vim /etc/banner.txt vim /etc/hostname vim /etc/hosts</p>"},{"location":"software/vscode/","title":"Visual Studio Code","text":""},{"location":"software/vscode/#tools-versions","title":"Tools versions","text":"Os / Tool Version Linux Mint 19.3 Visual Studio Code 1.63.2 Molecule 3.0.3"},{"location":"software/vscode/#how-to-install","title":"How to install","text":"<pre><code>wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -o root -g root -m 644 packages.microsoft.gpg /etc/apt/trusted.gpg.d/\nsudo sh -c 'echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/trusted.gpg.d/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\nrm -f packages.microsoft.gpg\n</code></pre> <pre><code>sudo apt install apt-transport-https\nsudo apt update\nsudo apt install code # or code-insiders\n</code></pre>"},{"location":"software/vscode/#configuration","title":"Configuration","text":"<p>Installation of extensions :</p> <p>Installation of extensions :</p> <pre><code>code --install-extension bbenoist.vagrant --force\ncode --install-extension donjayamanne.python-environment-manager --force\ncode --install-extension EditorConfig.EditorConfig --force\ncode --install-extension iciclesoft.workspacesort --force\ncode --install-extension ms-azuretools.vscode-docker --force\ncode --install-extension ms-python.python --force\ncode --install-extension ms-vscode.atom-keybindings --force\ncode --install-extension redhat.ansible --force\ncode --install-extension redhat.vscode-yaml --force\ncode --install-extension yzhang.markdown-all-in-one --force\n</code></pre>"},{"location":"software/vscode/#knowledge-base","title":"Knowledge Base","text":""},{"location":"software/vscode/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"software/vscode/#source","title":"Source","text":"<p>Visual Studio Code Debian and Ubuntu based distributions Keyboard Shortcuts Reference</p>"}]}